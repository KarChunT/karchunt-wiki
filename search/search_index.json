{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to KarChunT Wiki","text":"<p>Hi, I'm Kar Chun! I'm an Infrastructure and DevOps Engineer at Intel. Welcome to my website, visit my website for documentation, tutorials, blog and more. I love to code and design software architecture.</p> <p>My ambition is to develop a new technology that can revolutionize the world. As part of my motivation to inspire people, this site shares what I have learned and studied previously. It would be the greatest thing I could ever hope for if someone looked at me and said, <code>Thanks to you, I didn't give up</code>\ud83e\udd73.</p> <p>You can contact me via these channels!</p> <ul> <li>Website</li> <li>GitHub</li> <li>LinkedIn</li> <li>Newsletter</li> <li>Email</li> </ul> <p>Here is my Credly profile, where you can find my certifications and badges. It is my hope that you can find something useful or helpful here. Thank you!</p>"},{"location":"docs/","title":"Documentation","text":"<ul> <li> <p> Secure Shell Protocol (SSH)</p> <p>SSH stands for Secure Shell (SSH) Protocol that is mainly used to connect to a Linux server remotely.</p> <p> Getting Started</p> </li> <li> <p> 12 Factor App</p> <p>It is a methodology for building software-as-a-service applications with best practices.</p> <p> Getting Started</p> </li> <li> <p> Git</p> <p>Git is a distributed version control system that tracks file changes and who made changes.</p> <p> Getting Started</p> </li> <li> <p> Python OOP</p> <p>Object-Oriented Programming (OOP) is a programming paradigm that uses objects and classes to structure software programs.</p> <p> Getting Started</p> </li> <li> <p> Docker</p> <p>Docker is an open platform for developing, shipping, and running applications using containers.</p> <p> Getting Started</p> </li> <li> <p> Taskfile</p> <p>Taskfile is a simple task runner for automating tasks in your development workflow and streamline your tasks.</p> <p> Getting Started</p> </li> <li> <p> Kubernetes</p> <p>Kubernetes, also known as K8s, is an open source system for automating deployment, scaling, and management of containerized applications.</p> <p> Getting Started</p> </li> </ul>"},{"location":"docs/12-factor-app/","title":"12 Factor App","text":"<p>Info</p> <p>Documentation - 12factor.net</p>"},{"location":"docs/12-factor-app/#what-is-12-factor-app","title":"What is 12 Factor App?","text":"<p>It's a methodology for building software-as-a-service (SaaS) or Cloud Native applications by providing a set of best practices to create web apps that are easy to deploy, scalable, maintainable, portable, and resilient.</p> <ul> <li>App can run in different execution environments without having the change the source code (Portability).</li> <li>Suitable for deployment on modern cloud platforms</li> <li>Minimize divergence between development and production</li> <li>Enable continuous deployment</li> <li>Easy to scale up</li> </ul>"},{"location":"docs/12-factor-app/#the-twelve-factors","title":"The Twelve Factors","text":"<p>Developers should consider 12 factors when building SaaS applications in accordance with this methodology. Of course, this methodology is not limited to building SaaS applications, instead, it can apply to other applications as well.</p>"},{"location":"docs/12-factor-app/#codebase","title":"Codebase","text":"<p>In summary, a codebase is always tracked in a version control system such as Git, and every app has only one codebase, but it will be deployed many times.</p> <ul> <li>Codebase = repository</li> <li>One to one relationship between codebase and app</li> <li>I'm referring app = service</li> </ul> <p>Imagine that you have a food web application that collaborates with multiple developers. So far, it only offers food ordering service. However, you need to ensure that the codebase of all developers is the same so that all deployments are consistent. In this case, Git will help all developers work on the same application at the same time, since everyone will push or pull changes from a central location like GitHub, GitLab, etc.</p> Folder structure<pre><code>Food web application/\n\u251c\u2500\u2500 ordering-service/\n\u2502   \u2514\u2500\u2500 source_code.py\n\u251c\u2500\u2500 delivery-service/\n\u2502   \u2514\u2500\u2500 source_code.py\n\u2514\u2500\u2500 payment-service/\n    \u2514\u2500\u2500 source_code.py\n</code></pre> <p>In the future, we may have delivery service, payment service, etc, but now the codebase has all the related services. When you have multiple application services, it's a distributed system and multiple application services sharing the same codebase already violates the 12 Factor.</p> <p></p> <p>So, we can separate those services into its own codebase, and within that codebase, we can have multiple deploys. Therefore, each service in a distributed system can comply with (obey) 12 factor.</p>"},{"location":"docs/12-factor-app/#dependencies","title":"Dependencies","text":"<p>In summary, all dependencies should be explicitly declare and isolate dependencies. It does not rely on implicit existence of system tools or libraries or packages.</p> app.py<pre><code>from fastapi import FastAPI\n\napp = FastAPI(\n  title='Notifications',\n  description='This is notification API',\n  version='0.1.0'\n)\n\nnotifications = [\n  {'name': 'Raymond Melton', 'message': 'Fight and road more hard whose.'},\n  {'name': 'Kevin Dunn', 'message': 'Ten environmental soldier often.'},\n]\n\n@app.get(\"/notifications\")\ndef get_all_notifications():\n  return notifications\n</code></pre> <p>Let's take a look on this example. We're building a backend services using FastAPI Python web framework. Before you start coding, you have to install this FastAPI Python web framework first.</p> <p>If we based on the 12 factor Dependencies concept.</p> <p>It does not rely on implicit existence of system tools or libraries or packages.</p> <p>That means, there is no guarantee that dependencies such as FastAPI will exist on the system where your application will be run. So, you have to declare all dependencies and isolate the dependencies as well. In Python;</p> <ul> <li>pip is used for declaration: <code>pip install -r requirements.txt</code></li> </ul> requirements.txt<pre><code>fastapi==0.45.0\n</code></pre> <ul> <li>virtualenv is used for isolation: <code>python -m venv venv</code>. As a result, we will be able to create an isolated environment for each application with its own version of dependencies.</li> </ul> <p>However, how about those <code>curl</code>, <code>wget</code>, and other tools that are dependent on the system. What we can do, is using Docker, as Docker container is a standalone or executable package of software that has everything like application source code, tools, libraries, dependencies, etc that you need to run an application.</p>"},{"location":"docs/12-factor-app/#config","title":"Config","text":"<p>In summary, we will need to store config in the environment variables, as configuration may be different between deployments.</p> app.py<pre><code>from fastapi import FastAPI\nimport mysql.connector\n\napp = FastAPI(\n  title='Notifications',\n  description='This is notification API',\n  version='0.1.0'\n)\n\n# Creating connection object\nmydb = mysql.connector.connect(\n  host = \"localhost\",\n  user = \"username\",\n  password = \"password\",\n  database = \"test\"\n)\n\ncursorObject = mydb.cursor()\n\n@app.get(\"/notifications\")\ndef get_all_notifications():\n  query = \"SELECT * FROM NOTIFICATIONS\"\n  cursorObject.execute(query)\n  myresult = cursorObject.fetchall()\n  return myresult\n</code></pre> <p>Let's take a look on this example. Right now, we're hard-coded the mysql host, user, password, and database values in the code. This already violates the 12 factor methodology concept, as the database configuration may be different between deployments.</p> .env<pre><code>HOST=localhost\nUSER=username\nPASSWORD=password\nDATABASE=test\n</code></pre> <p>So, we have to keep those configuration separately. What we can do is creating an <code>.env</code> file with those config values and inject this <code>.env</code> file into environment variable in the code.</p> app.py<pre><code>import os\nimport mysql.connector\n\nfrom fastapi import FastAPI\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napp = FastAPI(\n  title='Notifications',\n  description='This is notification API',\n  version='0.1.0'\n)\n\n# Creating connection object\nmydb = mysql.connector.connect(\n  host = os.getenv('HOST'),\n  user = os.getenv('USER'),\n  password = os.getenv('PASSWORD'),\n  database = os.getenv('DATABASE')\n)\n\ncursorObject = mydb.cursor()\n\n@app.get(\"/notifications\")\ndef get_all_notifications():\n  query = \"SELECT * FROM NOTIFICATIONS\"\n  cursorObject.execute(query)\n  myresult = cursorObject.fetchall()\n  return myresult\n</code></pre> <p>With this setup, we can use different configurations for different deployments, without compromising any credentials.</p>"},{"location":"docs/12-factor-app/#backing-services","title":"Backing Services","text":"<p>What is backing services?</p> <p>Any service the app consumes over the network.</p> <p>In summary, we will treat all backing services as attached resources.</p> <p>Example of Backing services;</p> <ul> <li>database (MySQL, PostgreSQL)</li> <li>caching (Redis)</li> <li>Messaging/queueing systems (Kafka, RabbitMQ)</li> <li>SMTP</li> <li>etc</li> </ul> <p>It makes no distinction between local and third party services</p> <p>\u2014 12factor.net</p> <p></p> <p>Imagine you have integrated PostgreSQL service to your application to store your data. A PostgreSQL database is an attached resource to your app, which can be run locally, in the cloud, or on a server. If it is hosted somewhere, it will work without having to change the application code since all the configurations, such as URL, credentials, etc., are stored in a config file like <code>.env</code>.</p>"},{"location":"docs/12-factor-app/#build-release-run","title":"Build, release, run","text":"<p>In summary, we will need to strictly separate the stage of building, release, and running.</p> <p></p> <ul> <li> <p>build stage</p> <ul> <li>Transform or convert the code into an executable or binary format. For example, you have a Python application that's going to run on Windows or Linux server, you can use setup tools to build the application, while in Java, you can use maven or etc to build the application. Of course you can use Docker as well.</li> <li>The build stage compiles assets and binaries based on vendor dependencies and the code.</li> </ul> </li> <li> <p>release stage</p> <ul> <li>It combines the executable file and config file to become the release object.</li> <li>executable + config = release object</li> <li>Every release should have a unique release Id. For example, timestamp (2024-05-11-13-30-10) or incrementing number (v1, v2, v3)</li> <li>Any changes to the codebase must create a new release and once a release has been created, it cannot be modified.</li> </ul> </li> <li> <p>run stage (also known as \"runtime\")</p> <ul> <li>Run the release object in the respective environment. You can deploy and run the release object into different environments and this will ensure that all environments will have the same codebase as they're coming from the same release object.</li> </ul> </li> </ul> <p>With that said, we can easily roll back to the previous releases based on the build artifacts that generated from the build stage.</p>"},{"location":"docs/12-factor-app/#processes","title":"Processes","text":"<p>In summary, applications should be deployed as one or more stateless processes (persist data stored on a backing service) and share-nothing.</p> app.py<pre><code>from flask import Flask\n\napp = Flask(__name__)\ntotal_visit = 0\n\n@app.route(\"/\")\ndef homepage():\n  global total_visit\n  total_visit += 1\n  return \"Welcome to my homepage\"\n\nif __name__ == \"__main__\":\n  app.run(host=\"0.0.0.0\")\n</code></pre> <p></p> <p>Let's take a look on this example. Right now, we have a <code>total_visit</code> global variable, it will increase it each time when the request/user comes in, but this only works with one application. Having multiple applications means they will each have their own version of this variable and they will not sync with each other (record everything separately), as the memory state is different (single-transaction cache).</p> <p>The another scenario is that when the user login to the website, normally we will cache the user session data. But, if we store all these user sessions data in the process memory or filesystem of the app, then when the user is redirected to another application, what will happen is that the user session data isn't available. Of course, it is possible to use sticky sessions to redirect the same user to the same application using load balancers. However, there is the possibility that the application may crash. In that case, all the data or sessions will be lost.</p> <p></p> <p>Sticky sessions are a violation of 12 factor and should never be used or relied upon. So, we should store all the data and session information on a backing service like database or caching system. Redis is a good option for storing session state data as it offers time-expiration.</p>"},{"location":"docs/12-factor-app/#port-binding","title":"Port binding","text":"<p>In summary, we have to export services via port binding, as 12 factor app is completely self-contained (services) and does not rely on a specific web server to function. This means that by declaring the port your application uses, the runtime and development environment know where to access your services. You must do this to ensure portability across platforms and environments.</p> app.py<pre><code>import os\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello_world():\n    return \"&lt;p&gt;Hello, World!&lt;/p&gt;\"\n\nif __name__ == '__main__':\n    app.run(debug=True, port=os.environ.get(\"PORT\", 5000))\n</code></pre> <p>For example, Flask application is using port 5000 by default. So, in the web application, HTTP is exported as a service by binding to a port 5000 and listening for requests. That means in local development environment, you can visit a service url like <code>http://localhost:5000</code> to access the service exported by their app.</p>"},{"location":"docs/12-factor-app/#concurrency","title":"Concurrency","text":"<p>In summary, we have to scale out via the process model. So the applications should scale out horizontally and not vertically by running multiple instances of the application concurrently.</p> <p></p> <p>As an example, you currently have one instance of your application serving multiple users. What if you have more users visiting your application? It's possible to scale your resources vertically by adding resources (RAM, Storage, etc) to the server, but that means the server must be taken down. This approach isn't good.</p> <p></p> <p>Because processes are the first class citizens of the twelve-factor app, we should scale out horizontally by adding/provisioning more servers so we can spin more instances. Of course, you can have a load balancer as well to balance your load across the instances of the application.</p>"},{"location":"docs/12-factor-app/#disposability","title":"Disposability","text":"<p>In summary, we will need to maximize robustness of a system with fast startup and graceful shutdown, as 12 factor app's processes are disposable, meaning they can be started or stopped at a moment's notice.</p> <p></p> <p>It's important for processes to make an effort to minimize startup time by avoiding complex startup scripts for provisioning the application and this concept also applies to reduce instances of the application.</p> <p>Processes shut down gracefully when they receive a SIGTERM signal from the process manager.</p> <p>\u2014 12factor.net</p> <p></p> <p>Here is an example, when executing the <code>docker stop</code> command for Docker containers, Docker initiates the SIGTERM signal to the container initially. If the container does not stop within a grace period, Docker will then send the SIGKILL signal to forcibly terminate the process running within the container.</p> <p>Info</p> <p>SIGTERM -&gt; SIGKILL -&gt; Terminate container process</p> <p>These two signals will allow the application to gracefully shutdown by stopping the acceptance of new requests and ensuring the completion of all ongoing requests. Therefore, this will prevent any impact on users who are waiting for a response from the application.</p>"},{"location":"docs/12-factor-app/#devprod-parity","title":"Dev/prod parity","text":"<p>In summary, we have to keep development, staging, and production environments as similar as possible by applying CI/CD concept.</p> <p></p> <p>Typically, there is a disconnect between the development and production environments, leading to gaps that can be categorized into three distinct areas</p> <ul> <li> <p>Time gap</p> <ul> <li>It could take the developer days, weeks, or even months to finalize code modifications and move them to production.</li> </ul> </li> <li> <p>Personnel gap</p> <ul> <li>Developers are responsible for writing code, while Ops engineers handle the deployment of changes. However, in most cases, they have limited or no knowledge about the new changes, making it difficult to identify issues caused by these new changes.</li> </ul> </li> <li> <p>Tools gap</p> <ul> <li>Different tools used in different environments, often leading to unforeseen outcomes (consequences). For instance, developers may use SQLite and Nginx in development environment, whereas MySQL and HAProxy use in production environment.</li> </ul> </li> </ul> <p>The twelve-factor app is designed for continuous deployment by keeping the gap between development and production small. The twelve-factor developer resists the urge to use different backing services between development and production.</p> <p>\u2014 12factor.net</p> <p>By applying Continuous Integration, Continuous Delivery, and Continuous Deployment. We can</p> <ul> <li>make the time gap small, as the developer can write code and deploy it in a matter of hours or even minutes.</li> <li>make the personnel gap small, the developer who writes the code is actively involved in deploying it and monitoring its behavior in production.</li> <li>make the tools gap small, we must ensure the tools as similar as possible that are being used in development and production environment. (Example: Docker)</li> </ul> Areas Traditional app 12-factor app Time Weeks or Months Hours or minutes Personnel Different people, Developer and Ops Engineers Same people, Developer Tools Different As similar as possible"},{"location":"docs/12-factor-app/#logs","title":"Logs","text":"<p>In summary, we need to treat logs as event streams and leave the execution environment to aggregate.</p> <p></p> <p>Logs give insight into how a running app behaves, such as detecting errors in code and recording all incoming requests. Normally, we save logs in a local file called <code>logfile</code>. However, this method has a drawback. In the era of containers, the container could be terminated at any moment, causing the logs to disappear.</p> <p></p> <p>In a different situation, an application may choose to send logs to a centralized logging system like fluentd. Although this is encouraged, it is not advisable to tightly couple or rely exclusively on that particular logging solution for the application.</p> <p>A twelve-factor app never concerns itself with routing or storage of its output stream.</p> <p>\u2014 12factor.net</p> <p></p> <p>We should</p> <ul> <li>Ensure your logs are stored in a structured format (JSON) and kept in a centralized location for easy access. An agent can easily transfer log data to a central location for querying, consolidating, and analyzing.</li> <li>Avoid writing to a particular file or being restricted to a specific logging system.</li> </ul>"},{"location":"docs/12-factor-app/#admin-processes","title":"Admin processes","text":"<p>In summary, admin/management tasks should be run as one-time processes, stored in source control, and separate them from application processes and they should be running on the same systems as the production application.</p> <p></p> <p>Let's say you discover that the total number of visits recorded in your webpage's PostgreSQL database is inaccurate. In such cases, you can reset the count. To do this, you can create a script that performs the reset as a one-time task. This process is similar to other tasks like database migration or fixing specific issues.</p> <p>You need to ensure the admin task is running on the same systems as the production application, but as a separate process.</p>"},{"location":"docs/docker/best-practices-for-creating-docker-image/","title":"Best Practices for creating a Docker image","text":""},{"location":"docs/docker/best-practices-for-creating-docker-image/#base-vs-parent-vs-custom-image","title":"Base vs Parent vs Custom Image","text":"<p>Note</p> <p>Before we dive into the best practices for creating a Docker image, it is important to understand the difference between base image, parent image, and custom image.</p> <pre><code>graph LR\n  A[scratch] --&gt; B[Base Image - ubuntu:22.04]\n  B --&gt; C[Parent Image - myparent]\n  C --&gt; D[Custom Image]</code></pre> <p>This is an example of base image. We can see that when an image is built from <code>scratch</code> image, it is called as base image.</p> Dockerfile - ubuntu:22.04 (base image)<pre><code>FROM scratch\nADD 433cf0b8353e08be3a6582ad5947c57a66bdbb842ed3095246a1ff6876d157f1 /\nCMD [\"bash\"]\n</code></pre> <p>This is an example of parent image. We can see that when an image is built from base image, it is called as parent image.</p> Dockerfile - mypythonparent (parent image)<pre><code>FROM ubuntu:22.04\nWORKDIR /app\n# Install additional dependencies if needed\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    python3 \\\n    python3-pip &amp;&amp; \\\n    pip3 install -r /app/requirements.txt &amp;&amp; \\\n    apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\nCMD [\"bash\"]\n</code></pre> <p>This is an example of custom image. We can see that when an image is built from parent image, it is called as custom image.</p> Dockerfile - mycustomimage (custom image)<pre><code>FROM mypythonparent\n\n# Copy application files into the container\nCOPY . /app\n\n# Set environment variables\nENV APP_ENV=production \\\n    APP_PORT=8080\n\n# Expose the application port\nEXPOSE 8080\n\n# Set the default command to run the application\nCMD [\"python3\", \"/app/main.py\"]\n</code></pre>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#best-practices-for-creating-a-docker-image_1","title":"Best Practices for Creating a Docker Image","text":""},{"location":"docs/docker/best-practices-for-creating-docker-image/#do-not-build-images-that-combine-multiple-applications","title":"Do not build images that combine multiple applications","text":"<p>Each image should be focused on a single application or service. For example, if you have a web server and a database, create separate images for each service.</p> <p>This allows for better scalability, maintainability, and reusability of images. It also helps in reducing the size of the images, as each image will only contain the necessary dependencies for that specific application.</p>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#do-not-store-data-or-state-in-the-container","title":"Do not store data or state in the container","text":"<p>Containers are ephemeral by nature, meaning they can be created and destroyed at any time. Therefore, if you store data or state in the container, it will be lost when the container is removed or destroyed.</p> <p>Instead, use volumes or any external storage (eg, Redis) solutions to persist data outside of the container.</p>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#keep-images-updated","title":"Keep images updated","text":"<p>Regularly update your images and dependencies to include the latest security patches.</p>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#scan-images-for-vulnerabilities","title":"Scan images for vulnerabilities","text":"<p>Regularly scan your images for vulnerabilities using tools like <code>docker scan</code> or third-party tools like Trivy.</p>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#avoid-installing-unnecessary-packages","title":"Avoid installing unnecessary packages","text":"<p>Only install the dependencies required for your application to run. Remove build tools and temporary files after installation.</p> Docker<pre><code>RUN apt-get update &amp;&amp; apt-get install -y \\\n  curl &amp;&amp; \\\n  apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#use-dockerignore-file","title":"Use <code>.dockerignore</code> file","text":"<p>Exclude unnecessary files and directories from the build context by using a <code>.dockerignore</code> file. This reduces the image size and speeds up the build process.</p> .dockerignore<pre><code>node_modules\n.git\n*.log\n</code></pre>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#leverage-multi-stage-builds","title":"Leverage Multi-Stage Builds","text":"<p>Use multi-stage builds to separate the build environment from the runtime environment, ensuring only the necessary files are included in the final image. This helps in reducing the image size and improving security by not including build tools in the final image.</p> Dockerfile<pre><code># Stage 1: Build\nFROM golang:1.20 AS builder\nWORKDIR /app\nCOPY . .\nRUN go build -o main .\n\n# Stage 2: Runtime\nFROM alpine:latest\nWORKDIR /app\nCOPY --from=builder /app/main .\nCMD [\"./main\"]\n</code></pre>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#optimize-layers","title":"Optimize Layers","text":"<p>Use the <code>RUN</code>, <code>COPY</code>, and <code>ADD</code> commands wisely to minimize the number of layers in your image. Each command creates a new layer, so combining commands can help reduce the overall size of the image, but keep readability in mind.</p> Docker<pre><code>RUN apt-get update &amp;&amp; apt-get install -y curl &amp;&amp; \\\n  apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#set-explicit-tags-for-base-images","title":"Set Explicit Tags for Base Images","text":"<p>Always use specific tags (e.g., <code>ubuntu:22.04</code>) instead of <code>latest</code> to ensure consistent builds and avoid unexpected changes. This helps in maintaining the stability of your application and avoiding compatibility issues that may arise from using the latest version of a base image.</p>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#use-non-root-user","title":"Use non-root user","text":"<p>Avoid running your application as the root user. Create a non-root user for better security. This helps in reducing the attack surface of your application and preventing unauthorized access to sensitive files and directories.</p> Docker<pre><code>RUN addgroup --system appgroup &amp;&amp; adduser --system --ingroup appgroup appuser\nUSER appuser\n</code></pre>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#document-the-image","title":"Document the Image","text":"<p>Use labels to document metadata about the image, such as the maintainer and version. This helps in keeping track of the image and providing information to users about the image.</p> <p>Benefits in Practice</p> <ul> <li>Automation: Tools like CI/CD pipelines can use labels to automate tasks (e.g., deploying specific versions).</li> <li>Compliance: Metadata can include compliance-related information, such as licensing or security details.</li> <li>Discoverability: Documented images are easier to search and identify in container registries.</li> </ul> Docker<pre><code>LABEL maintainer=\"karchunt\"\nLABEL version=\"1.0.0\"\nLABEL description=\"A custom Docker image for my application\"\n</code></pre>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#look-images-with-authenticity","title":"Look images with authenticity","text":"<p>Use official images or verified published tag from Docker Hub or other trusted registries.</p> <p>Use Docker Content Trust (DCT) to ensure that the images you are using are signed and verified. This helps in ensuring the authenticity of the images and preventing the use of malicious images.</p> Bash<pre><code># Enable Docker Content Trust\nexport DOCKER_CONTENT_TRUST=1\n</code></pre>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#use-minimalslim-base-images","title":"Use Minimal/Slim Base Images","text":"<p>Start with a minimal base image like <code>alpine</code> or <code>debian-slim</code> to reduce the image size and surface area for vulnerabilities. Only install the necessary dependencies for your application and remove any unnecessary tools like <code>curl</code>, <code>wget</code>, etc that could be used by attackers to download malicious files.</p>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#use-distroless-images","title":"Use Distroless Images","text":"<p>Distroless Images</p> <p>https://github.com/GoogleContainerTools/distroless</p> <p>Use distroless images for production deployments. Distroless images contain only the application and its runtime dependencies, without any package manager, shell, network tools, text editors or other unwanted programs. This reduces the attack surface and improves security.</p> <p>For example;</p> <ul> <li><code>gcr.io/distroless/python3-debian12</code></li> <li><code>gcr.io/distroless/base-debian12</code></li> </ul>"},{"location":"docs/docker/container/","title":"Container","text":"<p>Info</p> <p>All changes that are made to containers or networks are logged under the Docker System Events.</p>"},{"location":"docs/docker/container/#create-container","title":"Create container","text":"<p>When we create a new container, Docker will create a new directory <code>/var/lib/docker/containers/&lt;id&gt;.json</code> and all the container logs will be stored under that file by default.</p> Bash<pre><code>docker container create &lt;image&gt;\ndocker container create ubuntu\n</code></pre> <p>After the container is created, you have to start the container.</p>"},{"location":"docs/docker/container/#list-container-details","title":"List container details","text":"Bash<pre><code>docker container ls # same as docker ps\ndocker container ls -a # list all containers, including exited containers\ndocker container ls -q # only display container id\n</code></pre>"},{"location":"docs/docker/container/#start-container","title":"Start container","text":"<p>You can get container id from <code>docker container ls</code>.</p> Bash<pre><code>docker container start &lt;container-id&gt;\n</code></pre>"},{"location":"docs/docker/container/#run-a-container","title":"Run a container","text":"<p>run container = create container + start container</p> <p>Some options:</p> <ul> <li><code>-it</code> = interactive terminal (creates and enter the terminal session with the command you specify. This means you can execute commands inside the container while it is still running. Useful for debugging purposes.)</li> <li><code>-d</code> = detach (Run container in background)</li> <li><code>--name</code> = container name</li> <li><code>--rm</code> = remove container after the process is done</li> <li><code>--hostname</code> = setup container hostname</li> <li><code>--user</code> - setup container username</li> <li><code>-p</code> = port mapping</li> <li><code>--env</code> or <code>-e</code> or <code>--env-file</code> = setup environment variables</li> <li><code>--health-cmd</code> and <code>--health-interval</code> = check container state</li> <li><code>--privileged</code> = Login as root user, the Privileged container has full access to all the host's devices and files</li> <li> <p><code>--restart</code> = Restart policy</p> Restart Options Description no The container will never be restarted on-failure When the container fails, it will restart the container always The container will always be restarted unless-stopped It's very similar to \"always\" option, but the container will not restart when it was manually stopped and not even when the Docker Daemon restarts </li> </ul> Bash<pre><code>docker container run &lt;image&gt;\ndocker container run ubuntu # same as docker run ubuntu\n\n# creates and enter the terminal session\ndocker container run -it ubuntu\n\n# bash is the command that runs inside the container, it will start a new Bash shell within the container\ndocker container run -it ubuntu bash\n\n# Run in background with container name = myubuntu\ndocker container run -d --name=myubuntu ubuntu\n\n# remove the container after the process is done\ndocker container run --rm ubuntu\n\n# set hostname\ndocker container run --hostname=myubuntu ubuntu\n\n# Security - setup a username instead of root\ndocker container run --user=1000 ubuntu\n\n# Port mapping\ndocker container run -p &lt;local_port&gt;:&lt;container_port&gt; &lt;image&gt;\ndocker container run -p 80:5000 ubuntu\n\n# the application is only available on the network 192.168.1.*\ndocker container run -p 192.168.1.10:8000:5000 ubuntu\n\n# map the container port to a random port on the host (Ephemeral Port Range 32768 - 60999)\ndocker container run -p 5000 ubuntu\n\n# Check Health intervals (liveness probe)\n# Note: You need to append health-* in front of all parameters and cmd\ndocker run --health-cmd \"curl -f http://localhost:8000\" --health-interval=5s web-ubuntu\n\n# Define environment variables\ndocker run --env &lt;key&gt;=&lt;value&gt; &lt;image&gt;\ndocker run --env PORT=8000 ubuntu\ndocker run -e PORT=8000 ubuntu\n\n# Define environment variables using env file\ndocker run --env-file &lt;filename&gt; &lt;image&gt;\ndocker run --env-file .env ubuntu\n\n# Privileged container\ndocker run --privileged ubuntu\n\n# Restart policy\ndocker run --restart=no ubuntu\n</code></pre>"},{"location":"docs/docker/container/#expose-container-port-capital-p","title":"Expose container port (Capital P)","text":"<p>Normally it will auto-publish the ports of the container on the host, but what port?</p> <ul> <li>So with the Capital P option, it will expose all the ports configured in the Dockerfile (expose instruction) when the image is being built.</li> <li>Docker uses IPTables to map a port on a container to a port on the host and it uses Docker IPTables chains to modify or configure port mapping on a host.</li> </ul> Dockerfile<pre><code>FROM ubuntu:22.04\n\nRUN apt-get update\n\n...\n\nEXPOSE 8000\n</code></pre> Bash<pre><code>docker run -P ubuntuWebApp\n# Add additional ports that were not specific in the Dockerfile\ndocker run -P --expose=5000 ubuntuWebApp\n</code></pre>"},{"location":"docs/docker/container/#rename-container","title":"Rename container","text":"<p>Use <code>docker ps</code> command to get the current container name.</p> Bash<pre><code>docker rename &lt;old-name&gt; &lt;new-name&gt;\ndocker container rename &lt;old-name&gt; &lt;new-name&gt;\n</code></pre>"},{"location":"docs/docker/container/#run-a-new-command-in-a-running-container","title":"Run a new command in a running container","text":"Bash<pre><code>docker exec &lt;container-id&gt; &lt;command&gt;\ndocker container exec &lt;container-id&gt; &lt;command&gt;\ndocker container exec -it &lt;container-id&gt; /bin/bash\n</code></pre>"},{"location":"docs/docker/container/#attach-the-terminals-io-to-a-running-container","title":"Attach the terminal's I/O to a running container","text":"Bash<pre><code>docker attach &lt;container-id&gt;\ndocker container attach &lt;container-id&gt;\n</code></pre> <p>When you attach the terminal's I/O to a running container, you enter the command, it will display the result to all the users who attach back the container. As an example, if you exit the container, all people will leave the container at the same time.</p>"},{"location":"docs/docker/container/#inspect-container","title":"Inspect container","text":"Bash<pre><code>docker inspect &lt;container-id&gt;\ndocker container inspect &lt;container-id&gt;\n</code></pre>"},{"location":"docs/docker/container/#display-a-live-stream-of-containers-resource-usage-statistics","title":"Display a live stream of containers resource usage statistics","text":"<p>It will list containers with CPU, memory, network, and disk consumption.</p> Bash<pre><code>docker stats\ndocker container stats\ndocker container stats &lt;container-id&gt;\ndocker container stats &lt;container-id&gt; &lt;container-id&gt;\n</code></pre>"},{"location":"docs/docker/container/#display-running-processes-of-a-container","title":"Display running processes of a container","text":"<p>Display the processes and their process IDs on the Docker host.</p> Bash<pre><code>docker container top &lt;container-id&gt;\n</code></pre>"},{"location":"docs/docker/container/#container-logs","title":"Container Logs","text":"Bash<pre><code>docker container logs &lt;container-id&gt;\ndocker container logs -f &lt;container-id&gt; # view live logs\n</code></pre>"},{"location":"docs/docker/container/#pause-and-unpause-container","title":"Pause and Unpause container","text":"Bash<pre><code>docker container pause &lt;container-id&gt;\ndocker container unpause &lt;container-id&gt;\n</code></pre>"},{"location":"docs/docker/container/#restart-container","title":"Restart container","text":"Bash<pre><code>docker container restart &lt;container-id&gt;\n</code></pre>"},{"location":"docs/docker/container/#update-container","title":"Update container","text":"<p>Reference</p> <p>https://docs.docker.com/reference/cli/docker/container/update/</p> Bash<pre><code>docker container update --restart always &lt;container-id&gt;\ndocker container update --cpus=1.5 &lt;container-id&gt;\n</code></pre>"},{"location":"docs/docker/container/#stop-remove-and-prune-the-container","title":"Stop, remove, and prune the container","text":"<p>Info</p> <p>SIGTERM -&gt; SIGKILL -&gt; Terminate container process</p> <p>When executing the <code>docker stop</code> command for Docker containers, Docker initiates the SIGTERM signal to the container initially. If the container does not stop within a grace period, Docker will then send the SIGKILL signal to forcibly terminate the process running within the container.</p> Bash<pre><code>docker container stop &lt;container-id&gt;\ndocker container stop $(docker container ls -q) # stop all containers\n\ndocker container rm &lt;container-id&gt;\ndocker container rm $(docker container ls -qa) # remove all containers\n\ndocker container prune # remove all stopped containers\n</code></pre>"},{"location":"docs/docker/daemon-configuration/","title":"Daemon Configuration","text":""},{"location":"docs/docker/daemon-configuration/#docker-service-configuration","title":"Docker Service Configuration","text":"Bash<pre><code>sudo systemctl start docker # start docker service\nsudo systemctl status docker # check docker service status\nsudo systemctl stop docker # stop docker service\n</code></pre>"},{"location":"docs/docker/daemon-configuration/#start-docker-daemon-manually","title":"Start Docker Daemon manually","text":"Bash<pre><code>dockerd\ndockerd --debug # useful for troubleshooting and debugging purposes\n</code></pre>"},{"location":"docs/docker/daemon-configuration/#unix-socket","title":"Unix Socket","text":"<p>Unit socket is an IPC (inter-process communication mechanism) that enables communication between Docker clients on the same host, such as CLI, SDK, and the Docker Daemon.</p> <p>It will listen on an internal Unix Socket at the path <code>/var/run/docker.sock</code> when the Docker daemon starts. So when the container is built, the Docker socket file from the host machine will be mounted into the filesystem of the Docker container, so the Docker container can access to the Docker Daemon API via Docker CLI, as Docker CLI is configured to interact with the Docker Daemon on this socket.</p> <p>Warning</p> <p>Making Docker Daemon accessible outside of the Docker host is not a good approach due to security reasons.</p> <p>By default, the Docker Daemon can only accessible within the same host as it's only listening on the Unix Socket. Of course, Docker Daemon can also listen on a TCP interface on the Docker host.</p> Bash<pre><code>dockerd --debug --host=tcp://192.168.0.196:2375\n</code></pre> <ul> <li><code>192.168.0.196</code> = IP address of the host/machine</li> <li><code>2375</code> = standard port for Docker (unencrypted traffic)</li> </ul> <p>Then, other hosts can trigger any Docker commands to this Docker host by targeting their Docker Daemon to the TCP interface.</p> Bash<pre><code>export DOCKER_HOST=\"tcp://192.168.0.196:2375\"\nexport DOCKER_TLS=true # initiate secure connection\n</code></pre> <p>You can fix the TCP security issue (unencrypted traffic) by setting up TLS encryption to Docker Daemon.</p> Bash<pre><code>dockerd --debug \\\n  --host=\"tcp://192.168.0.196:2376\" \\\n  --tls=true \\\n  --tlscert=\"/var/docker/server.pem\" \\\n  --tlskey=/var/docker/serverkey.pem\n</code></pre> <ul> <li>When TLS is enabled, remember to change the port to 2376, as it's the standard port for encrypted traffic.</li> </ul> <p>However, there is still no authentication for others. Therefore, they can do whatever they want with Docker Daemon straight away. So, we have to enable certificate-based authentication.</p> Bash<pre><code>dockerd --debug \\\n  --host=\"tcp://192.168.0.196:2376\" \\\n  --tls=true \\\n  --tlscert=\"/var/docker/server.pem\" \\\n  --tlskey=\"/var/docker/serverkey.pem\" \\\n  --tlsverify=true \\\n  --tlscacert=\"/var/docker/caserver.pem\"\n</code></pre> <ul> <li><code>tlsverify</code> = enable authentication</li> <li><code>tlscacert</code> = use to verify client certificates. Therefore, the client will only be able to access Docker Daemon when they have the respective certificate.</li> </ul> <p>On the client side, we will have to generate certificates for them called <code>client.pem</code> and <code>clientkey.pem</code>. After that, they will have to <code>export DOCKER_TLS_VERIFY=true</code>.</p> <p>Sometimes, it's hard for users to memorize and manually insert all those options and configurations. Therefore, those options and configurations can move to a file, <code>/etc/dockers/daemon.json</code>.</p> /etc/dockers/daemon.json<pre><code>{\n  \"debug\": true,\n  \"hosts\": [\"tcp://192.168.0.196:2376\"],\n  \"tls\": true,\n  \"tlscert\": \"/var/docker/server.pem\",\n  \"tlskey\": \"/var/docker/serverkey.pem\",\n  \"tlsverify\": true,\n  \"tlscacert\": \"/var/docker/caserver.pem\",\n  \"live-restore\": true // the container will continue to run even Docker Daemon stops\n}\n</code></pre> <p>If you continue to specify those options and configurations via <code>dockerd</code> command, it will display an error message.</p> <p>Once you edit this file <code>/etc/dockers/daemon.json</code>, remember to reload Docker.</p> Bash<pre><code>sudo systemctl reload docker\n# see your docker info after configuration\ndocker system info\n</code></pre>"},{"location":"docs/docker/daemon-configuration/#logging-driver","title":"Logging Driver","text":"<p>Info</p> <p><code>docker logs</code> command is used to get container logs.</p> <p>Docker Logging Driver Configuration</p> <p>Docker Daemon has a default logging driver, which is json-file. You can use <code>docker system info</code> to get the current logging driver. All the containers logs are stored under this file <code>/var/lib/docker/containers/&lt;id&gt;.json</code> by default.</p> Bash<pre><code>cat &lt;id&gt;.json\n</code></pre> <p>Of course, there are multiple logging driver options that the user can change;</p> <ul> <li>json-file (default)</li> <li>none</li> <li>syslog</li> <li>local</li> <li>journald (docker logs)</li> <li>splunk</li> <li>awslogs</li> </ul> /etc/docker/daemon.json<pre><code>{\n  \"log-driver\": \"awslogs\",\n  \"log-opt\": {\n    // additional options for logging region\n    \"awslogs-region\": \"ap-southeast-1\"\n  }\n}\n</code></pre> <p>With this setup, all the container logs will be sent to Amazon CloudWatch Logs.</p>"},{"location":"docs/docker/daemon-configuration/#storage-driver","title":"Storage Driver","text":"<p>Reference</p> <p>Docker uses the storage drivers to store image layers and to store data in the writable layer of a container. The storage driver controls how images and containers are stored and managed on your Docker host.</p> <p>\u2014 docker.docs</p> <p>Supported storage drivers;</p> <ul> <li>overlay2</li> <li>btrfs and zfs</li> <li>vfs</li> <li>fuse-overlayfs</li> </ul> <p>Proceed to this <code>/etc/docker/daemon.json</code> file to change the storage driver.</p> /etc/docker/daemon.json<pre><code>{\n  \"storage-driver\": \"overlay2\"\n}\n</code></pre>"},{"location":"docs/docker/daemon-configuration/#troubleshoot-docker-daemon","title":"Troubleshoot Docker Daemon","text":""},{"location":"docs/docker/daemon-configuration/#view-docker-daemon-logs","title":"View Docker Daemon Logs","text":"Bash<pre><code>journalctl -u docker.service\n</code></pre>"},{"location":"docs/docker/daemon-configuration/#check-free-disk-space-on-host","title":"Check free disk space on host","text":"Bash<pre><code>df -h\n</code></pre>"},{"location":"docs/docker/docker-architecture/","title":"Docker Architecture","text":""},{"location":"docs/docker/docker-architecture/#docker-engine-architecture","title":"Docker Engine Architecture","text":"<p>Docker Engine is the heart of the container and it consists of 3 core elements;</p> <ul> <li>Docker CLI<ul> <li>A command line interface that the user will use to run the commands to manage Docker objects.</li> </ul> </li> <li>REST API<ul> <li>Enables the communication between applications and Docker and gives Dockerd instructions.</li> </ul> </li> <li>Docker Daemon (dockerd) -&gt; Server<ul> <li>It's the server responsible for creating and managing objects.</li> <li>It's the heart of Docker.</li> </ul> </li> </ul>"},{"location":"docs/docker/docker-architecture/#containerd","title":"containerd","text":"<p>Open Container Initiative (OCI)</p> <p>A set of open industry standards around container formats and runtime. OCI created two specifications to make the creation of container standards more efficient.</p> <ul> <li>Runtime-spec</li> <li>Image-spec</li> </ul> <p>These two specifications mainly defines the lifecycle of a container/image technology. For example, delete command should delete a container/image, etc.</p> <p>It manages the container lifecycle (start, stop, pause, delete), image distribution (push, pull to/from registries).</p> <p>When the user makes a request to dockerd, containerd will push/pull the image to/from registries, and convert the image that was downloaded into an OCI compliance bundle.</p>"},{"location":"docs/docker/docker-architecture/#libcontainerrunc","title":"libcontainer/runC","text":"<p>At the very beginning, Docker has a monolithic architecture and used LXC (Linix Container) technology to build environments for applications. After a while, the architecture of Docker was modified to a modular design, allowing for quicker innovation. Also, they replaced LXC with libcontainer as the default execution environment, now known as runC.</p> <p>What is cgroups and namespaces?</p> <p>cgroups - Resource allocation for a given process can be easily monitored and managed via Linux OS, and resource limits can be set for memory, CPU, and network resources.</p> <p>namespaces - Isolating processes from one another. In each container, processes will run in their own namespace and won't be able to access anything outside of its namespaces. (Docker containers have network isolation (via libnetwork), allowing separate virtual interfaces and IP addresses for each container, that means that the container will have its own virtual interfaces, routing and ARP tables).</p> <p>Docker use namespaces to isolate</p> <ul> <li>Process Id (pid) - Two proccesses cannot have the same process Id, with the help of namespaces, each process can have multiple process IDs associated with it.</li> <li>Unix Timesharing Systems (UTS)</li> <li>Mount (mnt)</li> <li>Inter-Process Communication (IPC)</li> <li>Network (net)</li> </ul> <p>runC is a lightweight CLI and it's used to create and run containers. The user can use the CLI to spawn and run the containers without Docker, so it can be easily integrated with higher-level container orchestration systems like Kubernetes.</p> <ul> <li>It will interact with the cgroups and namespaces on the kernel levels to create and run a container.</li> <li>In each execution, <code>/tmp/docker/[uuid]/</code> is created as the container's root file system.</li> </ul>"},{"location":"docs/docker/docker-architecture/#containerd-shim","title":"containerd-shim","text":"<p>containerd-shim is mainly use to make the containers daemon less, monitors the state of the container, and it is in charge of handling input (STDIN) and output (STDOUT) and notifying the Docker Daemon about the exit status.</p> <p>It mainly takes care of the containers when the daemon is down or restarted. That means, the containers will run in the background and will be attached back to the daemon when it comes back or online.</p> <p>How containerd-shim make the container become daemonless container?</p> <ul> <li>Each time a container is created, containerd forks an instance of runC</li> <li>After runC creates the container, the runC process will exit, and shim will replace runC and become the new container parent.</li> </ul>"},{"location":"docs/docker/docker-architecture/#docker-objects","title":"Docker Objects","text":"<p>Docker Objects consists of 4 core elements;</p> <ul> <li>Images</li> <li>Containers</li> <li>Volumes</li> <li>Networks</li> </ul>"},{"location":"docs/docker/docker-architecture/#images","title":"Images","text":"<p>What is Dockerfile?</p> <p>A Dockerfile is a text file that will create a custom Docker image from a set of commands or instructions. Each instruction in the Dockerfile will represent a layer of the Docker image.</p> <p>Docker image acts as a set of instructions to build a Docker container, you can say it is a read-only template.</p> <p>The Docker image contains all the necessary components for the application to run as a container, including the source code, tools, libraries, dependencies, and more. The user can build an image from a Dockerfile through <code>docker build</code> command.</p>"},{"location":"docs/docker/docker-architecture/#containers","title":"Containers","text":"<p>Docker Container is an instance of an image running as a process. It is a standalone or executable package of software that has everything like application source code, tools, libraries, dependencies, runtime, settings, etc that you need to run an application.</p>"},{"location":"docs/docker/docker-architecture/#volumes","title":"Volumes","text":"<p>Docker volume is used to persist and share the container's data across containers. Folders on your host machine's hard drive are mounted into containers as volumes, which allows the container to write its data into the host volumes.</p> <p>Benefits: Volumes are easy to backup.</p>"},{"location":"docs/docker/docker-architecture/#networks","title":"Networks","text":"<p>Docker networking enables a container to be able to communicate with other containers. It can link a Docker container to as many networks as the user requires. It's used to create an isolated environment (provide an isolation) for Docker containers.</p> <p>Example of Docker network drivers;</p> <ul> <li>Bridge</li> <li>Host</li> <li>None</li> <li>Overlay</li> <li>Macvlan</li> </ul>"},{"location":"docs/docker/docker-architecture/#registry","title":"Registry","text":"<p>Note</p> <p>Docker Enterprise Edition provides a private trusted registy known as Docker Trusted Registry (DTR).</p> <p>Docker images are stored in a registry via <code>docker push</code> command, which enables the sharing and publishing of images either publicly or within a private organization.</p> <p>Example;</p> <ul> <li>Docker Hub</li> <li>Harbor</li> </ul>"},{"location":"docs/docker/docker-compose/","title":"Docker Compose","text":"<p>Info</p> <p>Use to startup multiple Docker containers at the same time.</p>"},{"location":"docs/docker/docker-compose/#composeyaml","title":"compose.yaml","text":"<ul> <li>Reference </li> <li>Examples</li> </ul> <p>Note</p> <p>All configurations here are the same from the <code>docker run</code> command. Currently, I'm showing docker compose v2 instead of v1. So some syntax might be different due to some syntax already deprecated.</p> <p>A default network is created for all the composed containers. that means all containers that are created will be automatically added to that network.</p> compose.yaml<pre><code>name: apiapp\nservices:\n  postgres:\n    image: postgres:14\n    networks:\n      - db\n    environment:\n      POSTGRES_USER: sonar\n      POSTGRES_PASSWORD: sonar\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n  sonarqube:\n    image: sonarqube:lts\n    ports:\n      - '9000:9000'\n      - '9092:9092'\n    networks:\n      - db\n    environment:\n      SONAR_JDBC_URL: jdbc:postgresql://postgres:5432/sonar\n      SONAR_JDBC_USERNAME: sonar\n      SONAR_JDBC_PASSWORD: sonarpasswd\n    depends_on:\n      - postgres\n  apiapplication:\n    build: ./\n    volumes:\n      - /app/node_modules\n      - ./app:/app\n    depends_on:\n      - postgres\n  server:\n    build:\n      context: ./server # system look dir to search for Dockerfile\n      dockerfile: Dockerfile.dev\nnetworks:\n  db:\nvolumes:\n  postgres_data:\n</code></pre> <p>Additional;</p> <ul> <li>If you want to specify <code>-it</code> in <code>compose.yaml</code>, you will have to specify these two fields (mandatory)<ul> <li><code>stdin_open: true</code> - This service needs an open input connection</li> <li><code>tty: true</code> - Attaching this terminal</li> <li><code>-it</code> = <code>stdin_open</code> + <code>tty</code></li> </ul> </li> <li>If you want to override the <code>Dockerfile</code> entrypoint, you can specify <code>entrypoint</code> within <code>compose.yaml</code>, then it will override the entrypoint in <code>Dockerfile</code> that is defined.</li> </ul>"},{"location":"docs/docker/docker-compose/#commands","title":"Commands","text":"<p>Reference</p> <p>Use <code>docker compose</code> instead of <code>docker-compose</code>.</p>"},{"location":"docs/docker/docker-compose/#list-containers","title":"List containers","text":"Bash<pre><code>docker compose ps\n</code></pre>"},{"location":"docs/docker/docker-compose/#build-or-rebuild-containers","title":"Build or rebuild containers","text":"<p>It only builds containers without starting it.</p> Bash<pre><code>docker compose build\n</code></pre>"},{"location":"docs/docker/docker-compose/#create-and-start-containers","title":"Create and start containers","text":"<p>Up = Build/rebuild + start</p> Bash<pre><code>docekr compose up\ndocker compose up --build # rebuild containers (build images before starting containers)\ndocker compose -d up # running in backgound\ndocker compose -f &lt;compose-file&gt; up\ndocker compose up &lt;service&gt; &lt;service&gt; # start only a few service\n</code></pre>"},{"location":"docs/docker/docker-compose/#execute-a-command-in-a-running-container","title":"Execute a command in a running container","text":"<p>By default, it will allocate a TTY by default, so you can straight away get an interactive prompt from this command <code>docker compose exec apiservice bash</code>.</p> Bash<pre><code>docker compose exec &lt;service-name&gt; &lt;commands&gt;\ndocker compose exec -it apiservice bash # old version\ndocker compose exec apiservice bash # new version\n</code></pre>"},{"location":"docs/docker/docker-compose/#display-service-log-output","title":"Display service log output","text":"Bash<pre><code>docker compose logs\n</code></pre>"},{"location":"docs/docker/docker-compose/#stop-services","title":"Stop services","text":"<p>Stop running containers without removing them. The services can be started again with <code>docker compose start</code>.</p> Bash<pre><code>docker compose stop\n</code></pre>"},{"location":"docs/docker/docker-compose/#stop-and-remove-containers-networks","title":"Stop and remove containers, networks","text":"Bash<pre><code>docker compose down\n</code></pre>"},{"location":"docs/docker/docker-installation/","title":"Docker Installation","text":"<p>Info</p> <p>Here is the link for installing Docker Engine. I will only show how to install Docker Engine on Ubuntu, as installing Docker on Windows is very straightforward.</p>"},{"location":"docs/docker/docker-installation/#install-docker-engine-on-ubuntu","title":"Install Docker Engine on Ubuntu","text":"<p>All the following steps are coming from this reference.</p> <ol> <li> <p>Setup Docker's <code>apt</code> repository</p> Bash<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n    \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n    $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n    sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\nsudo apt-get update\n</code></pre> </li> <li> <p>Install the Docker packages.</p> Bash<pre><code>sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> </li> <li> <p>Verify that the Docker Engine installation is successful by running the hello-world image.</p> Bash<pre><code>sudo docker run hello-world\n</code></pre> </li> <li> <p>[Optional] Manage Docker as a non-root user.</p> Bash<pre><code>sudo groupadd docker\nsudo usermod -aG docker $USER\nnewgrp docker\n\ndocker run hello-world # without sudo\n</code></pre> </li> </ol>"},{"location":"docs/docker/future/","title":"Future","text":"<p>My Docker documentation does not stop here, there are still a lot of things I haven't gone through, especially Docker Swarm, Docker Engine Enterprise, Docker updates, etc. Stay tuned!</p>"},{"location":"docs/docker/image/","title":"Image","text":""},{"location":"docs/docker/image/#image-operations","title":"Image Operations","text":""},{"location":"docs/docker/image/#list-images","title":"List images","text":"Bash<pre><code># All these commands are the same\ndocker image ls\ndocker image list\ndocker images\n</code></pre>"},{"location":"docs/docker/image/#search-images","title":"Search images","text":"<p>Reference</p> Bash<pre><code>docker search &lt;image&gt;\ndocker search --limit 2 &lt;image&gt;\ndocker search --filter stars=2 --filter is-official=true &lt;image&gt;\n</code></pre>"},{"location":"docs/docker/image/#pull-download-the-image","title":"Pull / Download the image","text":"Bash<pre><code>docker image pull &lt;image&gt;\ndocker pull &lt;image&gt;\n</code></pre>"},{"location":"docs/docker/image/#push-image","title":"Push image","text":"Bash<pre><code>docker image push &lt;image&gt;\ndocker push &lt;image&gt;\n</code></pre>"},{"location":"docs/docker/image/#tag-image","title":"Tag image","text":"Bash<pre><code># same as docker tag\ndocker image tag &lt;image:tag&gt; &lt;newimage:newtag&gt;\ndocker image tag &lt;image:tag&gt; &lt;newimage&gt;\n\ndocker image tag ubuntu:22.10 gcr.io/karchunt/ubuntu:latest\n</code></pre>"},{"location":"docs/docker/image/#inspect-image","title":"Inspect image","text":"<p>It will display detailed information on one or more images.</p> Sample JSON after you docker image inspect<pre><code>[\n  {\n    \"Id\": \"sha256:bf3dc08bfed031182827888bb15977e316ad797ee2ccb63b4c7a57fdfe7eb31d\",\n    \"ContainerConfig\": {\n      \"Hostname\": \"4329d013a36a\",\n      \"Domainname\": \"\",\n      ...\n    },\n    \"Architecture\": \"amd64\",\n    \"Os\": \"linux\",\n    ...\n  }\n]\n</code></pre> Bash<pre><code>docker image inspect &lt;image&gt;\ndocker image inspect &lt;image&gt; -f '{{.Os}}' # retrieve OS\ndocker image inspect &lt;image&gt; -f '{{.ContainerConfig.Hostname}}'\n</code></pre>"},{"location":"docs/docker/image/#remove-image-and-remove-all-unused-image","title":"Remove image and remove all unused image","text":"<p>Before deleting an image, all containers must be removed or deleted first, as they are dependent on that image.</p> Bash<pre><code>docker image rm &lt;image&gt;\ndocker image prune -a\n</code></pre>"},{"location":"docs/docker/image/#display-image-layers","title":"Display image layers","text":"Bash<pre><code>docker image history &lt;image&gt;\ndocker history &lt;image&gt;\n</code></pre>"},{"location":"docs/docker/image/#save-or-load-image","title":"Save or load image","text":"<p>Imagine you are in an environment without access to Wifi to download the image. With image save commands, you can convert your image into a tar file, copy that to your environment, and then extract it.</p> Bash<pre><code>docker image save &lt;image&gt; -o &lt;tarfile-path&gt;\ndocker image load -i &lt;tarfile-path&gt;\n\n# example\ndocker image save ubuntu:latest -o ubuntu.tar\ndocker image load -i ubuntu.tar\n</code></pre>"},{"location":"docs/docker/image/#convert-container-into-image-in-a-tar-format-using-import-and-export-operations","title":"Convert container into image in a tar format using Import and Export operations","text":"<p>Info</p> <p>Make or modify the image to a single layer.</p> <p>With import and export commands, basically, we are flattening a Docker image into a single layer, therefore, we will get a smaller size of the image.</p> <p>Note</p> <p>You can also export the exited container as well.</p> <p>It only exports the contents of the directory, without the contents of the volume.</p> Bash<pre><code># You will see a lot of layers inside, for example: ubuntu\ndocker image history &lt;image&gt;\n\n# run the container\ndocker container run &lt;image&gt;\n\n# export a container as a tar archive\ndocker container export &lt;container&gt; &gt; &lt;tarfile-path&gt;\ndocker container export &lt;container&gt; -o &lt;tarfile-path&gt;\ndocker container export ubuntu_container_name &gt; ubuntu.tar\n\n# import tar archive into image\ndocker image import &lt;tarfile-path&gt; &lt;new-image&gt;\ndocker image import ubuntu.tar newubuntu:latest\n</code></pre>"},{"location":"docs/docker/image/#image-naming-convention-and-authenticate-to-registries","title":"Image naming convention and Authenticate to registries","text":"<p>Reference</p> <p>It's made up of a slash-separated name components. Before you push those images to respective registries, you have to perform authentication to the registry that you want to push.</p> <p>For example;</p> <ul> <li>Harbor: \"harbor_address\"/project/repository<ul> <li>\"karchunt.registry.com/python/auto-deploy\"</li> </ul> </li> <li>Dockerhub: \"docker.io/username/repository\"<ul> <li>\"docker.io/karchunt/maven-with-docker\"</li> </ul> </li> </ul> Bash<pre><code># By default us docker.io\ndocker login\ndocker login &lt;registry-address/server&gt;\n\n# example\ndocker login karchunt.registry.com\n</code></pre> <p>Once you login successfully, those credentials will be stored in <code>$HOME/.docker/config.json</code> on Linux or <code>%USERPROFILE%/.docker/config.json</code> on Windows.</p> Naming registry project/user/account image/repository karchunt.registry.com/python/auto-deploy karchunt.registry.com python auto-deploy docker.io/karchunt/maven-with-docker docker.io karchunt maven-with-docker"},{"location":"docs/docker/image/#dockerfile-build-a-custom-image","title":"Dockerfile (Build a custom image)","text":"<p>Reference</p> <p>Dockerfile is a text file that will consist of all the steps that are required to build a custom image. Here is a very basic and sample Dockerfile to dockerize Flask application.</p> Dockerfile<pre><code>FROM python:3.11.0-slim-bullseye\n\nARG PORT=5000\n\nWORKDIR /app\nCOPY requirements.txt ./\n\nRUN pip install -r requirements.txt\n\nCOPY ./ ./\nEXPOSE $PORT\n\nENTRYPOINT [\"python\"]\nCMD [\"app.py\"]\n</code></pre> <p>When you run the <code>docker build</code> command, all files under the build context are transferred to the Docker Daemon. It stores them in <code>/var/lib/docker/tmp</code> for temporary storage. Docker looks for <code>Dockerfile</code> at whatever path is specified in the build context.</p> Bash<pre><code>docker image build -t &lt;name:tag or just name&gt; &lt;PATH&gt;\ndocker build -t &lt;name:tag or just name&gt; &lt;PATH&gt;\n# ARG (available inside of Dockerfile)\ndocker build --build-arg &lt;key&gt;=&lt;value&gt; &lt;PATH&gt;\n\n# \".\" or any path is build context\ndocker build -f &lt;Dockerfile-path&gt; &lt;folder&gt;\n\n# build with no cache\ndocker build --no-cache &lt;PATH&gt;\n\n# Get Dockerfile from repo\ndocker build &lt;repo-url&gt;\ndocker build &lt;repo-url#&lt;branch&gt;&gt;\ndocker build &lt;repo-url:&lt;folder&gt;&gt;\n\n# build the image for only 1 or more stages\ndocker build --target &lt;stage-name&gt; &lt;PATH&gt;\n\n# Example\ndocker build -t order-api:v0.0.1 .\ndocker build --build-arg PORT=8000 ./\ndocker build https://github.com/karchunt/app\ndocker build https://github.com/karchunt/app#develop\ndocker build https://github.com/karchunt/app:manifests\ndocker build -f Dockerfile.prod /folder1\ndocker build --target deployment ./\n</code></pre> <ul> <li>You can also create <code>.dockerignore</code> file to tell the build context to exclude or ignore those files or directories.</li> </ul>"},{"location":"docs/docker/image/#dockerfile-explanation","title":"Dockerfile explanation","text":"Instruction Description ADD Add local or remote files and directories. ARG Use build-time variables. This argument will be used during the <code>docker build</code> section, to remove those hardcoded values CMD Specify default commands should be executed when container is running. COPY Copy files and directories. ENTRYPOINT Specify default executable. ENV Set environment variables. EXPOSE Describe which ports your application is listening on. It does not actually publish the port. FROM Create a new build stage from a base image. Our image will be customized using this initial set of programs or tools HEALTHCHECK Check a container's health on startup. LABEL Add metadata to an image and it's a key value pair. MAINTAINER Specify the author of an image. ONBUILD Specify instructions for when the image is used in a build. RUN Execute build commands. SHELL Set the default shell of an image. STOPSIGNAL Specify the system call signal for exiting a container. USER Set user and group ID. VOLUME Create volume mounts that need specified folder to be persistent inside container. We also need to specify <code>docker run -v &lt;path&gt;:&lt;path-in-container&gt;</code> WORKDIR Change working directory."},{"location":"docs/docker/image/#workdir","title":"WORKDIR","text":"<p>It can be used multiple times in a <code>Dockerfile</code>.</p> Docker<pre><code>WORKDIR /app\nWORKDIR main # it will display warning as the path is absolute\n\nRUN pwd # output = /app/main, it will stack\n</code></pre>"},{"location":"docs/docker/image/#healthcheck","title":"HEALTHCHECK","text":"<p>Basically, it will check a container's health on startup by telling the platform on how to test the application is healthy. It will also monitor the container process when it's running.</p> <p>Parameters for HEALTHCHECK</p> <ul> <li><code>--interval=DURATION</code> (default: 30s)</li> <li><code>--timeout=DURATION</code> (default: 30s)</li> <li><code>--start-period=DURATION</code> (default: 0s)</li> <li><code>--retries=N</code> (default: 3)</li> </ul> Exit status Description 0 success 1 failure 2 reserved (do not use the exit code)"},{"location":"docs/docker/image/#copy-vs-add","title":"COPY vs ADD","text":"<p>Info</p> <p>In Dockerfile, <code>COPY</code> is recommended over <code>ADD</code> to reduce layer count</p> <p>Both of them are just copying files, but the <code>ADD</code> instruction will have more usage compared to <code>COPY</code>. <code>COPY</code> just lets you copy, while <code>ADD</code> can auto extract the tar file into the path inside the image. For URL, it will only download, but does not perform the extraction.</p> Dockerfile<pre><code>FROM ubuntu\n\nCOPY myfile.txt /app\n\nADD newfile.txt /app\nADD archive.tar /app\nADD https://mysamplearchive.tar /app\n</code></pre>"},{"location":"docs/docker/image/#cmd-vs-entrypoint-utility-container","title":"CMD vs ENTRYPOINT (Utility container)","text":"<p>A lot of people confuse <code>CMD</code> and <code>ENTRYPOINT</code> instructions.</p> Dockerfile<pre><code>FROM ubuntu\nENTRYPOINT [\"sleep\"]\nCMD [\"5\"]\n</code></pre> <p>ENTRYPOINT</p> <ul> <li>Specify the default executable, which means setting the image's main command. It cannot be overridden.</li> </ul> <p>CMD</p> <ul> <li>Able to override, for example <code>docker run &lt;image&gt; 7</code>, \"7\" will be replaced <code>CMD</code> command that you specify in <code>Dockerfile</code>.</li> </ul>"},{"location":"docs/docker/image/#build-cache","title":"Build cache","text":"<p>Every layer in the Dockerfile has a cache. It will compare instructions in Dockerfile and checksums of files in <code>ADD</code> or <code>COPY</code>. If the instructions have been modified, then it will rebuild that layer.</p> Bash<pre><code>docker build &lt;PATH&gt; # with cache\ndocker build --no-cache &lt;PATH&gt; # build with no cache\n</code></pre>"},{"location":"docs/docker/image/#multi-stage-builds","title":"Multi-stage builds","text":"<p>Multi-stage builds will help to generate smaller images. It makes the developers easy to read and maintain as you only keep the required dependencies, therefore resulting in a more secure container.</p> Dockerfile<pre><code>###### build stage\nFROM python:3.11.0-slim-bullseye as build\n\nENV VIRTUAL_ENV=/app\nENV PATH=\"$VIRTUAL_ENV/venv/bin:$PATH\"\n\nWORKDIR $VIRTUAL_ENV\n\nRUN apt-get update \\\n    &amp;&amp; apt-get install -y libpq-dev python3-psycopg2 \\\n    &amp;&amp; python -m venv $VIRTUAL_ENV/venv\n\nCOPY requirements.txt .\n\nRUN pip install --upgrade pip \\\n    &amp;&amp; pip install --no-cache-dir --upgrade -r requirements.txt\n\n###### base environment\nFROM python:3.11.0-slim-bullseye as base\n\n# Create system user and group\nRUN groupadd -g 999 apiuser \\\n    &amp;&amp; useradd -r -u 999 -g apiuser apiuser\n\nRUN mkdir /app &amp;&amp; chown apiuser:apiuser /app\nWORKDIR /app\n\n# 0 means build stage\n# You can also use 0, COPY --chown=apiuser:apiuser --from=0 /app/venv ./venv\nCOPY --chown=apiuser:apiuser --from=build /app/venv ./venv\nCOPY --chown=apiuser:apiuser . .\n\n# system user\nUSER 999\n\nEXPOSE 8000\n\nENV PATH=\"/app/venv/bin:$PATH\"\n\n###### testing environment\nFROM base as test\nRUN pip install --no-cache-dir --upgrade -r requirements-test.txt\nENTRYPOINT [ \"pytest\" ]\nCMD [\"-v\", \"--disable-pytest-warnings\"]\n\n##### development environment\nFROM python:3.11.0-slim-bullseye as development\nCMD [\"uvicorn\", \"src.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--reload\"]\n\n###### deployment environment\nFROM base as deployment\nCMD [\"uvicorn\", \"src.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\nHEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 CMD [ \"curl\", \"-f\", \"http://localhost:8000/healthcheck\" ]\n</code></pre>"},{"location":"docs/docker/image/#create-custom-image-from-running-container","title":"Create custom image from running container","text":"<p>Reference</p> <p>Warning</p> <p>Create custom image from running container is not recommended. Please use Dockerfile instead.</p> <p>When you change files or settings inside the container, <code>docker commit</code> command can be useful to commit them to a\u00a0new image. Do take note that, the processes within the container will be paused when the image is being committed.</p> Bash<pre><code>docker container commit &lt;container-id&gt; &lt;new-image-name&gt;\n# change the default command in container\ndocker container commit -a \"Author\" -c 'CMD [\"sleep\", \"5\"]' &lt;container-id&gt; &lt;new-image-name&gt;\n</code></pre> <p>The <code>--change</code> or <code>-c</code> option will apply <code>Dockerfile</code> instructions to the image that is created, it's supported <code>Dockerfile</code> instructions.</p> <ul> <li><code>CMD</code></li> <li><code>ENTRYPOINT</code></li> <li><code>ENV</code></li> <li><code>EXPOSE</code></li> <li><code>LABEL</code></li> <li><code>ONBUILD</code></li> <li><code>USER</code></li> <li><code>VOLUME</code></li> <li><code>WORKDIR</code></li> </ul>"},{"location":"docs/docker/network/","title":"Network","text":"<p>Info</p> <p>By default, Docker will create three networks automatically after installing Docker successfully.</p> <ul> <li>bridge</li> <li>host</li> <li>none</li> </ul> <p>There are two more network types;</p> <ul> <li>overlay - Multiple Docker Daemon hosts (Docker running on different machines) are able to connect with each other.</li> <li>macvlan - Set a custom MAC address to a container, this address can then be used for communication with that container.</li> </ul>"},{"location":"docs/docker/network/#network-types","title":"Network types","text":""},{"location":"docs/docker/network/#none","title":"None","text":"<p>No network is attached to the containers, and they have no access to any external network or other containers. As they run in an isolated network, there is no IP configuration for the containers.</p> Bash<pre><code>docker run --network=none ubuntu\n</code></pre>"},{"location":"docs/docker/network/#host-local","title":"Host (local)","text":"<p>Info</p> <p>container port 80 = host port 80</p> <p>The host network driver removes the network isolation between the Docker host and the Docker containers to use the host's networking directly. Therefore, the container won't get its own IP address.</p> <p>For instance, if you run a container which binds to port 80, the container's application will be available on port 80 on the host's IP address (localhost, IP). Therefore, you no need to specify <code>-p 80:80</code> for host networking, as that option is only for bridge network.</p> Bash<pre><code>docker run --network=host apiapp\n</code></pre>"},{"location":"docs/docker/network/#bridge","title":"Bridge","text":"<p>Info</p> <p>docker0 (172.17.0.1) is the bridge network, so all the bridge network will need to go through docker0 bridge (use <code>ip addr</code> to see more details)</p> <p>By default, the bridge driver network is attached to all container.</p> Bash<pre><code>docker run ubuntu # it uses Bridge network by default\n</code></pre> <p>Docker creates a private internal network called the bridge network on the host. The bridge network is attached to all containers by default and will get an internal IP address, normally in the <code>172.17</code> series range. Therefore, the containers can access each other using this internal IP.</p> <p>Additional information;</p> <ul> <li>Containers connected to the same bridge network can communicate using a software bridge while remaining isolated from containers not connected to it.</li> <li>Containers can find each other by name in the same network instead of using an IP address.</li> <li>It provides better isolation and interoperability between containerized applications.</li> <li>Environment variables are shared between linked containers on the bridge network.</li> </ul>"},{"location":"docs/docker/network/#user-defined-bridge-network","title":"User-defined bridge network","text":"<p>You can create your own internal network.</p> Bash<pre><code>docker network create --driver bridge --subnet 192.168.10.0/24 mynewinternalnetwork\n</code></pre> <p>With this setup, it will create a network with IP range of <code>192.168.10.0/24</code>. (192.168.10.1-192.168.10.254)</p>"},{"location":"docs/docker/network/#list-neworks","title":"List neworks","text":"Bash<pre><code>docker network ls\n</code></pre>"},{"location":"docs/docker/network/#inspect-networks","title":"Inspect networks","text":"Bash<pre><code>docker network inspect &lt;network-name&gt;\n</code></pre>"},{"location":"docs/docker/network/#remove-network","title":"Remove network","text":"Bash<pre><code>docker network rm &lt;network-name&gt;\ndocker network prune\n</code></pre>"},{"location":"docs/docker/network/#connectdisconnect-container-to-network","title":"Connect/Disconnect container to network","text":"Bash<pre><code>docker network connect &lt;network-name&gt; &lt;container&gt;\ndocker network disconnect &lt;network-name&gt; &lt;container&gt;\n</code></pre>"},{"location":"docs/docker/network/#container-communication","title":"Container Communication","text":""},{"location":"docs/docker/network/#container-to-host-local-communication","title":"Container to host (local) communication","text":"<p>If you want your container to communicate with the host, you have to replace the container's localhost or IP address to host.docker.internal. This domain will translate to the IP address of your localhost machine as seen from inside the Docker container.</p> <p>For example, now you have a Python API application that links to mongoDB. Here is the endpoint <code>mongodb://localhost:27017/user</code>. So, you have to change to <code>mongodb://host.docker.internal:27017/user</code>.</p>"},{"location":"docs/docker/network/#container-to-container-communication","title":"Container to container communication","text":"<p>There are two methods to let a container communicate with another container.</p> <ul> <li>Method 1 (not recommended)<ul> <li>Use <code>docker inspect &lt;container&gt;</code> to get the container IP address, then within your application code, put that specific IP address into it <code>mongodb://172.17.0.3:27017/user</code></li> </ul> </li> <li>Method 2<ul> <li>Use the container name as the input <code>mongodb://mymongodb:27017/user</code></li> <li>The container name refers to its IP address allocated by docker0.</li> <li></li> </ul> </li> </ul>"},{"location":"docs/docker/resource-limits/","title":"Resource Limits","text":""},{"location":"docs/docker/resource-limits/#cpu","title":"CPU","text":"<p>Reference</p> <p>Info</p> <p>By default, each container's access to the host machine's CPU cycles is unlimited. Docker us using default Completely Fair Scheduler (CFS), but it also supports Real-time Scheduler.</p>"},{"location":"docs/docker/resource-limits/#cpu-shares","title":"CPU Shares","text":"<p>By default, the Docker Host will assign a CPU share of 1024 to each container. You can modify that by using <code>--cpu-shares</code> option while creating the container.</p> Bash<pre><code>docker run --cpu-shares=&lt;amount&gt; &lt;image&gt;\ndocker run --cpu-shares=512 ubuntu\n</code></pre> <p><code>--cpu-shares</code> - It specifies how the host's total CPU resources are shared among its containers.</p>"},{"location":"docs/docker/resource-limits/#cpu-sets","title":"CPU Sets","text":"<p>It limits the specific CPUs or cores a container can use. A comma-separated list or hyphen-separated range of CPUs a container can use. Let's said that you have more than one CPU. Then the first cpu is numbered 0.</p> <ul> <li>1st CPU is numbered 0</li> <li>2nd CPU is numbered 1</li> </ul> <p>Value of 0-3 means, use the first, second, third, and fourth CPU. Value of 1,3 means, use the second, and fourth CPU.</p> Bash<pre><code>docker run --cpuset-cpus=&lt;value&gt; &lt;image&gt;\n\n# example\ndocker run --cpuset-cpus=0-1 ubuntu # this container can use first and second CPU\ndocker run --cpuset-cpus=1,3 ubuntu # this container can use second and fourth CPU\n</code></pre> <p><code>cpuset-cpus</code> - It specifies the CPU numbers to use.</p>"},{"location":"docs/docker/resource-limits/#cpu-count","title":"CPU Count","text":"<p>What happens when a container tries to consume more CPUs?</p> <p>Containers can consume all CPU resources available on the host, which can even affect other processes running on the host, such as other containers and Docker Daemons.</p> <p>It specifies how much of the available CPU resources a container can use. If host has 2 CPUs and if you set 1 CPU, then the container is guaranteed at most one CPU.</p> Bash<pre><code>docker run --cpus=&lt;value&gt; &lt;image&gt;\n\n# example\n# Let's say you have 4 CPUs, you specify 2.5, that means the container can only use as much as 2.5 out of 4.\ndocker run --cpus=2.5 ubuntu\n</code></pre>"},{"location":"docs/docker/resource-limits/#memory","title":"Memory","text":"<p>Reference</p> <p>We want to set the maximum amount of memory the container can use.</p> Bash<pre><code>docker run -m/--memory &lt;size&gt; &lt;image&gt;\n\n# example\ndocker run -m 512m ubuntu\ndocker run --memory=512m ubuntu\n</code></pre> <ul> <li>m = megabytes</li> <li>g = gigabytes</li> </ul> <p>So, setting <code>512m</code> will allocate 512MB of memory for the container. By default, the container will use the same amount of memory that is specified as swap space, that means when the container consumes more than 512MB, it can also consume another 512MB as swap space. Therefore, the container will get 1GB of memory, which effectively combines memory and swap space.</p> <p>Sum of memory = memory + swap space</p> <p>You can limit the container's use of swap space by specifying the <code>--memory-swap</code> option while creating the container.</p> Bash<pre><code>docker run --memory=&lt;size&gt; --memory-swap=&lt;size&gt; &lt;image&gt;\ndocker run --memory=512m --memory-swap=512m ubuntu\n</code></pre> <p>In this case, there is no swap space configured because the <code>--memory</code> and <code>--memory-swap</code> options are the same.</p> <p>Swap space = 512m - 512m = 0m If you want to allocate 256 megabytes for swap space, you have to set <code>--memory-swap=768m</code>.</p> Bash<pre><code>docker run --memory=512m --memory-swap=768m ubuntu\n</code></pre> <p>Swap space = 768m - 512m = 256m</p>"},{"location":"docs/docker/resource-limits/#additional-add-on","title":"Additional add-on","text":"Bash<pre><code>docker run --memory=512m --memory-swap=-1 ubuntu # unlimited memory swap(infinity)\n\n# Docker will ensure the memory allocated to this container\ndocker run --memory=512m --memory-swap=-1 --memory-reservation=100m ubuntu\n</code></pre> <p><code>--memory-reservation</code> = Allows you to specify a soft limit smaller than --memory which is activated when Docker detects contention or low memory on the host machine. It must be set lower than <code>--memory</code> for it to takje precedence. Because it is a soft limit, it doesn't guarantee that the container doesn't exceed the limit.</p>"},{"location":"docs/docker/useful-commands/","title":"Useful commands","text":""},{"location":"docs/docker/useful-commands/#docker-system-events","title":"Docker system events","text":"<p>Reference</p> <p>It will get real-time events that were logged from the server. So, all changes that are made to containers or networks are logged under this command.</p> Bash<pre><code>docker system events --since 60m\ndocker system events --since 2024-05-30\ndocker system events --filter '&lt;key&gt;=&lt;value&gt;'\ndocker system events --filter 'container=588a23dac085'\n</code></pre>"},{"location":"docs/docker/useful-commands/#disk-usage-metrics-for-docker-objects","title":"Disk usage metrics for docker objects","text":"<p>It will display the actual size of the different objects that are managed by Docker.</p> Bash<pre><code>docker system df\ndocker system df -v\n</code></pre>"},{"location":"docs/docker/useful-commands/#copy","title":"Copy","text":"<p>Info</p> <p>Make sure the directory exists for both side paths. You can copy entire folder too.</p> <p>Host to container</p> Bash<pre><code>docker container cp &lt;local-path&gt; &lt;container&gt;:&lt;container-path&gt;\ndocker container cp /home/username/config.yaml apiapp:/etc/api/config.yaml\n</code></pre> <p>Container to host</p> Bash<pre><code>docker container cp &lt;container&gt;:&lt;container-path&gt; &lt;local-path&gt;\ndocker container cp apiapp:/etc/api/config.yaml /home/username/config.yaml\n</code></pre>"},{"location":"docs/docker/volume/","title":"Volume","text":"<p>Info</p> <p>All the volumes are stored in <code>/var/lib/docker/volumes</code>. The volumes are mounted as read-write by default.</p> <p>Docker volume is used to persist and share the container's data across containers. Folders on your host machine's hard drive are mounted into containers as volumes, which allows the container to write its data into the host volumes.</p>"},{"location":"docs/docker/volume/#volume-types","title":"Volume types","text":""},{"location":"docs/docker/volume/#anonymous-and-named-volumes","title":"Anonymous and named volumes","text":"Volume Type Description Anonymous The volume will only exist as long as our container exists. Bind mount volumes can prevent some files from being overlapped. Named The volume persists even if you kill or close the container. With this volume, you can't edit the data inside the volume, instead you can use bind mounts for editing Bash<pre><code># Example of Anonymous volume\ndocker run -v /app/logs &lt;image&gt;\n\n# Example of Named volume\ndocker run -v mylog:/app/logs apiapp\ndocker run --mount source=mylog,destination=/app/logs apiapp\n</code></pre>"},{"location":"docs/docker/volume/#bind-mounts","title":"Bind mounts","text":"<p>Reference</p> <p>The source and the destination must be absolute path</p> <p>The use of bind mounts is good for development, but not for production since production might not have the same folder structure as development. To make managing different environments easier, I recommend the container has the same structure as your local.</p> <ul> <li>macOS / Linux = <code>$(pwd):/app</code></li> <li>Windows = <code>\"%cd%\":/app</code></li> </ul> <p>Bind mounts allow you to edit or reflect your code changes from local to container. Bind mount volumes can prevent some files from being overlapped. It's great for persistent.</p> Bash<pre><code>docker run -v &lt;local-path&gt;:&lt;container-path&gt; &lt;image&gt;\ndocker run --mount type=&lt;type&gt;,source=&lt;local-path&gt;,destination=&lt;container-path&gt; &lt;image&gt;\n\n# example\ndocker run -v /app/api:/app/api myapi\ndocker run --mount type=bind,source=/app/api,destination=/app/api myapi\n</code></pre>"},{"location":"docs/docker/volume/#read-only-volume","title":"Read-only volume","text":"<p>Read-only volume will prevent the local files from being modified when the files within the container have been modified.</p> Bash<pre><code>docker run --mount type=&lt;type&gt;,source=&lt;local-path&gt;,target=&lt;container-path&gt;,readonly &lt;image&gt;\ndocker run -v \"&lt;local-path&gt;:&lt;container-path&gt;:ro\"\n\n# example\ndocker run --mount type=bind,source=/root/logs,target=/api/logs,readonly myapi\ndocker run -v \"/root/logs:/api/logs:ro\"\n</code></pre> <p><code>readonly</code> and <code>:ro</code> are to indicate that this volume is only for reading</p>"},{"location":"docs/docker/volume/#list-volume","title":"List volume","text":"Bash<pre><code>docker volume ls\n</code></pre>"},{"location":"docs/docker/volume/#create-volume","title":"Create volume","text":"Bash<pre><code>docker volume create &lt;volume-name&gt;\n</code></pre>"},{"location":"docs/docker/volume/#inspect-volume","title":"Inspect volume","text":"Bash<pre><code>docker volume inspect &lt;volume-name&gt;\n</code></pre>"},{"location":"docs/docker/volume/#remove-volume","title":"Remove volume","text":"Bash<pre><code>docker volume rm &lt;volume-name&gt;\ndocker volume remove &lt;volume-name&gt;\ndocker volume prune\n</code></pre>"},{"location":"docs/docker/what-is-docker/","title":"What is Docker?","text":"<p>Docker is an open platform for developing, shipping, and running applications using containers.</p> <p>\u2014 docker.docs</p> <p>Consider the case where you build a Python application that contains a wide range of dependencies (legacy and latest). This Python application needs to be shipped to the production environment on different computers with different operating systems. You have to ensure that every computer has the same configurations as your local. Traditionally, doing it manually takes time and sometimes you may run into issues with different operating systems.</p> <p>With Docker, you can easily manage your infrastructure like you manage your applications, because it will package all the application source code as well as the tools, libraries, settings, runtime, and dependencies that your application needs into a container. So, you can deploy your Python application with the same configuration and settings into each computer.</p>"},{"location":"docs/git/branches/","title":"Branches","text":""},{"location":"docs/git/branches/#what-is-git-branch","title":"What is Git Branch?","text":"<p>Info</p> <p>Default branch will be created either master or main depends on the settings.</p> <p>Branch is an independent line of development work. As an example, if you are working on a large project and you want to add a new feature, you can't commit all your changes straight away and push them to the default branch, since that would make the commit history pretty messy and it would be hard to tell which changes were made.</p> <p>Regardless of how big or small your changes are, always create a branch and work on it to encapsulate your changes.</p> <p>All new commits you make to that branch will be recorded in that branch as well.</p>"},{"location":"docs/git/branches/#git-branch-commands","title":"Git branch commands","text":""},{"location":"docs/git/branches/#create-branch","title":"Create branch","text":"<pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    commit id: \"docs: create third story\"\n    branch develop\n    checkout develop\n    commit</code></pre> Bash<pre><code>git branch &lt;branch-name&gt;\n</code></pre>"},{"location":"docs/git/branches/#checkoutswitch-branch","title":"Checkout/Switch branch","text":"<p>Info</p> <p>Remember that the HEAD is Git's way of referring to the current snapshot</p> <p>Once we have created the branch, we have to switch or checkout to that branch.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    commit id: \"docs: create third story\"\n    branch develop\n    checkout develop\n    commit id: \"docs: edit first story\"\n    commit id: \"docs: edit second story\"</code></pre> <ul> <li>the default branch can still proceed with the commits even though we have checkout to develop branch</li> </ul> Bash<pre><code>git checkout &lt;branch-name&gt;\n# if the branch name is not exists, it will auto create and checkout to that branch\ngit checkout -b &lt;branch-name&gt;\n</code></pre> <p></p> <p>Watch animation</p>"},{"location":"docs/git/branches/#merge-branch-fast-forward","title":"Merge branch (Fast-forward)","text":"<p>The <code>git merge</code> command will combine two branches, so it will consolidate multiple commit sequences into a single history.</p> <p>Here is the scenario, since develop branch came directly from main and no other changes had been made to main, so it can fast-forward.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    branch develop\n    checkout develop\n    commit id: \"docs: edit first story\"\n    commit id: \"docs: edit second story\"\n    checkout main\n    commit id: \"docs: create programming\"\n    merge develop</code></pre> Bash<pre><code>git merge &lt;branch&gt;\n\n# example, if you want to merge develop branch to main\ngit checkout main\ngit merge develop\n</code></pre> <p></p> <p>Watch animation</p>"},{"location":"docs/git/branches/#merge-conflicts","title":"Merge conflicts","text":"<p>A merge conflict will arise when two developers have changed the same lines in a file. Because of that, Git does not know which one is correct.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    branch develop\n    checkout develop\n    commit id: \"docs: edit first story\"\n    commit id: \"docs: edit second story\"\n    checkout main\n    commit id: \"docs: create programming\"\n    commit id: \"docs: set title to first story\"\n    merge develop\n    commit id: \"merged with develop after fixing conflicts\"</code></pre> <p>To resolve merge conflict, you have to open the file to make necessary changes. Then, use <code>git add</code> command to stage the new merged commit and the final step is to merge your latest commits into it.</p> Bash<pre><code>git add &lt;file-that-has-conflicts&gt;\ngit commit -m \"merged with &lt;branch&gt; after fixing conflicts\"\n</code></pre> <p></p>"},{"location":"docs/git/branches/#list-branches","title":"List branches","text":"Bash<pre><code>git branch\ngit branch -a # list all local and remote branches\n</code></pre>"},{"location":"docs/git/branches/#delete-branch","title":"Delete branch","text":"Bash<pre><code>git branch -d &lt;branch-name&gt; # delete branch with safe operation\ngit branch -D &lt;branch-name&gt; # force delete branch\n</code></pre>"},{"location":"docs/git/branches/#rename-branch","title":"Rename branch","text":"Bash<pre><code>git branch -m &lt;branch-name&gt; &lt;new-branch-name&gt;\ngit branch -M &lt;branch-name&gt; &lt;new-branch-name&gt; # force rename branch\n</code></pre>"},{"location":"docs/git/branches/#create-remote-branches-and-push","title":"Create remote branches and push","text":"<p>Before we want to push our local branches to our remote repository. The remote repository needs to be added to our local project if it hasn't already been. Normally the remote name is origin, but you can change it.</p> Bash<pre><code>git remote add &lt;remote-name&gt; &lt;remote-repo-url&gt;\n# example\ngit remote add origin https://github.com/KarChunT/git-training\n</code></pre> <p>Once you initialize, you have access to this remote repository. So you can push your local branch to the remote at remote name.</p> <ul> <li>The reason why we need to set-upstream from local to remote is because it makes our jobs easier when we want to perform push and pull operation. With an upstream branch set, you can simply <code>git pull</code> or <code>git push</code> instead of <code>git push origin &lt;branch&gt;</code></li> </ul> <p>What is upstream?</p> <p>An upstream is just a remote tracking branch that is associated with your local branch. Do take note that, each branch only has one upstream.</p> Bash<pre><code># set-upstream, push the local branch to the remote at remote name\ngit push -u &lt;remote-name&gt; &lt;local-branch-name&gt;\ngit push &lt;branch-name&gt; # after you set up-stream, branch-name is optional\ngit push --force # force push, will overwrite the things in remote\n\n# example\ngit push -u origin develop\n</code></pre> <p>You can use the following command:</p> <ul> <li>if the remote branch already exists or</li> <li>you want to change upstream branch</li> </ul> Bash<pre><code>git branch -u origin/&lt;branch-name&gt;\n</code></pre>"},{"location":"docs/git/git-in-depth/","title":"Git in Depth","text":""},{"location":"docs/git/git-in-depth/#a-real-git","title":"A real Git","text":"<p>Git is a key-value store. A file is hashed using the SHA-1 algorithm when it's added to a commit. This hash is then used as a key name for the folder and used to store the file.</p> <p>Git has Porcelain and Plumbing commands</p> <ul> <li>Porcelain Commands (easy to remember)</li> <li>example: git add, status, commit, stash, etc</li> <li>Plumbing Commands (get access to Git internals)</li> <li>example: git hash-object, cat-file, ls-files, rev-parse, ls-remote, etc</li> </ul>"},{"location":"docs/git/git-in-depth/#hash-object","title":"hash-object","text":"<p>The <code>git hash-object</code> command hashes the file contents.</p> Bash<pre><code>git hash-object &lt;filename&gt;\n# example\ngit hash-object hello\n</code></pre> <p></p> <p></p> <p>As you can see, the key is made up of the first two characters of the hash. The contents of this file (hello) will be stored in this hash file 492f47831b16c8217339fcb1449345b424c72fcb.</p> <p>To see the hash file contents, you have to use git cat-file command.</p>"},{"location":"docs/git/git-in-depth/#cat-file","title":"cat-file","text":"<p>The <code>git cat-file</code> command will display the contents of the hash file by using the hash value.</p> Bash<pre><code>git cat-file -p &lt;object-hash&gt;\n# example\ngit cat-file -p 492f47831b16c8217339fcb1449345b424c72fcb # pretty prints the contents\n</code></pre>"},{"location":"docs/git/git-in-depth/#git-object-contents","title":"Git Object Contents","text":"<p>There are three types of folders in the object folder.</p> <ol> <li>commit<ul> <li>A commit is simply just a commit hash.</li> </ul> </li> <li>tree<ul> <li>It's a folder associated with the repository on your file system.</li> </ul> </li> <li>blob<ul> <li>It's just a piece of data.</li> <li>For example, the file that hashed (hello) is called a blob.</li> </ul> </li> </ol>"},{"location":"docs/git/git-installation/","title":"Git Installation","text":"<p>Reference</p> <p>Git Installation</p>"},{"location":"docs/git/git-installation/#install-git-on-windows","title":"Install Git on Windows","text":"<ol> <li> <p>Go to this link: Download for Windows</p> </li> <li> <p>Select your installer, for my case, I'm selecting 64-bit Git for Windows Setup.</p> <p></p> </li> <li> <p>Open the downloaded exe file and follow the instructions. I put all options as default.</p> <p></p> </li> <li> <p>After you have done the installation. Open your terminal and enter the following command.</p> Bash<pre><code>git --version\n</code></pre> <p>It will show the latest Git version that you have installed.</p> </li> <li> <p>Configure your Git username and email. Remember to replace the name and email with your own. Any commits you make will include these details.</p> Bash<pre><code>git config --global user.name \"Kar Chun\"\ngit config --global user.email \"karchuntan.1999@gmail.com\"\n</code></pre> </li> </ol>"},{"location":"docs/git/git-installation/#install-git-on-ubuntulinux","title":"Install Git on Ubuntu/Linux","text":"<p>The typical way of installing Git, will not install the latest version. With the following commands, it will help you to install the latest Git.</p> <p>More Information</p> <p>PPA Git</p> <ol> <li> <p>Install stable Git version</p> Bash<pre><code>sudo add-apt-repository ppa:git-core/ppa\nsudo apt-get update\nsudo apt-get install -y git\n</code></pre> </li> <li> <p>Check Git version</p> Bash<pre><code>git --version\n</code></pre> </li> <li> <p>Configure your Git username and email. Remember to replace the name and email with your own. Any commits you make will include these details.</p> Bash<pre><code>git config --global user.name \"Kar Chun\"\ngit config --global user.email \"karchuntan.1999@gmail.com\"\n</code></pre> </li> </ol>"},{"location":"docs/git/rebasing/","title":"Rebasing","text":""},{"location":"docs/git/rebasing/#rebase","title":"Rebase","text":"<p>Differences between Merge and Rebase</p> Merge Rebase Unique identifier (hash) won't be modified as we are not modifying the git commits history Unique identifier (hash) will be updated, as we are modifying the Git history when we are rebasing (copying the commits from one branch to another) the branches <p>When Developer A has been working on his own story in his own branch. Meanwhile, his team continues to contribute additional stories and updates to the main branch. The Developer A wants to make sure that his branch is up to date with the latest changes from the main branch.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    branch develop\n    checkout develop\n    commit id: \"docs: create third story\"\n    commit id: \"docs: edit first story\"\n    commit id: \"docs: edit second story\"\n    checkout main\n    commit id: \"docs: create programming\"\n    checkout develop\n    merge main id: \"Merged main into Develop\"</code></pre> <p>To do this, the Developer A would need to merge the main branch into his branch, but that will create a new merge commit and the Developer A doesn't want to do that.</p> Bash<pre><code>git merge main\n</code></pre> <p>The other way to accomplish this is by rebasing branches. When we rebase a branch, we are layering/putting it on top of another. That means, we can rebase develop branch on top of the main branch, so now the develop branch will have all the changes that were made to the main branch.</p> Bash<pre><code>git checkout develop\ngit rebase main\n</code></pre> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\n%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': { 'mainBranchName': 'develop' }} }%%\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    commit id: \"docs: create programming\"\n    commit id: \"docs: create third story\"\n    commit id: \"docs: edit first story\"\n    commit id: \"docs: edit second story\"</code></pre> <p></p> <p>Watch animation</p>"},{"location":"docs/git/rebasing/#interactive-rebasing","title":"Interactive rebasing","text":"<p>The <code>git rebase</code> command also allows us to modify the Git history on a certain branch before rebasing it.</p> <p>Let's just assume the Developer A is working on his branch (main), but there are many commits that should actually have been just one commit. To do that, we can use <code>git rebase -i</code> command to combine/squash/meld into one commit.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    commit id: \"docs: create programming\"\n    commit id: \"docs: create same story 1\"\n    commit id: \"docs: edit same-story\"\n    commit id: \"docs: more changes to same-story\"\n    commit id: \"docs: more more changes to same-story\"</code></pre> <p>Let's modify the latest 4 commits, as now we want to combine/squash/meld the latest 3 commits into the 1st commit.</p> Bash<pre><code>git rebase -i HEAD~4\n</code></pre> <p></p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    commit id: \"docs: create programming\"\n    commit id: \"squashed latest 3 commits into here\"</code></pre>"},{"location":"docs/git/rebasing/#cherry-pick","title":"Cherry-pick","text":"<p>In some cases, you may want to apply a certain change made to one branch to another branch without applying the complete changes made to that branch. By cherry-picking that particular commit, we can create a copy of it on your own branch.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    branch develop\n    checkout develop\n    commit id: \"docs: create third story\"\n    commit id: \"docs: edit first story\"\n    commit id: \"docs: edit second story\"\n    checkout main\n    commit id: \"docs: create programming\"</code></pre> <p>For example, let's say that the Developer A is on the main branch and he want to add the changes that were made to the \"docs: edit second story\" commit on the develop branch. In that case, we can cherry-pick that commit and it will only take that commit changes.</p> Bash<pre><code>git cherry-pick &lt;commit-id&gt;\ngit cherry-pick &lt;docs: edit second story-commit-id&gt;\n</code></pre> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    branch develop\n    checkout develop\n    commit id: \"docs: create third story\"\n    commit id: \"docs: edit first story\"\n    commit id: \"docs: edit second story\"\n    checkout main\n    commit id: \"docs: create programming\"\n    cherry-pick id: \"docs: edit second story\"</code></pre> <ul> <li>the cherry-pick: \"docs: edit second story\" is the copy of that commit (docs: edit second story) on develop branch. We now have a copy of that commit on the main branch.</li> <li>Do take note that, it will only get the edit second story changes</li> </ul> <p>Steps to perform cherry-pick:</p> <ol> <li>identified the commit</li> <li>checkout the branch that you want to fetch other branch commit to your branch</li> <li>perform cherry-pick command</li> </ol> Bash<pre><code>git log &lt;branch-name&gt; --oneline # remember to commit hash, example 123\ngit checkout main\ngit cherry-pick &lt;commit-id&gt; # 123\n</code></pre> <ul> <li>It will straight away merge the file and create a new commit on top of the current commit.</li> <li>If you end up in merge conflict, merge the file manually and make sure to leave the latest contents in the file itself.</li> </ul> <p></p> <p>Watch animation</p>"},{"location":"docs/git/reflog/","title":"Reflog","text":""},{"location":"docs/git/reflog/#git-reflog","title":"Git Reflog","text":"<p>The <code>git reflog</code> command shows us all the actions that have been taken on our repository. Everyone will make mistake and if you make a mistake, you can simply undo that change by resetting your HEAD based on the information (commit-id) that reflog provides to us.</p> Bash<pre><code>git reflog\ngit reset --hard &lt;commit-id&gt;\n</code></pre> <p></p>"},{"location":"docs/git/remote-repositories/","title":"Remote repository","text":""},{"location":"docs/git/remote-repositories/#configuration-and-setup-git-config","title":"Configuration and setup Git config","text":"<p>You can refer more details in Create remote branches and push section.</p> Bash<pre><code>git remote add &lt;remote-name&gt; &lt;remote-repo-url&gt;\ngit push -u &lt;remote-name&gt; &lt;local-branch-name&gt;\n</code></pre>"},{"location":"docs/git/remote-repositories/#fetching-and-pulling","title":"Fetching and pulling","text":"Fetch Pull Only changes are copied into your local Git repository and does not reflect the changes It copies changes from a remote repository directly into your working directory and reflect the changes. Fetch just fetch pull = fetch + merge Bash<pre><code># one way\ngit fetch origin &lt;branch-name&gt;\ngit merge origin/&lt;branch-name&gt;\n\n# another way\ngit pull origin &lt;branch-name&gt; # or just git pull\n</code></pre>"},{"location":"docs/git/remote-repositories/#fetching","title":"Fetching","text":""},{"location":"docs/git/remote-repositories/#pulling","title":"Pulling","text":"<p>Watch animation</p>"},{"location":"docs/git/remote-repositories/#list-all-remote-repositories","title":"List all remote repositories","text":"Bash<pre><code>git remote -v\n</code></pre>"},{"location":"docs/git/reset-and-revert/","title":"Reset and Revert","text":"<p>Sometimes we commit changes that we don't intend to commit. If that is the case, we have several options to undo the action.</p> <ul> <li>Revert</li> <li>Reset<ul> <li>soft</li> <li>hard</li> <li>mixed</li> </ul> </li> </ul>"},{"location":"docs/git/reset-and-revert/#revert","title":"Revert","text":"<p>Info</p> <p>If you want to undo changes and keep them in your Git history, you can use the <code>git revert</code> command</p> <p>The <code>git revert</code> command will create a new commit, which reverses all the changes made on the specified commit. Let's say that we want to revert the commit where we create the <code>test</code> file.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n  commit id: \"docs: create first story\"\n  commit id: \"docs: create second story\"\n  commit id: \"docs: create programming\"\n  commit id: \"docs: create test\"</code></pre> Bash<pre><code>git revert &lt;docs: create test-commit-id&gt;\n</code></pre> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n  commit id: \"docs: create first story\"\n  commit id: \"docs: create second story\"\n  commit id: \"docs: create programming\"\n  commit id: \"docs: create test\"\n  commit id: \"Revert docs: create test\"</code></pre> <ul> <li>In the \"docs: create test\" commit, <code>test</code> file has created. The reverted commit contains all the opposite changes. It will then deletes the <code>test</code> file.</li> </ul> <p></p> <p>Watch animation</p>"},{"location":"docs/git/reset-and-revert/#reset","title":"Reset","text":"<p>The another way is to use the <code>git reset</code> command. There are three ways to reset the commit to undo it.</p> <ul> <li>soft</li> <li>hard</li> <li>mixed</li> </ul>"},{"location":"docs/git/reset-and-revert/#soft","title":"soft","text":"<p>With the <code>--soft</code> flag, we still have all the changes that we have made, and these changes will move to staged changes. You can use <code>git status</code> to check all the staged changes.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n  commit id: \"docs: create first story\"\n  commit id: \"docs: create second story\"\n  commit id: \"docs: create programming\"\n  commit id: \"docs: create test\"\n  commit id: \"docs: create hello\"</code></pre> <p>Let's reset the \"docs: create hello\" commit with a soft flag. Reset commands also receive the number of commits that we want to reset, in this case we want to reset one commit.</p> Bash<pre><code>git reset --soft &lt;commit-id&gt;\ngit reset --soft HEAD~1\n</code></pre> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n  commit id: \"docs: create first story\"\n  commit id: \"docs: create second story\"\n  commit id: \"docs: create programming\"\n  commit id: \"docs: create test\"</code></pre> <p></p> <p>Watch animation</p>"},{"location":"docs/git/reset-and-revert/#hard","title":"hard","text":"<p>With the <code>--hard</code> flag, we will lose all the changes that we have made on that commit.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n  commit id: \"docs: create first story\"\n  commit id: \"docs: create second story\"\n  commit id: \"docs: create programming\"\n  commit id: \"docs: create test\"\n  commit id: \"docs: create hello\"</code></pre> <p>Let's reset the \"docs: create hello\" commit with a hard flag.</p> Bash<pre><code>git reset --hard &lt;commit-id&gt;\ngit reset --hard HEAD~1\n</code></pre> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n  commit id: \"docs: create first story\"\n  commit id: \"docs: create second story\"\n  commit id: \"docs: create programming\"\n  commit id: \"docs: create test\"</code></pre> <p></p> <p>Watch animation</p>"},{"location":"docs/git/reset-and-revert/#mixed","title":"mixed","text":"<p>With the <code>--mixed</code> flag, we still have all the changes that we have made, and these changes will move to unstaged changes. You can use <code>git status</code> to check all the unstaged changes.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n  commit id: \"docs: create first story\"\n  commit id: \"docs: create second story\"\n  commit id: \"docs: create programming\"\n  commit id: \"docs: create test\"\n  commit id: \"docs: create hello\"</code></pre> <p>Let's reset the \"docs: create hello\" commit with a mixed flag.</p> Bash<pre><code>git reset --mixed &lt;commit-id&gt;\ngit reset --mixed HEAD~1\n</code></pre> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n  commit id: \"docs: create first story\"\n  commit id: \"docs: create second story\"\n  commit id: \"docs: create programming\"\n  commit id: \"docs: create test\"</code></pre> <p></p> <p>Web animation</p>"},{"location":"docs/git/save-changes/","title":"Save Changes","text":"<p>Warning</p> <p>Please follow the sequence.</p> <p>Clone the repository KarChunT/git-training to work-on.</p>"},{"location":"docs/git/save-changes/#git-add","title":"git add","text":"<p>The <code>git add</code> command simply pushes files or directories to staging area/environment. By doing this, you tell Git to include those changes in the next commit. Of course, you can add more than one file/directory at a time.</p> Bash<pre><code>git add &lt;file/directory&gt;\ngit add . # all files and directories\n\n# example\ntouch third-story programming # create new files\ngit add third-story\n</code></pre> <p></p> <p>Watch animation</p>"},{"location":"docs/git/save-changes/#git-status","title":"git status","text":"<p>The <code>git status</code> command simply lists which files are staged, unstaged, and untracked.</p> Bash<pre><code>git status\n</code></pre> <p></p>"},{"location":"docs/git/save-changes/#git-commit","title":"git commit","text":"<p>The <code>git commit</code> command simply saves or commits that file changes into a Git project. You can think of as snapshots or milestones along the timeline of a Git project.</p> <pre><code>gitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"</code></pre> Bash<pre><code>git commit # opens a new editor\ngit commit -a # automatically stage files that have been modified and deleted\ngit commit -m \"commit message\" # flag of the message\ngit commit -am \"commit message\" # combination of am\ngit commit -s # user signed-off, certifies who is the author of the commit, tracking for patches\ngit commit --amend # modify the last commit\n\n# example\ngit commit -m \"docs: create third story\"\n</code></pre> <p> Watch animation</p>"},{"location":"docs/git/save-changes/#git-log","title":"git log","text":"<p>The <code>git log</code> command shows the information that you need to know about all the commits, such as</p> <ul> <li>commit hash</li> <li>author name</li> <li>data of the commit: which files to be committed</li> <li>commit message</li> </ul> Bash<pre><code>git log\ngit log --oneline # don't care about all extra information\ngit log --name-only # list the changed files\ngit log --graph --decorate\n</code></pre> <p></p>"},{"location":"docs/git/save-changes/#git-diff","title":"git diff","text":"<p>The <code>git diff</code> command shows the differences between two data sources. Data sources can be</p> <ul> <li>commits</li> <li>branches</li> <li>files, etc</li> </ul> Bash<pre><code>git diff # comparing all changes\ngit diff &lt;file&gt;\ngit diff HEAD &lt;filename&gt; # filename is optional\ngit diff --staged/--cached &lt;filename&gt; # only for staged changes, filename is optional\ngit diff &lt;commit_hash&gt; &lt;commit_hash&gt;\ngit diff &lt;branch&gt; &lt;branch&gt;\ngit diff &lt;branch&gt; &lt;branch&gt; &lt;file&gt; # comparing files from two branches\ngit diff --color-words # Highlighting changes with much better granularity\n\n# Example\nnano first-story # write some texts into it\ngit diff\n</code></pre> <p></p> <p>How to read this?</p> Text Only<pre><code>@@ -50,8 +50,12 @@\n</code></pre> <p>In this example, 8 lines have been extracted starting from line number 50. Then, 12 lines have been added starting at line number 50.</p>"},{"location":"docs/git/save-changes/#git-restore","title":"git restore","text":"<p>The <code>git restore</code> command will discard changes in working directory or if a file is tracked then you can restore or unstage that file to match the version in HEAD.</p> Bash<pre><code># discard changes\ngit restore . # all files\ngit restore &lt;file&gt; # same as git checkout &lt;file&gt;\ngit restore &lt;pattern&gt; # pattern = '*.c'\n\n# unstage\ngit add first-story\ngit restore --staged &lt;file&gt; # same as git reset HEAD &lt;file&gt;\n\n## unstage all files\ngit restore --staged .\ngit reset\n\n# restore both index and working tree ---&gt; same as git checkout\ngit restore --source=HEAD --staged --worktree &lt;file&gt;\n\n# Example\ngit restore first-story\ngit restore --staged second-story\n# even it's staged, it will restore all changes to same as HEAD commit\ngit restore --source=HEAD --staged --worktree second-story\n</code></pre> <p></p> <p>Watch animation</p>"},{"location":"docs/git/save-changes/#gitignore","title":".gitignore","text":"<p>Info</p> <p>Sources</p> <ul> <li>https://git-scm.com/docs/gitignore</li> <li>https://github.com/github/gitignore</li> <li>https://www.toptal.com/developers/gitignore</li> <li>https://www.atlassian.com/git/tutorials/saving-changes/gitignore</li> </ul> <p><code>.gitignore</code> is a file that will include all the globbing patterns to ignore the file to be commited. These could be the files like</p> <ul> <li>build artifacts like <code>/bin</code>, <code>/target</code></li> <li>machine generated files like <code>.pyc</code></li> <li>dependency caches like <code>node_modules</code></li> <li>log file</li> <li>environments or secrets like <code>.env</code></li> </ul> .gitignore<pre><code>*.log\n.env*\ntest.txt\nnode_modules/\n/bin\n</code></pre>"},{"location":"docs/git/setup-repository/","title":"Setup repository","text":""},{"location":"docs/git/setup-repository/#initialize-a-new-git-repo","title":"Initialize a new Git repo","text":"<p>To create a repo, you will use the <code>git init</code> command to initialize a new git repository. Do take note that <code>git init</code> is just a one-time command process. You will only use it when you want to initialize a new Git repo.</p> <ol> <li> <p>Create a folder</p> Bash<pre><code>mkdir project\ncd project\n</code></pre> </li> <li> <p>Initialize a Git repository       Bash<pre><code>git init\ngit init &lt;directory&gt; # initialize/point to an existing directory\n</code></pre></p> <ul> <li>As a result, Git knows that the folder you initiated it on should be monitored.</li> <li>It will also create a hidden folder named <code>.git/</code> to keep track of all the changes you made.</li> </ul> </li> </ol>"},{"location":"docs/git/setup-repository/#clone-an-existing-repo","title":"Clone an existing repo","text":"<p>If your project already been setup on centralized server like GitHub, Bitbucket, etc. Then you have to clone that remote repository to your local. Do take note that <code>git clone</code> is also just a one-time command process. You will only use it when you want to clone an existing repository.</p> Bash<pre><code>git clone &lt;repo_url&gt;\ngit clone https://github.com/KarChunT/karchunt.com\n</code></pre> <p></p>"},{"location":"docs/git/setup-repository/#clone-to-a-specific-folder","title":"Clone to a specific folder","text":"Bash<pre><code>git clone &lt;repo_url&gt; &lt;directory&gt;\ngit clone https://github.com/KarChunT/karchunt.com karchunt\n</code></pre>"},{"location":"docs/git/setup-repository/#clone-a-specific-branch","title":"Clone a specific branch","text":"Bash<pre><code>git clone --branch &lt;branch&gt; &lt;repo_url&gt;\ngit clone --branch development https://github.com/KarChunT/karchunt.com\n</code></pre>"},{"location":"docs/git/stashing/","title":"Stashing","text":""},{"location":"docs/git/stashing/#git-stash","title":"Git Stash","text":"<p>Let's say that the Developer A is working on a third story on the develop branch. Suddenly, the Developer B says that there are some critical bugs that needs to be corrected immediately on the main branch.</p> <p>Before the Developer A checkout to the main branch, it's necessary to place the changes made to the third story somewhere in order to have a clean working area if we want to solve the bugs on the main branch. As the story isn't finished yet, the Developer A don't want to commit it to his branch (develop).</p> <p>In this case, we can use <code>git stash</code> command to stash all changes in the working area. We can keep on pushing the changes to the stash and the changes will simlpy pile up (treat like queue FIFO).</p> Bash<pre><code># Stash the changes\ngit stash\ngit stash push\ngit stash save \"message\"\n\n# Apply the stash changes\ngit stash apply # re-applying the stashed changes without pop it\ngit stash pop\ngit stash pop stash@{1}\ngit stash branch branch-name stash@{1} # creating a branch from your stash, as you may run into conflicts when popping or applying your stash\n\n# List all the stash\ngit stash list\n\n# Show the differences between the current and a stash\ngit stash show\ngit stash show -p # view the full diff of a stash\n\n# clear or drop stash\ngit stash clear # clear all the stashes\ngit stash drop stash@{1}\n</code></pre>"},{"location":"docs/git/stashing/#stash-push","title":"stash push","text":"<p>Watch animation</p>"},{"location":"docs/git/what-is-git/","title":"What is Git?","text":""},{"location":"docs/git/what-is-git/#git-introduction","title":"Git Introduction","text":"<p>Git is a distributed version control system that tracks</p> <ul> <li>file changes</li> <li>who made changes</li> </ul>"},{"location":"docs/git/what-is-git/#local-vs-remote-repositories","title":"Local vs Remote repositories","text":"<p>Git has two repository types, local and remote.</p> Local Remote You can access the local repository directly from your own machine Repository is usually located on centralized server. For example, GitHub, Bitbucket, etc"},{"location":"docs/kubernetes/4c-cloud-native-k8s-security/","title":"4C's of Cloud-Native Kubernetes Security","text":""},{"location":"docs/kubernetes/4c-cloud-native-k8s-security/#overview-of-the-4cs","title":"Overview of the 4C's","text":"<pre><code>graph TD\n  A[Cloud] --&gt; B[Data Center]\n  A --&gt; C[Network]\n  A --&gt; D[Servers]\n  B --&gt; E[Cluster]\n  C --&gt; E\n  D --&gt; E\n  E --&gt; F[Authentication]\n  E --&gt; G[Authorization]\n  E --&gt; H[Admission]\n  E --&gt; I[Network Policy]\n  F --&gt; J[Container]\n  G --&gt; J\n  H --&gt; J\n  I --&gt; J\n  J --&gt; K[Restrict Images]\n  J --&gt; L[Supply Chain]\n  J --&gt; M[Sandboxing]\n  J --&gt; N[Privileged]\n  K --&gt; O[Code]\n  L --&gt; O\n  M --&gt; O\n  N --&gt; O\n  O --&gt; P[Code Security Best Practices]</code></pre> <p>4C's of Cloud-Native Kubernetes Security are:</p> <ul> <li>Cloud - Security of the cloud infrastructures that hosting the Kubernetes clusters. This could be a public or private cloud, data center. Also, the network firewall must be configured properly so that can prevent unauthorized remote access to the Kubernetes cluster.</li> <li>Cluster - Security of the Kubernetes cluster involves managing authentication, authorization, admission controls, and network policies. With proper configuration of these components, we can secure the Kubernetes cluster or docker daemon.</li> <li>Container - Security of the containers involves restricting the use of untrusted images, securing the supply chain, utilizing sandboxing containers (isolate your program from the rest of the system using lightweight virtual machines which start containers inside pods) for isolation, and avoiding the use of privileged containers.</li> <li>Code - Code refers to the application code that is running inside the containers. So, the developers must follow the code security best practices to prevent the vulnerabilities in the code.</li> </ul>"},{"location":"docs/kubernetes/4c-cloud-native-k8s-security/#cloud-provider-security","title":"Cloud provider security","text":"<pre><code>flowchart TD\n  A[\"Sources\"] --&gt; B\n  subgraph B[\"Cloud Infrastructure\"]\n    subgraph C[\"Firewall\"]\n      D[\"Application\"]\n    end\n  end</code></pre> <p>Cloud provider is responsible for the security of the cloud and physical infrastructure like hardware, while the customer is responsible for managing network and firewall settings.</p> <p>To secure the cloud provider,</p> <ul> <li>Activate the network firewall - It can permit or deny the traffic based on the source and destination IP address, port, and protocol, and can be used to prevent unauthorized access to the system.</li> <li>Shared Responsibility Model - The Shared Responsibility Model outlines the division of security responsibilities between the cloud provider and the customer. <ul> <li>Cloud provider is responsible for the security of the cloud and physical infrastructure such as hardware, software, networking, and facilities.</li> <li>Customer is responsible for managing network and firewall settings, as well as securing their data, applications, identity and access management, and operating systems.</li> </ul> </li> <li> <p>Cloud Provider Security Capabilities - Cloud providers offer security capabilities to enhance cloud security. It can be categorized into Thread Detection, Application Firewall, and Container Security.</p> <ul> <li> <p>Threat Management &amp; Response Techniques</p> Provider Tool Description AWS GuardDuty Uses machine learning to detect threats GCP Security Command Center Offers a centralized dashboard to monitor therats, security health, etc. Azure Microsoft Sentinel It combines SIEM (Security Information and Event Management) and SOAR (Security Orchestration, Automation, and Response). It will auto detect security threats and deal with them. </li> <li> <p>Application Firewall</p> Provider Tool Description AWS AWS WAF Web Application Firewall (WAF) helps protect the web applications from common web exploits like SQL injection, cross-site scripting (XSS), etc GCP Cloud Armor DDoS protection and other common attacks Azure Azure WAF Web Application Firewall (WAF) helps protect the web applications from common web exploits like SQL injection, cross-site scripting (XSS), etc </li> <li> <p>Container Security</p> Provider Tool AWS EKS, Bottlerocket, kube-bench to meet **Center for Internet Security (CIS) benchmarks GCP GKE, Google's Anthos, Open Policy Agent (OPA) to align with security standards** Azure AKS </li> </ul> </li> </ul>"},{"location":"docs/kubernetes/4c-cloud-native-k8s-security/#cluster-security","title":"Cluster Security","text":"<pre><code>graph\n  A[Isolate the applications]\n  B[Use firewall rules and policies on Docker ports]\n  C[Apply least privilege to container]\n  D[Store sensitive data in secrets]\n  E[Encrypt ETCD data and use TLS authentication]</code></pre> <p>Server hardening is the process of enhancing server security through a series of steps.</p> <ul> <li>Isolate the applications on separate servers/networks to reduce the risk of cross-contamination.</li> <li>Use firewall rules and policies to restrict Docker port access.</li> <li>Apply least privilege to containers and secure (restrict access) Kubernetes dashboard using RBAC, so that we can remove vulnerability (Dirty Cow).</li> <li>Store sensitive data in Kubernetes secrets.</li> <li>Encrypt ETCD data at REST and use TLS authentication for secure communication between the Kubernetes components. You can also set RBAC and perform regular backups.</li> </ul> <pre><code>graph LR\n  A[Hacker] --&gt; B[IP Address]\n  B --same IP address--&gt; C[karchun.com]\n  B --same IP address--&gt; D[karchunt.com]</code></pre> <p>Based on this scenario, we know that <code>karchun.com</code> and <code>karchunt.com</code> are using the same IP address, meaning that they are hosted on the same physical or virtual server. This creates a security risk: lacks of network segmentation, which means that if one of the websites is compromised, the other websites on the same server are also at risk.</p> <p>So the solution is to isolate the application on separate networks/servers. Besides, you can remove the public IP address of the <code>kube-apiserver</code> and use a private IP address to prevent unauthorized access. Also, you can implement stringent (strict) access controls like VPN.</p> <pre><code>graph LR\n  A[Hacker] --&gt; B[Firewall]\n  B --&gt; C\n  subgraph C[Host Machine]\n    direction LR\n    D[\"Docker Port (2375)\"] --&gt; E[Docker]\n  end</code></pre> <p>Based on this scenario, we know that Docker port 2375 is an unauthenticated port, meaning that anyone can access the Docker daemon without any authentication. So, the solution is to implement a proper network security policy. For example, you can use firewall rules to restrict the access to the Docker port to only trusted and authenticated users.</p>"},{"location":"docs/kubernetes/4c-cloud-native-k8s-security/#kubernetes-isolation-techniques","title":"Kubernetes Isolation Techniques","text":"<pre><code>graph\n  A[Namespaces]\n  B[Network Policies]\n  C[RBAC]\n  D[Resource Quotas]\n  E[Pod Security Policies]</code></pre> <ul> <li>Use Namespaces to isolate the resources in the cluster. Namespaces are a way to manage the resources for multiple teams or environments, which known as multitenancy. Meaning that, different teams/projects can use the same cluster without interfering with each other.</li> <li>Implement Network Policies to control the traffic flow between the pods. It can be used to allow or deny the traffic based on the source and destination.</li> <li>Implement RBAC (Role-Based Access Control) to control the access to the resources in the cluster. It can be used to grant or deny the access to the resources based on the roles and permissions.</li> <li>Set the Resource Quotas to limit the amount of resources that can be used by the pods in the namespace. It can be used to prevent the resource exhaustion or monopolization.</li> <li>Use Pod Security Policies to control the security settings of the pods. Pod Security Policies can be used to enforce security standards such as using security contexts to run containers as non-root, disallowing privileged escalation, and restricting the use of host namespaces.</li> </ul>"},{"location":"docs/kubernetes/4c-cloud-native-k8s-security/#artifact-repository-and-image-security","title":"Artifact Repository and Image Security","text":"<pre><code>graph\n  A[Artifact Repository]\n  B[Scan Images]\n  C[Trusted Base Images]\n  D[Digital Signatures]</code></pre> <ul> <li>Use Artifact Repository to store the container images securely, for example, Artifactory, Nexus, etc.</li> <li>Scan the container images for the vulnerabilities before deploying them into the cluster. For example, Trivy, Clair, etc.</li> <li>Use trusted base images or minimal images to reduce the attack surface. Using minimal base image from a reputable source like Alpine Linux can reduce the attack surface, as they will perform regular updates and security scans.</li> <li>Use digital signatures to ensure or verify the container image authenticity and integrity</li> </ul>"},{"location":"docs/kubernetes/api-groups/","title":"API Groups","text":""},{"location":"docs/kubernetes/api-groups/#concept-and-usage-of-api-groups","title":"Concept and Usage of API Groups","text":"<p>Refer to the Kubernetes API Reference and Kubernetes API for the complete list of API groups and resources.</p> <p>In Kubernetes, we have some endpoints that are grouped together based on their functionality. These groups are called API groups.</p> <ul> <li><code>/version</code> - Provides version information about the kube-apiserver or cluster.</li> <li><code>/healthz</code> - Health check endpoint to verify if the kube-apiserver is running correctly.</li> <li><code>/metrics</code> - Exposes metrics for monitoring and performance analysis.</li> <li><code>/logs</code> - Provides access to the logs of the kube-apiserver or to integrate with third-party logging applications.</li> <li><code>/api</code> - The core API group, which includes core resources such as pods, services, nodes, etc.</li> <li><code>/apis</code> - The extended API group (named group), which includes additional resources and features.</li> </ul> <p>The reason to understand this is that when you are working with RBAC (Role-Based Access Control) in Kubernetes, you need to specify the API group and resource to which the role applies.</p> <p>There are two ways to access the API paths:</p> <ol> <li> <p>Use cluster IP and port <code>6443</code> Bash<pre><code># View all API paths\ncurl https://&lt;cluster-ip&gt;:6443\n\n# View the core API group\ncurl https://&lt;cluster-ip&gt;:6443/api\n\n# View the named API group\ncurl https://&lt;cluster-ip&gt;:6443/apis\n\n# View the kube-apiserver version\ncurl https://&lt;cluster-ip&gt;:6443/version\n\n# Get the lists of pods\ncurl https://&lt;cluster-ip&gt;:6443/api/v1/pods\n\n# View the service endpoint\ncurl https://&lt;cluster-ip&gt;:6443/api/v1/namespaces/&lt;namespace-name&gt;/services/&lt;service-name&gt;/&lt;service-endpoint&gt;\ncurl https://&lt;cluster-ip&gt;:6443/api/v1/namespaces/default/services/kubernetes/api\n\ncurl https://&lt;cluster-ip&gt;:6443 --key &lt;path-to-key&gt; --cert &lt;path-to-cert&gt; --cacert &lt;path-to-ca-cert&gt;\n</code></pre>     You might need to provide authentication details to access the API paths.</p> </li> <li> <p>Use <code>kubectl proxy</code> <pre><code>flowchart LR\n  user --&gt; k[kubectl proxy] --&gt; kube-apiserver</code></pre></p> <p>Bash<pre><code>kubectl proxy\ncurl http://localhost:8001\n\n# You can also use this to access Kubernetes services\n# the endpoint URL will end with **/proxy/\n# this indicates that the request is being proxied to the service within the cluster\ncurl http://localhost:8001/api/v1/namespaces/default/services/service-name/proxy/\n\n# after /proxy/ you can add the path to the service\n# the #/ is required\ncurl http://localhost:8001/api/v1/namespaces/default/services/service-name/proxy/#/&lt;endpoint&gt;\ncurl http://localhost:8001/api/v1/namespaces/default/services/service-name/proxy/#/login\n</code></pre> This will create a proxy server that will forward the requests to the kube-apiserver, it will use the kubeconfig file to authenticate these requests.</p> <p>kube-proxy != <code>kubectl proxy</code></p> <ul> <li><code>kube-proxy</code> is a network proxy that runs on each node in the cluster to enable network communication to the pods from the outside world.</li> <li><code>kubectl proxy</code> is a proxy server that forwards requests to the kube-apiserver.</li> </ul> </li> </ol>"},{"location":"docs/kubernetes/api-groups/#core-api-group","title":"Core API Group","text":"<p>The core API group is the default API group in Kubernetes. It includes the core resources such as pods, services, nodes, etc. The core API group is accessed using the <code>/api/v1</code> endpoint.</p> <pre><code>flowchart\n  a[\"/api\"] --&gt; b[\"/v1\"] --&gt; resources\n  subgraph resources\n    direction BT\n    pods\n    nodes\n    namespaces\n    services\n    configmaps\n    secrets\n    pv\n    pvc\n    events\n    endpoints\n    replicationcontrollers\n    bindings\n  end</code></pre>"},{"location":"docs/kubernetes/api-groups/#named-api-group","title":"Named API Group","text":"<p>The named API group is an extended API group that includes additional resources and features that can extend the Kubernetes functionality by adding custom resources. Besides, all the new resources and features are added to the named API group. The named API group is accessed using the <code>/apis/&lt;group&gt;/&lt;version&gt;</code> endpoint.</p> <pre><code>flowchart\n  a[\"/apis\"] --&gt; group\n\n  subgraph group[\"API Groups\"]\n      direction BT\n      a1[\"/apps\"]\n      a2[\"/extensions\"]\n      a3[\"/networking.k8s.io\"]\n      a4[\"/storage.k8s.io\"]\n      a5[\"/authentication.k8s.io\"]\n      a6[\"/certificates.k8s.io\"]\n  end\n\n  a1 --&gt; v[\"/v1\"] --&gt; r1\n\n  subgraph r1[Resources]\n      r11[\"/deployments\"] \n      r12[\"/replicasets\"]\n      r13[\"/statefulsets\"]\n      r14[\"/daemonsets\"]\n  end\n\n  r11[\"/deployments\"]  --&gt; action\n\n  subgraph action[Actions]\n      direction BT\n      list\n      get\n      create\n      delete\n      update\n      watch\n  end</code></pre> <p>Each API group has its own set of resources and each resource has its own set of actions.</p>"},{"location":"docs/kubernetes/audit-logging/","title":"Audit Logging","text":""},{"location":"docs/kubernetes/audit-logging/#concept-of-audit-logging","title":"Concept of Audit Logging","text":"<p>Reference</p> <p>https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/</p> <p>Before I explain what is audit logging? It is important to answer the following questions:</p> <ul> <li>How do we audit and monitor what is happening in our Kubernetes cluster? <ul> <li>For example, if a user creates a new pod, how do we know <ul> <li>who created it?</li> <li>when it was created?</li> <li>which namespace it was created in?</li> <li>where was the request made from?</li> <li>what was the response?</li> <li>what happened?</li> </ul> </li> </ul> </li> </ul> <p>Audit logging is a way to record the actions taken by users and system components in a Kubernetes cluster. It is important to have audit logging enabled to track changes made to the cluster and to identify potential security issues.</p> <p>Important</p> <p>Audit logging is handled by the kube-apiserver component. It is important to note that audit logging is not enabled by default in Kubernetes. You need to enable it explicitly.</p> <p>Each request on each stage will generate an event. The event will contain information about the request. The event will be stored in a file.</p>"},{"location":"docs/kubernetes/audit-logging/#four-stages-of-audit-logging","title":"Four Stages of Audit Logging","text":"<p>Each request can be recorded with an associated stage and there are four stages in the audit logging process.</p> <ol> <li>RequestReceived - This stage occurs when the API server receives a request. At this point, the server has not yet processed the request. It is useful for tracking when a request was initiated and by whom. For example, if a user creates a new pod, this stage logs the initial receipt of that request.</li> <li>ResponseStarted - This stage is logged when the API server starts sending a response (only send response headers without response body) back to the client. It indicates that the server has begun processing the request and has enough information to start generating a response. For example, this could occur after validating the request and determining that it can proceed.</li> <li>ResponseComplete - This stage is logged when the API server has fully processed the request and completed sending the response (response body) to the client. It includes details about the outcome of the request, such as whether it succeeded or failed, and any associated metadata.</li> <li>Panic - This stage is logged if the API server encounters an unexpected error or crash while processing request or got invalid request, the request goes through a panic stage. It is used to capture critical failures that may require further investigation or debugging.</li> </ol>"},{"location":"docs/kubernetes/audit-logging/#usage-of-audit-logging","title":"Usage of Audit Logging","text":"<p>You can use Log or Webhook backend (eg, Falco) to enable audit logging. In this example, we will use Log backend.</p>"},{"location":"docs/kubernetes/audit-logging/#step-1-create-an-audit-policy-file","title":"Step 1: Create an Audit Policy File","text":"<p>Important</p> <p>Remember, we only want to record the important events (eg, delete pods in specific namespace). We do not want to record all events, as if we record all events for every stage within the cluster, it will generate a lot of data.</p> <p>There are four audit levels in Kubernetes:</p> Level Description None Don't log events that match this rule Metadata Log events with metadata (requesting user, timestamp, resources, verb, etc.) but not request or response body Request Log events with request metadata and body but not response body. This does not apply for non-resource requests RequestResponse Log events with request metadata, request body and response body. This does not apply for non-resource requests /etc/kubernetes/audit-policy.yaml<pre><code>apiVersion: audit.k8s.io/v1\nkind: Policy\n# Don't generate audit events for all requests in RequestReceived stage.\nomitStages: # optional\n  - \"RequestReceived\"\nrules:\n  # Log pod changes at RequestResponse level\n  - level: RequestResponse\n    resources:\n    - group: \"\"\n      # Resource \"pods\" doesn't match requests to any subresource of pods,\n      # which is consistent with the RBAC policy.\n      resources: [\"pods\"]\n\n  # Log \"pods/log\", \"pods/status\" at Metadata level\n  - level: Metadata\n    resources:\n    - group: \"\"\n      resources: [\"pods/log\", \"pods/status\"]\n\n  # Don't log requests to a configmap called \"controller-leader\"\n  - level: None\n    resources:\n    - group: \"\"\n      resources: [\"configmaps\"]\n      resourceNames: [\"controller-leader\"]\n\n  # Don't log watch requests by the \"system:kube-proxy\" on endpoints or services\n  - level: None\n    users: [\"system:kube-proxy\"]\n    verbs: [\"watch\"]\n    resources:\n    - group: \"\" # core API group\n      resources: [\"endpoints\", \"services\"]\n\n  # Don't log authenticated requests to certain non-resource URL paths.\n  - level: None\n    userGroups: [\"system:authenticated\"]\n    nonResourceURLs:\n      - \"/api*\" # Wildcard matching.\n      - \"/version\"\n\n  # Log the request body of configmap changes in kube-system.\n  - level: Request\n    resources:\n      - group: \"\" # core API group\n        resources: [\"configmaps\"]\n    # This rule only applies to resources in the \"kube-system\" namespace.\n    # The empty string \"\" can be used to select non-namespaced resources.\n    namespaces: [\"kube-system\"]\n\n  # Log configmap and secret changes in all other namespaces at the Metadata level.\n  - level: Metadata\n    resources:\n      - group: \"\" # core API group\n        resources: [\"secrets\", \"configmaps\"]\n\n  # Log all other resources in core and extensions at the Request level.\n  - level: Request\n    resources:\n      - group: \"\" # core API group\n      - group: \"extensions\" # Version of group should NOT be included.\n\n  # A catch-all rule to log all other requests at the Metadata level.\n  - level: Metadata\n    # Long-running requests like watches that fall under this rule will not generate an audit event in RequestReceived.\n    omitStages:\n      - \"RequestReceived\"\n\n  # Log all requests at the Metadata level.\n  - level: Metadata\n</code></pre> <ul> <li><code>omitStages</code> is optional, but when it is defined, the events of this stage will not be recorded.</li> <li><code>level</code> is mandatory</li> <li><code>verbs</code> is optional, when it is not defined, it will be applied to all verbs, like <code>get</code>, <code>list</code>, <code>watch</code>, <code>create</code>, <code>update</code>, <code>patch</code>, <code>delete</code>, etc.</li> <li><code>pods/log</code> - This subresource is used to access the logs of a specific pod. <code>kubectl logs &lt;pod-name&gt;</code></li> <li><code>pods/status</code> - This subresource is used to retrieve the status of a specific pod. <code>kubectl get pod &lt;pod-name&gt; -o jsonpath='{.status}'</code></li> </ul>"},{"location":"docs/kubernetes/audit-logging/#step-2-enable-audit-logging-in-kube-apiserver","title":"Step 2: Enable Audit Logging in <code>kube-apiserver</code>","text":"<p>Reference</p> <p>https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#log-backend</p> <p>You can enable audit logging in <code>kube-apiserver</code> by passing the <code>--audit-policy-file</code> and <code>--audit-log-path</code> flags to the <code>kube-apiserver</code> command and then you need to mount with volumes and <code>volumeMounts</code>.</p> <ul> <li><code>--audit-policy-file</code> flag specifies the path to the audit policy file</li> </ul> <p>You can also add additional fields </p> <ul> <li><code>--audit-log-path</code> - specifies the log file path that log backend uses to write audit events.</li> <li><code>--audit-log-maxage</code> - defines the maximum number of days to retain old audit log files.</li> <li><code>--audit-log-maxbackup</code> - defines the maximum number of audit log files to retain.</li> <li><code>--audit-log-maxsize</code> - defines the maximum size in megabytes of the audit log file before it gets rotated.</li> </ul> kube-apiserver.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver-kind-cluster-control-plane\n  namespace: kube-system\nspec:\n  containers:\n    - command:\n      - kube-apiserver\n      - --advertise-address=10.89.0.2\n      - --allow-privileged=true\n      - --authorization-mode=Node,RBAC\n      - --client-ca-file=/etc/kubernetes/pki/ca.crt\n      - --enable-admission-plugins=NodeRestriction,PodSecurity\n      - --audit-policy-file=/etc/kubernetes/audit-policy.yaml\n      - --audit-log-path=/var/log/kubernetes/audit/audit.log\n      - --audit-log-maxage=10\n      - --audit-log-maxbackup=5\n      - --audit-log-maxsize=100\n      ...\n      image: registry.k8s.io/kube-apiserver:v1.30.0\n      volumeMounts:\n        - mountPath: /etc/kubernetes/audit-policy.yaml\n          name: audit\n          readOnly: true\n        - mountPath: /var/log/kubernetes/audit/\n          name: audit-log\n          readOnly: false\n  volumes:\n    - name: audit\n      hostPath:\n        path: /etc/kubernetes/audit-policy.yaml\n        type: File\n    - name: audit-log\n      hostPath:\n        path: /var/log/kubernetes/audit/\n        type: DirectoryOrCreate\n</code></pre> <p></p> <p></p>"},{"location":"docs/kubernetes/authentication/","title":"Authentication","text":""},{"location":"docs/kubernetes/authentication/#different-users-in-kubernetes","title":"Different users in Kubernetes","text":"<p>In a Kubernetes cluster, we have different users, for example,</p> <ul> <li>administrators - who manage the cluster</li> <li>developers - who deploy or test applications</li> <li>end-users - who access the application running in the cluster</li> <li>third-party applications or bots - that interact with the cluster for integration purpose</li> </ul>"},{"location":"docs/kubernetes/authentication/#authentication-in-kubernetes","title":"Authentication in Kubernetes","text":"<p>All user access is managed by the kube-apiserver. As mentioned before, the kube-apiserver is the front-end of the Kubernetes control plane. It authenticates the user and authorizes the user to perform the requested operation.</p> <pre><code>flowchart\n  user[Admin or Developer] --&gt; a\n  a[User Request] --authenticate user--&gt; kube-apiserver --&gt; process[Process Request]</code></pre> <p>We have different ways to authenticate users in Kubernetes.</p> <ul> <li>Client certificates</li> <li>Static password file (Deprecated) - Contains a list of usernames and passwords</li> <li>Static token file - Containers a list of usernames and tokens</li> <li>Connect to an identity provider (third-party) - Like LDAP or Kerberos</li> <li>Service account tokens</li> </ul>"},{"location":"docs/kubernetes/authentication/#static-password-file-deprecated","title":"Static password file (Deprecated)","text":"<p>In this method, we will create a csv file with password, username, and user Id. This file will be passed to the kube-apiserver using the <code>--basic-auth-file</code> flag. You can find your kube-apiserver configuration file in <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>.</p> passwords.csv<pre><code># password, username, user Id, group (optional)\npassword1,joe_user,joe_userID\npassword1,joe_user,joe_userID,group1\n</code></pre> <ul> <li>You can also specify the fourth column as the group to which the user belongs, but it is optional.</li> </ul> <p>To authenticate users, you can use the following command.</p> Bash<pre><code>curl -v -k https://master-node-ip:6443/api/v1/pods -u \"username:password\"\n</code></pre>"},{"location":"docs/kubernetes/authentication/#static-token-file","title":"Static token file","text":"<p>The concept is similar to the static password file. In this method, we will create a csv file with token, username, and user Id. This file will be passed to the kube-apiserver using the <code>--token-auth-file</code> flag.</p> tokens.csv<pre><code># token, username, user Id, group (optional)\nmjpuauwabcnIXBicj12cnXoaS,joe_user,joe_userID\najpuauwabcnIXBicj12cnXoaS,joe_user,joe_userID,group1\n</code></pre> <p>To authenticate users, you can use the following command.</p> Bash<pre><code>curl -v -k https://master-node-ip:6443/api/v1/pods --header \"Authorization: Bearer &lt;replace-your-token&gt;\"\n</code></pre>"},{"location":"docs/kubernetes/authentication/#authenticate-using-kubeconfig","title":"Authenticate using kubeconfig","text":"<p>Refer to kubeconfig for more information.</p>"},{"location":"docs/kubernetes/authentication/#authenticate-using-kubectl-proxy","title":"Authenticate using kubectl proxy","text":"<p>Refer to api-groups for more information.</p> Bash<pre><code>kubectl proxy\n</code></pre>"},{"location":"docs/kubernetes/authentication/#authenticate-using-bootstrap-token","title":"Authenticate using bootstrap token","text":"<p>Reference</p> <p>Boostrap token is a token that is used to authenticate with the kube-apiserver when creating a new cluster or joining a new node to the cluster. The bootstrap token is valid for 24 hours by default.</p> <ul> <li>Once a node has successfully joined the Kubernetes cluster using a bootstrap token, the expiration of the bootstrap token does not affect the node's membership in the cluster. Bsc, the node will use a client certificate for authentication after joining the cluster.</li> <li>Bootstrap token is used only for the initial join process.</li> </ul>"},{"location":"docs/kubernetes/authentication/#step-1-create-a-bootstrap-token","title":"Step 1: Create a bootstrap token","text":"<p>There are two ways to create a bootstrap token.</p> <ul> <li>Using kubeadm. Remember run this command on the node that has installed <code>kubeadm</code> tool.<ul> <li><code>kubeadm token generate</code> - Generate and print a bootstrap token, but do not create it on the server.</li> <li><code>kubeadm token create</code> - Create bootstrap tokens on the server<ul> <li>You can append <code>--dry-run --print-join-command</code> to get the secret YAML content and join command.</li> </ul> </li> </ul> </li> <li>Manually</li> </ul> <p>The bootstrap token format must match the following regex: <code>^[a-z0-9]{6}\\.[a-z0-9]{16}$</code></p>"},{"location":"docs/kubernetes/authentication/#step-2-store-the-bootstrap-token-into-kubernetes-secret","title":"Step 2: Store the bootstrap token into Kubernetes secret","text":"<p>The bootstrap token must exist in the <code>kube-system</code> namespace.</p> bootstrap-token-07401b.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  # Name MUST be of form \"bootstrap-token-&lt;token id&gt;\"\n  name: bootstrap-token-07401b\n  namespace: kube-system\ntype: bootstrap.kubernetes.io/token\nstringData:\n  # Human readable description. Optional.\n  description: \"The default bootstrap token generated by 'kubeadm init'.\"\n\n  token-id: 07401b\n  token-secret: f395accd246ae52d\n  usage-bootstrap-authentication: \"true\"\n  usage-bootstrap-signing: \"true\"\n\n  # Extra groups to authenticate the token as. Must start with \"system:bootstrappers:\"\n  auth-extra-groups: system:bootstrappers:worker,system:bootstrappers:ingress,system:bootstrappers:kubeadm:default-node-token\n</code></pre> <ul> <li><code>usage-bootstrap-authentication</code> - indicates that the token can be used to authenticate to the <code>kube-apiserver</code> as a bearer token</li> <li><code>usage-bootstrap-signing</code> - indicates that the token may be used to sign the <code>cluster-info</code> ConfigMap.</li> <li><code>auth-extra-groups</code><ul> <li><code>system:bootstrappers:worker</code> - This group is typically used for worker nodes that are joining the cluster. Nodes in this group have permissions necessary to register themselves with the control plane and start running workloads.</li> <li><code>system:bootstrappers:ingress</code> - This group is used for nodes that are specifically joining the cluster to run ingress controllers. Nodes in this group have permissions tailored for ingress-related operations.</li> <li><code>system:bootstrappers:kubeadm:default-node-token</code> (default) - This group is used by kubeadm during the node join process. Nodes in this group have the default permissions required to join the cluster using a bootstrap token created by kubeadm.</li> </ul> </li> </ul>"},{"location":"docs/kubernetes/authentication/#step-3-get-the-worker-node-join-command","title":"Step 3: Get the worker node join command","text":"<p>On the control plane node, use <code>kubeadm</code> with the <code>token-id</code> and <code>token-secret</code> that you generated in the previous steps to get the worker node join command.</p> Bash<pre><code>kubeadm token create &lt;token-id&gt;.&lt;token-secret&gt; --dry-run --print-join-command\n</code></pre>"},{"location":"docs/kubernetes/authentication/#step-4-join-the-worker-node-to-the-cluster","title":"Step 4: Join the worker node to the cluster","text":"<p>On the worker node or <code>ssh</code> into the worker node, run the join command that you got from the previous step. In this example, I am using <code>ssh</code> to connect to the worker node.</p> <ul> <li>Ensure that the worker node has <code>kubeadm</code>, <code>kubelet</code>, and <code>kubectl</code> installed.</li> </ul> Bash<pre><code># ssh into the worker node\nssh node1\n\n# run the join command\nkubeadm join kind-cluster-control-plane:6443 --token joisdm.akqtwa3n4swya2ol --discovery-token-ca-cert-hash sha256:dae81a955efb64d6e568d1dafc6ad7a3c2afc6be65345c9bb86a3429440e4c07\n</code></pre>"},{"location":"docs/kubernetes/authentication/#step-5-verify-the-worker-node-already-joined-the-cluster","title":"Step 5: Verify the worker node already joined the cluster","text":"<p>You can verify the result from the master node (control plane node).</p> Bash<pre><code>kubectl get nodes\n</code></pre>"},{"location":"docs/kubernetes/authentication/#step-6-authenticate-using-the-bootstrap-secrets-optional","title":"Step 6: Authenticate using the bootstrap secrets (Optional)","text":"<p>This step is optional. You can further authenticate using the secrets.</p> Bash<pre><code># Get the kube-apiserver address\nkubectl cluster-info\nkubectl config view\n\n# Get and decode the token id and secret from Kubernetes secrets\nkubectl get secret bootstrap-token-07401b -n kube-system -o jsonpath='{.data.token-id}' | base64 --decode\nkubectl get secret bootstrap-token-07401b -n kube-system -o jsonpath='{.data.token-secret}' | base64 --decode\n\nexport TOKEN=&lt;decoded-token-id&gt;.&lt;decoded-token-secret&gt;\ncurl -v -k -X GET https://kind-cluster-control-plane:6443/api/v1/pods --header \"Authorization: Bearer $TOKEN\"\n</code></pre>"},{"location":"docs/kubernetes/authentication/#authenticate-using-service-account-token","title":"Authenticate using service account token","text":""},{"location":"docs/kubernetes/authentication/#step-1-create-a-service-account","title":"Step 1: Create a service account","text":"data-sa.yaml<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: data-sa\n</code></pre>"},{"location":"docs/kubernetes/authentication/#step-2-associate-the-secret-with-the-service-account","title":"Step 2: Associate the secret with the service account","text":"data-sa-secret.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: data-sa-secret\n  annotations:\n    kubernetes.io/service-account.name: data-sa # service account name\ntype: kubernetes.io/service-account-token\n</code></pre>"},{"location":"docs/kubernetes/authentication/#step-3-view-the-secret","title":"Step 3: View the secret","text":"Bash<pre><code>kubectl get secret/data-sa-secret -o jsonpath='{.data.token}' | base64 -d\nexport TOKEN=&lt;decoded-token&gt;\n</code></pre>"},{"location":"docs/kubernetes/authentication/#step-4-create-a-role-and-rolebinding","title":"Step 4: Create a Role and RoleBinding","text":"<p>We need to create a Role and RoleBinding to grant permissions to the service account. Let's said we only allow this service account to list pods in the <code>default</code> namespace.</p> Bash<pre><code>kubectl create role pod-reader --verb=list --resource=pods\nkubectl create rolebinding pod-reader-binding --role=pod-reader --serviceaccount=default:data-sa\n</code></pre>"},{"location":"docs/kubernetes/authentication/#step-5-authenticate-using-the-service-account-token","title":"Step 5: Authenticate using the service account token","text":"Bash<pre><code>curl -v -k -X GET https://kind-cluster-control-plane:6443/api/v1/namespaces/default/pods --header \"Authorization: Bearer $TOKEN\"\n</code></pre>"},{"location":"docs/kubernetes/authorization/","title":"Authorization","text":""},{"location":"docs/kubernetes/authorization/#authorization-mechanisms-modes","title":"Authorization Mechanisms (Modes)","text":"<p>Refer here for more details.</p> <p>In summary;</p> Mode Description AlwaysAllow Allows all requests without any authorization checks. AlwaysDeny Blocks all requests. Node Authorization A special-purpose authorization mode that authorizes API requests made by kubelets. ABAC It defines an access control paradigm whereby access rights are granted to users through the use of policies that combine attributes together. RBAC It is a method of regulating (control) access to resources in a Kubernetes cluster based on the roles of individual users or service accounts. Webhook A mode that allows you to manage authorization externally. For example, Open Policy Agent (OPA). <p>By default, the mode is set to <code>AlwaysAllow</code> in the kube-apiserver. You can change the mode by setting the <code>--authorization-mode</code> flag in the kube-apiserver. You can also specify multiple modes by separating them with a comma. When you specify multiple nodes, the kube-apiserver will try to authorize the request with the first mode in the list. If the request is denied, it will try the next mode in the list.</p> Bash<pre><code>--authorization-mode=RBAC,ABAC,Webhook\n</code></pre>"},{"location":"docs/kubernetes/authorization/#node-authorization","title":"Node Authorization","text":"<p>Info</p> <p>The node authorizer only handles node requests.</p> <p>Reference</p> <p>The node authorizer allows a kubelet to perform API operations. All these operations are handled by node authorizer. So, we know that kubelet is part of a system nodes group and have a prefix <code>system:node</code>. The kubelet will be granted these privileges after the node authorizer has authorized the kubelet so that it can perform the following operations.</p> <p>Read operations;</p> <ul> <li>services</li> <li>endpoints</li> <li>nodes</li> <li>pods</li> <li>secrets, configmaps, persistent volume claims and persistent volumes related to pods bound to the kubelet's node</li> </ul> <p>Write operations;</p> <ul> <li>node status</li> <li>pod status</li> <li>events</li> </ul>"},{"location":"docs/kubernetes/authorization/#attribute-based-access-control-abac","title":"Attribute-based Access Control (ABAC)","text":"<p>Reference</p> <p>ABAC is a method of restricting access to resources based on the attributes of the user. It is a static file that defines the access control policy. The policy file is a set of rules that specify what kind of access is granted to which users.</p>"},{"location":"docs/kubernetes/authorization/#step-1-create-a-policy-file","title":"Step 1: Create a Policy File","text":"<p>For example, the following policy file</p> <ul> <li>allows Alice to create pods and get pods in the default namespace.</li> <li>allows Bob to get pods in the prod namespace, but not create pods.</li> <li>allows group dev to access all resources in all namespaces.</li> </ul> policy.jsonl<pre><code>{\"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\", \"kind\": \"Policy\", \"spec\": { \"user\": \"alice\", \"namespace\": \"default\", \"resource\": \"pods\", \"readonly\": false}}\n{\"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\", \"kind\": \"Policy\", \"spec\": {\"user\": \"bob\", \"namespace\": \"prod\", \"resource\": \"pods\", \"readonly\": true}}\n{\"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\", \"kind\": \"Policy\", \"spec\": {\"group\": \"dev\", \"namespace\": \"*\", \"resource\": \"*\"}}\n</code></pre> <ul> <li>you can specify multiple rules in the policy file.</li> </ul>"},{"location":"docs/kubernetes/authorization/#step-2-enable-abac-mode","title":"Step 2: Enable ABAC mode","text":"<p>Once you have the policy file, you will need to enable ABAC in the kube-apiserver by setting the <code>--authorization-mode</code> flag to <code>ABAC</code> and the <code>--authorization-policy-file</code> flag to the path of the policy file. But this method is not recommended, as everytime you need to update the policy file, you need to restart the kube-apiserver.</p> /etc/kubernetes/manifests/kube-apiserver.yaml<pre><code>--authorization-mode=ABAC\n--authorization-policy-file=policy.jsonl\n</code></pre> <ul> <li>Remember to mount the policy file through a volume.</li> </ul>"},{"location":"docs/kubernetes/authorization/#step-3-create-a-new-user-and-test","title":"Step 3: Create a new user and test","text":"<p>You can create a service account and use service account token to create a new user.</p> Bash<pre><code># create a new service account\nkubectl create serviceaccount alice\n</code></pre> <p>Create service account token.</p> sa-token-secret.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: alice-token\n  annotations:\n    kubernetes.io/service-account.name: alice\ntype: kubernetes.io/service-account-token\n</code></pre> <p>Retrieve the service account token and store the token securely in a file as it is used to authenticate the user.</p> Bash<pre><code>kubectl get secret alice-token -o jsonpath='{.data.token}' | base64 --decode &gt; token.txt\n</code></pre> <p>Setup new user credentials with the token.</p> Bash<pre><code>export ALICE_TOKEN=$(cat token.txt)\nkubectl config set-credetials alice --token=$ALICE_TOKEN\n</code></pre> <p>Setup new context with the new user.</p> Bash<pre><code>kubectl config set-context alice-context --user=alice --cluster=kubernetes --namespace=default\n</code></pre> <p>Verify the permissions of the user.</p> Bash<pre><code>kubectl get pods -n default # should work\nkubectl get secrets -n default # should not work\n</code></pre>"},{"location":"docs/kubernetes/authorization/#role-based-access-control-rbac","title":"Role-based Access Control (RBAC)","text":"<p>Info</p> <p>RBAC mainly used to authorize users or service accounts to namespace resources.</p> <p>Reference</p> <p>RBAC is a method of regulating (control) access to resources based on the roles of individual users or service accounts. So we will create and define a role with a set of permissions and then bind the role to a user or service account.</p> <p>Before we dive into RBAC, we need to understand what resources are under namespace and cluster scope (non-namespaced resources).</p> Bash<pre><code># list all resources under namespace\nkubectl api-resources --namespaced=true\n\n# this will help you to get the verb\nkubectl api-resources --namespaced=true --sort-by name -o wide\n</code></pre> Name API Version Kind bindings v1 Binding configmaps v1 ConfigMap endpoints v1 Endpoints events v1 Event limitranges v1 LimitRange persistentvolumeclaims v1 PersistentVolumeClaim pods v1 Pod podtemplates v1 PodTemplate replicationcontrollers v1 ReplicationController resourcequotas v1 ResourceQuota secrets v1 Secret serviceaccounts v1 ServiceAccount services v1 Service controllerrevisions apps/v1 ControllerRevision daemonsets apps/v1 DaemonSet deployments apps/v1 Deployment replicasets apps/v1 ReplicaSet statefulsets apps/v1 StatefulSet localsubjectaccessreviews authorization.k8s.io/v1 LocalSubjectAccessReview horizontalpodautoscalers autoscaling/v2 HorizontalPodAutoscaler cronjobs batch/v1 CronJob jobs batch/v1 Job leases coordination.k8s.io/v1 Lease endpointslices discovery.k8s.io/v1 EndpointSlice events events.k8s.io/v1 Event ingresses networking.k8s.io/v1 Ingress networkpolicies networking.k8s.io/v1 NetworkPolicy poddisruptionbudgets policy/v1 PodDisruptionBudget contourconfigurations projectcontour.io/v1alpha1 ContourConfiguration contourdeployments projectcontour.io/v1alpha1 ContourDeployment extensionservices projectcontour.io/v1alpha1 ExtensionService httpproxies projectcontour.io/v1 HTTPProxy tlscertificatedelegations projectcontour.io/v1 TLSCertificateDelegation rolebindings rbac.authorization.k8s.io/v1 RoleBinding roles rbac.authorization.k8s.io/v1 Role csistoragecapacities storage.k8s.io/v1 CSIStorageCapacity <p>Now, from the above table, we can see that roles and rolebindings are under namespace scope, meaning that they are created within a namespace. If you do not specify a namespace, they will be created in the default namespace and control access to resources within that namespace.</p>"},{"location":"docs/kubernetes/authorization/#step-1-create-a-role","title":"Step 1: Create a Role","text":"<p>You can use the following command to get the verb for the resource.</p> Bash<pre><code>kubectl api-resources --namespaced=true --sort-by name -o wide\n</code></pre> role.yaml<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: developer\nrules:\n  - apiGroups: [\"\"] # \"\" indicates the core API group\n    resources: [\"pods\"]\n    verbs: [\"get\", \"watch\", \"list\"]\n  - apiGroups: [\"apps\"]\n    resources: [\"deployments\"]\n    verbs: [\"create\", \"delete\"]\n  - apiGroups: [\"\"]\n    resources: [\"secrets\"] # use * to allow all resources\n    verbs: [\"get\"] # use * to allow all verbs\n    resourceNames: [\"secret-name\", \"my-secret\"] # optional\n</code></pre> <ul> <li><code>apiGroups</code> - The API group of the resource. If you are not sure, you can see the above table (API Version).</li> <li><code>resourceNames</code> - The names of the resources that the rule applies to. If you do not specify, the rule applies to all resources of the specified type.<ul> <li>In this example, the user can only read the secrets with the name <code>secret-name</code> and <code>my-secret</code>.</li> <li><code>secret-name</code> and <code>my-secret</code> are the names of the secrets (metadata name).</li> </ul> </li> </ul> Bash<pre><code># You cannot create a role with different rules, as they will mix it up\nkubectl create role developer --verb=get,watch,list --resource=pods\nkubectl create role developer --verb=create,delete --resource=deployments.apps # need to append .apps due to apiGroups\nkubectl create role developer --verb=get --resource=secrets --resource-name=secret-name --resource-name=my-secret\n\nkubectl get roles\n</code></pre>"},{"location":"docs/kubernetes/authorization/#step-2-create-a-rolebinding","title":"Step 2: Create a RoleBinding","text":"<p>Link the role to a user, group or service account.</p> rolebinding.yaml<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: developer-binding\nsubjects: # users, groups, or service accounts\n  - kind: User\n    name: user1 # user name\n    apiGroup: rbac.authorization.k8s.io\n  - kind: Group\n    name: backend-developers # group name\n    apiGroup: rbac.authorization.k8s.io\n  - kind: ServiceAccount\n    name: my-service-account # service account name\n    namespace: default # namespace of the service account\n  - kind: Group\n    name: system:serviceaccounts:default # all service accounts in the default namespace\n    apiGroup: rbac.authorization.k8s.io\n  - kind: Group\n    name: system:serviceaccounts # all service accounts in any namespace\n    apiGroup: rbac.authorization.k8s.io\n  - kind: Group\n    name: system:authenticated # all authenticated users\n    apiGroup: rbac.authorization.k8s.io\n  - kind: Group\n    name: system:unauthenticated # all unauthenticated users\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role # Role or ClusterRole\n  name: developer # role name\n</code></pre> <ul> <li>You can also reference a ClusterRole instead of a role to grant the permissions defined in that ClusterRole to resources inside the RoleBinding's namespace. This kind of reference lets you define a set of common roles across your cluster, then reuse them within multiple namespaces.</li> </ul> Bash<pre><code>kubectl create rolebinding developer-binding --role=developer --user=user1\nkubectl create rolebinding developer-binding --role=developer --group=backend-developers\n\nkubectl create rolebinding developer-binding --role=developer --serviceaccount=&lt;namespace&gt;:&lt;service-account-name&gt;\nkubectl create rolebinding developer-binding --role=developer --serviceaccount=default:my-service-account\n\nkubectl get rolebinding\n</code></pre>"},{"location":"docs/kubernetes/authorization/#step-3-verify-the-access-optional","title":"Step 3: Verify the access (Optional)","text":"<p>You can check if the user have access to a resource by using the following command.</p> Bash<pre><code>kubectl auth can-i delete nodes\n\nkubectl auth can-i get pods --as &lt;name-of-the-user&gt;\nkubectl auth can-i get pods --as developer\n\n# check if the user can create deployments in the prod namespace\nkubectl auth can-i create deployments --as developer --namespace prod\n\n# Check if the group 'developers' can delete nodes\nkubectl auth can-i delete nodes --as-group=developers\n\n# Check if the service account 'my-service-account' in the 'default' namespace can get pods\nkubectl auth can-i get pods --as=system:serviceaccount:default:my-service-account\n\n# Check if the service account 'my-service-account' in the 'default' namespace can create deployments in the 'prod' namespace\nkubectl auth can-i create deployments --as=system:serviceaccount:default:my-service-account --namespace=prod\n</code></pre>"},{"location":"docs/kubernetes/backup-and-restore-methods/","title":"Backup and Restore Methods","text":""},{"location":"docs/kubernetes/backup-and-restore-methods/#backup-and-restore-methods-in-kubernetes","title":"Backup and Restore Methods in Kubernetes","text":"<p>Backup Candidates:</p> <ul> <li>Resource Configuration</li> <li>ETCD Cluster</li> <li>Persistent Volumes</li> </ul>"},{"location":"docs/kubernetes/backup-and-restore-methods/#backup-restore-resource-configs","title":"Backup &amp; Restore - Resource Configs","text":"<p>We will query the kube-apiserver using the <code>kubectl</code> command or access the API server directly to save all the objects with the respective resource configuration created on the cluster as a copy.</p> <p>Here is an example of getting all pods, deployments, and services.</p> Bash<pre><code>kubectl get all --all-namespaces -o yaml &gt; backup.yaml\nkubectl apply -f backup.yaml\n</code></pre> <p>Well, there are many tools available to backup Kubernetes resource like Velero, Kasten, etc.</p>"},{"location":"docs/kubernetes/backup-and-restore-methods/#backup-restore-etcd","title":"Backup &amp; Restore - ETCD","text":"<p>We know that ETCD is the database of the Kubernetes cluster that stores all the cluster data like nodes, pods, secrets, etc.</p> <p>While configuring ETCD, we actually got specified the data directory where the ETCD data is stored. We canbackup this data directory to backup the ETCD data.</p> <p>You can use this command to view the configuration of the ETCD service. <code>kubectl describe pod &lt;etcd-pod&gt; -n kube-system</code></p> etcd.service<pre><code>ExecStart=/usr/local/bin/etcd \\\\\n  ...\n  --data-dir=/var/lib/etcd\n</code></pre> <p>If you don't want to use above method, you can use ETCD snapshot to backup the ETCD data. This command will create a snapshot of the ETCD data and save it to the specified path.</p> <p>Backing up an etcd cluster reference</p> <p>ETCD recovery reference</p> <p>Info</p> <p>Make sure the <code>ETCDCTL_API</code> is set to <code>3</code> to perform backup and restore. </p> Bash<pre><code>export ETCDCTL_API=3\netcdctl version\n</code></pre> Bash<pre><code>ETCDCTL_API=3 etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\ #  --listen-client-urls\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ # --trusted-ca-file\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\ # --cert-file\n  --key=/etc/kubernetes/pki/etcd/server.key \\ # --key-file\n  snapshot save /var/lib/etcd/snapshot.db\n\n# View the snapshot status\nETCDCTL_API=3 etcdctl \\\n  snapshot status /var/lib/etcd/snapshot.db\n</code></pre> <ul> <li>You can specify any path to save the snapshot.</li> <li>Remember to specify the certificate, endpoint, key, etc when saving the snapshot.</li> </ul> <p>To restore the ETCD data.</p>"},{"location":"docs/kubernetes/backup-and-restore-methods/#1-stop-the-kube-apiserver-service","title":"1. Stop the kube-apiserver service","text":"<p>The reason for stopping the kube-apiserver service is ETCD cluster will require to restart to restore the data and the kube-apiserver depends on it.</p> Bash<pre><code>sudo service kube-apiserver stop\n</code></pre>"},{"location":"docs/kubernetes/backup-and-restore-methods/#2-run-the-etcd-restore-command","title":"2. Run the ETCD restore command","text":"<p>When the ETCD data restores from a backup (snapshot or etc), the ETCD will initialize a new cluster configuration and configure the members of ETCD as new members of the cluster to prevent a new member from joining an existing cluster.</p> Bash<pre><code>ETCDCTL_API=3 etcdctl \\\n  snapshot restore /var/lib/etcd/snapshot.db \\\n  --data-dir /var/lib/etcd-new # this data directory will be created\n</code></pre>"},{"location":"docs/kubernetes/backup-and-restore-methods/#3-configure-the-etcd-configuration-file","title":"3. Configure the ETCD configuration file","text":"<p>Configure the ETCD configuration file to use the new data directory. You can use <code>ps aux | grep etcd</code> or <code>kubectl describe pod &lt;control-plane-pod&gt; -n kube-system</code> (not etcd pod).</p> etcd.service<pre><code>ExecStart=/usr/local/bin/etcd \\\\\n  ...\n  --data-dir=/var/lib/etcd-new\n</code></pre> <p>After configuring the ETCD configuration file, remember to reload the service daemon and restart the ETCD service.</p> Bash<pre><code>sudo systemctl daemon-reload\nsudo service etcd restart\n</code></pre> <p>If you deploy ETCD as a pod, then you have to update <code>/etc/kubernetes/manifests/etcd.yaml</code>. Since this is a static pod, it will auto restart when you update the file.</p> /etc/kubernetes/manifests/etcd.yaml<pre><code>volumes:\n- hostPath:\n    path: /var/lib/etcd-from-backup # new directory\n    type: DirectoryOrCreate\n  name: etcd-data\n</code></pre>"},{"location":"docs/kubernetes/backup-and-restore-methods/#4-start-the-kube-apiserver-service","title":"4. Start the kube-apiserver service","text":"Bash<pre><code>sudo service kube-apiserver start\n</code></pre>"},{"location":"docs/kubernetes/benchmarks/","title":"Benchmarks","text":""},{"location":"docs/kubernetes/benchmarks/#cis-benchmarks","title":"CIS Benchmarks","text":"<p>References</p> <ul> <li>https://www.cisecurity.org/cis-benchmarks</li> <li>https://www.cisecurity.org/benchmark/kubernetes</li> </ul> <p>The Center for Internet Security (CIS) provides benchmarks for Kubernetes, etc. These benchmarks are a set of best practices for securing Kubernetes clusters. The benchmarks are available for free and can be downloaded from the CIS website.</p> <p>They also provide a tool called CIS-CAT that can be used to assess the security of your Kubernetes cluster against the benchmarks. It will generate a report (HTML) that will show you the areas where your cluster is not compliant with the benchmarks.</p> <ul> <li>CIS-CAT Lite</li> <li>CIS-CAT Pro</li> </ul>"},{"location":"docs/kubernetes/benchmarks/#kube-bench","title":"kube-bench","text":"<p>References</p> <p>https://github.com/aquasecurity/kube-bench</p> <p>kube-bench is a tool that can be used to run the CIS benchmarks on your Kubernetes cluster. It is an open source project and is available on GitHub.</p>"},{"location":"docs/kubernetes/certificate-api/","title":"Certificate API","text":""},{"location":"docs/kubernetes/certificate-api/#concept","title":"Concept","text":"<p>Kubernetes Certificate API will help the admin to manage and sign (provision) the certificates for authentication purpose.</p> <p>Assuming you are the admin of the Kubernetes cluster, now a new admin wants to join the cluster. If we're not using Certificate API, then you need to manually create or renew the certificate, csr, and key for the new admin. This is a manual process and is tedious.</p> <pre><code>flowchart LR\n  a[Admin] --&gt; b[Create CertificateSigningRequest Object]\n  b --&gt; c[Review and Approve request]\n  c --&gt; d[Share Certificate to user]</code></pre> <p>With the help of the Certificate API, you can automate the process of creating and renewing the certificates for the new admin. The admin can request the certificate using the CertificateSigningRequest (CSR) API object. The admin can approve or deny the certificate request using the CertificateSigningRequest API object.</p> <p>You might be wondering where's the CA server located?</p> <ul> <li>Well, actually, the CA server normally placed in the Kubernetes master node itself as it contains the certificate authority key and certificate.</li> </ul> <p>How the CertificateSigningRequest API works?</p> <ul> <li>We know that kube-controller-manager has a lot of controllers. There are multiple controllers that actually responsible for all the certificate-related stuff.<ul> <li>CSR Approver Controller: This controller is responsible for approving or denying the certificate request.</li> <li>CSR Signer Controller: This controller is responsible for signing the certificate request.</li> <li>CSR Cleaner Controller: This controller is responsible for cleaning up the certificate request.</li> </ul> </li> </ul> <p>So if you view the kube-controller-manager configuration, you will notice CA server's root certificate and private key as an option as they need that to sign the certificate.</p> kube-controller-manager.yaml<pre><code>spec:\n  containers:\n  - command:\n    - kube-controller-manager\n    - --allocate-node-cidrs=true\n    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf\n    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf\n    - --bind-address=127.0.0.1\n    - --client-ca-file=/etc/kubernetes/pki/ca.crt\n    - --cluster-cidr=10.244.0.0/16\n    - --cluster-name=kind-cluster\n    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt\n    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt\n    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key\n    - --controllers=*,bootstrapsigner,tokencleaner\n    - --enable-hostpath-provisioner=true\n    - --kubeconfig=/etc/kubernetes/controller-manager.conf\n    - --leader-elect=true\n    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt\n    - --root-ca-file=/etc/kubernetes/pki/ca.crt\n    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key\n    - --service-cluster-ip-range=10.96.0.0/16\n    - --use-service-account-credentials=true\n</code></pre>"},{"location":"docs/kubernetes/certificate-api/#steps","title":"Steps","text":""},{"location":"docs/kubernetes/certificate-api/#step-1-generate-key-and-csr","title":"Step 1: Generate key and CSR","text":"<p>After the user has generated the key and CSR, the user can send the CSR to the admin.</p> Bash<pre><code>openssl genrsa -out newuser.key 2048\nopenssl req -new -key newuser.key -out newuser.csr -subj \"/CN=newuser\"\n</code></pre>"},{"location":"docs/kubernetes/certificate-api/#step-2-create-certificatesigningrequest-object","title":"Step 2: Create CertificateSigningRequest Object","text":"<p>Convert the CSR to base64 format and pass it to the <code>request</code> field in the <code>CertificateSigningRequest</code> object.</p> Bash<pre><code>cat newuser.csr | base64 | tr -d '\\n'\n</code></pre> newuser-csr.yaml<pre><code>apiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\n  name: newuser-csr\nspec:\n  expirationSeconds: 3600 # 1 hour\n  usages:\n    - digital signature\n    - key encipherment\n    - server auth\n  # base64 encoded CSR, of course you can convert it to base64 and paste it here\n  request: $(cat newuser.csr | base64 | tr -d '\\n')\n  groups: # optional field\n    - system:authenticated\n    - my-custom-group\n</code></pre>"},{"location":"docs/kubernetes/certificate-api/#step-3-review-and-approve-request","title":"Step 3: Review and Approve request","text":"<p>After you approved the request, the Kubernetes will create and sign the certificate using the CA key pairs. The signed certificate will be stored in the csr object under the <code>status.certificate</code> field.</p> Bash<pre><code># Get the CSR requests\nkubectl get csr\n\n# Approve the request\nkubectl certificate approve newuser-csr\n\n# Deny the request\nkubectl certificate deny newuser-csr\n</code></pre>"},{"location":"docs/kubernetes/certificate-api/#step-4-share-signed-certificate-to-user","title":"Step 4: Share signed certificate to user","text":"<p>Remember to decode the base64 encoded certificate before sharing it with the user.</p> Bash<pre><code># get the signed certificate\nkubectl get csr newuser-csr -o jsonpath='{.status.certificate}' | base64 -d &gt; newuser.crt\n</code></pre>"},{"location":"docs/kubernetes/cluster-architecture/","title":"Kubernetes Architecture","text":""},{"location":"docs/kubernetes/cluster-architecture/#cluster-architecture","title":"Cluster Architecture","text":"<pre><code>---\ntitle: Cluster Architecture\n---\nflowchart\n  user --&gt; kube-apiserver\n\n  subgraph \"Cluster\"\n    subgraph \"Master Node (Control Plane)\"\n      id[ETCD cluster] --&gt; kube-apiserver\n      id1[Controller Manager] --&gt; kube-apiserver\n      id2[kube-scheduler] --&gt; kube-apiserver\n    end\n\n    subgraph \"Node 1 (Worker Node)\"\n      kubelet1[kubelet] --&gt; kube-apiserver\n      kubelet1 --&gt; sub1\n      kube-proxy1[kube-proxy] --&gt; sub1\n\n      subgraph sub1 [\"Container Runtime engine\"]\n        direction TB\n        id3[Pod]\n        id4[Pod]\n      end\n    end\n\n    subgraph \"Node 2 (Worker Node)\"\n      kubelet2[kubelet] --&gt; kube-apiserver\n      kubelet2 --&gt; sub2\n      kube-proxy2[kube-proxy] --&gt; sub2\n\n      subgraph sub2 [\"Container Runtime engine\"]\n        direction TB\n        id6[Pod]\n        id7[Pod]\n      end\n    end\n  end</code></pre> <p>Each Kubernetes cluster has a master node (control plane) and one or more worker nodes. It's responsible for managing the cluster, while the worker nodes (Nodes) are responsible for hosting application as containers and running the applications.</p> <p>Info</p> <p>Kubernetes supports other runtime engines that adhere to the OCI standards, like containerd or Rocket, so it is not necessary to install Docker before installing Kubernetes.</p> <p>Here is an overview of Kubernetes cluster.</p> <ul> <li>The cluster has master and worker nodes, both serve different purposes.</li> <li>Master node<ul> <li>ETCD Cluster is used to store information regarding the cluster.</li> <li>kube-controller-manager is used to manage various controllers, each controller, each controller has different functions to take care of its side.</li> <li>kube-scheduler is used to identify and schedule the pods on nodes. It only decides which pod goes to which node.</li> <li>kube-apiserver is used to orchestrate (manage) all cluster operations and it's the primary management component in Kubernetes which acts as the frontend to the cluster.</li> </ul> </li> <li>Worker node<ul> <li>kubelet is used to register the node with the kube-apiserver. It will create the pod on the node when it receives the instructions from the kube-apiserver, and monitor the node and container/pod state.</li> <li>kube-proxy is used to create appropriate routing rules when a new service is created to establish communication between containers via services within the cluster.</li> </ul> </li> </ul>"},{"location":"docs/kubernetes/cluster-architecture/#cluster","title":"Cluster","text":"<pre><code>flowchart\n  subgraph \"Cluster\"\n    id[\"Master Node (Control Plane)\"]\n    id1[\"Node 1 (Worker Node)\"]\n    id2[\"Node 2 (Worker Node)\"]\n  end</code></pre> <p>A cluster is a set of nodes grouped, with multiple nodes within a cluster, it can help to distribute or share the workload as well. As a result, your application will still be accessible even if one of the nodes fails.</p>"},{"location":"docs/kubernetes/cluster-architecture/#nodes","title":"Nodes","text":"<p>A node is a worker machine, it can be either a physical or virtual one depending on the Kubernetes cluster. Each of the node is managed by the control plane (master node) and a node can have multiple pods within it. This is where containers will be launched by Kubernetes.</p>"},{"location":"docs/kubernetes/cluster-architecture/#master-node-control-plane","title":"Master node (Control Plane)","text":"<p>Info</p> <p>Master node can be hosted in the form of containers.</p> <pre><code>flowchart\n  subgraph \"Master Node (Control Plane)\"\n    id[ETCD cluster] --&gt; kube-apiserver\n    id1[Controller Manager] --&gt; kube-apiserver\n    id2[kube-scheduler] --&gt; kube-apiserver\n  end</code></pre> <p>A master node is a node that is responsible for;</p> <ul> <li>managing the cluster</li> <li>managing, planning, scheduling, and monitoring the nodes</li> <li>storing the information regarding the cluster such as nodes, pods, configs, etc</li> <li>transferring the workload of the failed node to another worker node</li> </ul> <p>These tasks are performed by the master node through a set of components known as the control plane components.</p> <ul> <li>ETCD Cluster</li> <li>kube controller manager</li> <li>scheduler (kube-scheduler)</li> <li>kube-apiserver</li> </ul>"},{"location":"docs/kubernetes/cluster-architecture/#etcd-cluster","title":"ETCD Cluster","text":"<p>Info</p> <p>Reference</p> <p>ETCD is a distributed, reliable key-value stored database to store information regarding the cluster such as the nodes, pods, configs, secrets, accounts, roles, bindings, and others in a key-value format (JSON).</p> <ul> <li>cluster configuration data</li> <li>secrets and state information</li> <li>certificates and keys</li> </ul> <p>All information you see when you run the <code>kubectl get</code> command is from the ETCD server. Remember all changes made to the cluster like adding additional nodes, deploying pods, etc, will be updated in the ETCD server.</p> <p>There are two ways to deploy ETCD in Kubernetes environment.</p> <ul> <li> <p>Manual installation</p> <ul> <li>ETCD releases</li> <li>ETCD installation instructions<ul> <li><code>--advertise-client-urls 'http://{IPADDRESS}:2379'</code> = This is the address that ETCD listens, <code>2379</code> is the default port of ETCD listens. <code>kube-apiserver</code> will use this URL when trying to connect to ETCD, so this URL has to be configured on kube-apiserver configuration file.</li> </ul> </li> </ul> </li> <li> <p>kubeadm</p> <ul> <li>   Normally, we set up our cluster using <code>kubeadm</code> tool, the <code>kubeadm</code> tool will auto-deploy the ETCD server as a Pod in the <code>kube-system</code> namespace. Of course, you can <code>exec</code> into that pod to use the <code>etcdctl</code> command.<ul> <li><code>kubectl exec etcd-controlplane -n kube-system -- etcdctl get / --prefix --keys-only</code>. With this command, you can get all keys stored by Kubernetes, you will notice the root directory is the registry, and below that are various Kubernetes objects like nodes, pods, deployments, etc, as it stores data in a specific directory structure.</li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/kubernetes/cluster-architecture/#kube-controller-manager","title":"Kube Controller Manager","text":"<pre><code>---\ntitle: Example of various controllers\n---\nflowchart BT\n  subgraph \"kube controller manager\"\n    node-controller\n    replication-controller\n    namespace-controller\n  end</code></pre> <p>The kube controller manager manage various controllers in Kubernetes, for example;</p> <ul> <li>node-controller</li> <li>replication-controller</li> <li>namespace-controller</li> <li>deployment-controller</li> <li>endpoint-controller</li> <li>job-controller, etc</li> </ul> <p>The controller is a process that is responsible for monitoring the state of various components and resolving situations as necessary to the desired state. There are two ways to deploy kube controller manager in Kubernetes environment.</p> <ul> <li> <p>Manual installation</p> <ul> <li>kube controller manager releases</li> </ul> </li> <li> <p>kubeadm</p> <ul> <li>Similar to above, we can use <code>kubectl get pods -n kube-system</code> command to find the kube controller manager pod.</li> <li>Run on control-plane (master node)<ul> <li>If you want to see the kube-controller-manager pod config options, then you have to <code>cat /etc/kubernetes/manifests/kube-controller-manager.yaml</code>.</li> <li>If you want to see the kube-controller-manager service options, then you have to <code>cat /etc/systemd/system/kube-controller-manager.service</code>.</li> <li>If you want to see the kube-controller-manager running process, then you have to <code>ps -aux | grep kube-controller-manager</code>.</li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/kubernetes/cluster-architecture/#node-controller","title":"Node-controller","text":"<pre><code>flowchart RL\n  subgraph \"Master Node (Control Plane)\"\n    node-controller --&gt; controller-manager\n    controller-manager --&gt; kube-apiserver\n  end\n  node1[Worker Node 1] --&gt; kube-apiserver\n  node2[Worker Node 2] --&gt; kube-apiserver\n  node3[Worker Node 3] --&gt; kube-apiserver</code></pre> <p>The node controller monitors node status and takes necessary action to ensure that applications are running through the kube-apiserver.</p> <p></p> <p>Nodes are tested every 5 seconds to ensure the node is healthy by the node controller. After 40 seconds of not receiving heartbeats from a node, it marks the node as unreachable. The system gives a node 5 minutes to return after it has been marked unreachable. As a result, if the PODs are part of a replicaset, those pods will be removed from that node and will be provisioned those pods on the healthy nodes.</p>"},{"location":"docs/kubernetes/cluster-architecture/#replication-controller","title":"Replication-controller","text":"<pre><code>flowchart RL\n  subgraph \"Master Node (Control Plane)\"\n    replication-controller --&gt; controller-manager\n    controller-manager --&gt; kube-apiserver\n  end\n  node1[Worker Node 1] --&gt; kube-apiserver\n  node2[Worker Node 2] --&gt; kube-apiserver\n  node3[Worker Node 3] --&gt; kube-apiserver</code></pre> <p>The replication controller monitors the replicaset status and takes necessary action to ensure that the desired number of PODs are available at all times within the replication group. The POD creates a new one if it dies.</p>"},{"location":"docs/kubernetes/cluster-architecture/#kube-scheduler","title":"Kube Scheduler","text":"<p>Note</p> <p>It only decides which pod goes to which node. Additional add-on:</p> <ul> <li>Taints and tolerations</li> <li>Node Selectors</li> <li>Node Affinity</li> </ul> <p>The kube scheduler identifies and schedules the pods on nodes based on the pod resource requirements, but it does not place the pod on the nodes. The kubelet is the one who will place and create the pod on the node.</p> <p>The reason why we need a scheduler is because there could be different sizes of nodes and pods. You will need to ensure that the node has sufficient resources so that the pod can proceed. Therefore, each pod is analyzed by the scheduler to determine the best node.</p> <p><pre><code>---\ntitle: kube-scheduler ranking \n---\nflowchart LR\n  pod[\"Pod (CPU: 8)\"]\n  subgraph nodes[\"A List of nodes\"]\n    node1[\"Node 1 (CPU: 4)\"]\n    node2[\"Node 2 (CPU: 10)\"]\n    node3[\"Node 3 (CPU: 20)\"]\n  end\n  pod ---&gt; node3</code></pre> Here is an example, currently we have one pod with CPU requirements of 10. The kube scheduler will be going through 2 phases to identify and schedule the pod on the best node.</p> <ol> <li>The kube scheduler will filter out those nodes that do not fit the requirements. So in this case, node 1 will be filtered out as node 1 only has 4 CPUs.</li> <li>(Rank nodes) By using a priority function or class, the kube scheduler assigns a score and calculates how much free space is available on the nodes after the pod is placed. The highest score after calculation will place the pod on that node.<ul> <li>Assuming the priority score is 5<ul> <li>Score on node 2 = <code>10 - 5 = 5</code></li> <li>Score on node 3 = <code>20 - 5 = 15</code> (Win)</li> </ul> </li> </ul> </li> </ol> <p>There are two ways to deploy kube scheduler in Kubernetes environment.</p> <ul> <li> <p>Manual installation</p> <ul> <li>kube scheduler releases</li> </ul> </li> <li> <p>kubeadm</p> <ul> <li>Similar to above, we can use <code>kubectl get pods -n kube-system</code> command to find the kube scheduler pod.</li> <li>Run on control-plane (master node)<ul> <li>If you want to see the kube-scheduler pod config options, then you have to <code>cat /etc/kubernetes/manifests/kube-scheduler.yaml</code>.</li> <li>If you want to see the kube-scheduler running process, then you have to <code>ps -aux | grep kube-scheduler</code>.</li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/kubernetes/cluster-architecture/#kube-api-server","title":"Kube API Server","text":"<p>Info</p> <p>It orchestrates (manage) all cluster operations. The Kubernetes API is exposed for external users to manage the cluster, as well as for the controllers to monitor the state of the cluster and make the necessary changes, and for the worker nodes to communicate with the server.</p> <p>In Kubernetes, the kube-apiserver is the primary management component, which acts as the frontend to the cluster, that means all internal (controller-manager, ETCD, kube-scheduler, kubelet) and external communication to the cluster is via the kube-apiserver component.</p> <pre><code>---\ntitle: Kube-apiserver\n---\nflowchart \n  user --\"kubectl get 'something'\"--&gt; kube-apiserver\n  etcd[ETCD Cluster] --\"Retrieve data from ETCD Cluster\"--&gt; kube-apiserver</code></pre> <p>All the <code>kubectl</code> commands will first reach to the kube-apiserver. There are some steps that kube-apiserver to go through. These 3 steps normally are for <code>kubectl get &lt;something&gt;</code>.</p> <ol> <li>Authenticate the request</li> <li>Validate the request</li> <li>Retrieve the data from ETCD cluster and returns the requested information</li> </ol> <p>Bash<pre><code># Reference: https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/\ncurl -X POST /api/v1/namespace/default/pods\n</code></pre> Of course, you can interact with kube-apiserver by calling the API directly instead of using <code>kubectl</code> command. Here is an another example of creating a pod.</p> <ol> <li>Authenticate the request</li> <li>Validate the request</li> <li>Retrieve the data from ETCD cluster</li> <li>Create a Pod Object without the node assign, update the information in the ETCD server, and return the information (Pod has been created) back to the user</li> <li>The scheduler will identify and schedule the pod on node by monitoring the kube-apiserver and return the information back to kube-apiserver. After that, the kube-apiserver will update the information in the ETCD cluster</li> <li>The kube-apiserver will send that information to the appropriate worker node in the kubelet<ol> <li>The kubelet will create the pod on the node and tell the container runtime engine to deploy the application image.</li> <li>After everything is completed, the kubelet will update the status back to the kube-apiserver and the kube-apiserver will update the information in the ETCD cluster</li> </ol> </li> </ol> <p>All these steps are very similar when every time a change is requested and the only component that directly interacts with etcd is kube-apiserver. Scheduler, kube-controller manager, and kubelet use the kube-apiserver to perform updates in their respective areas of the cluster. </p> <p>There are two ways to deploy kube-apiserver in Kubernetes environment.</p> <ul> <li> <p>Manual installation</p> <ul> <li>kube-apiserver releases</li> </ul> </li> <li> <p>kubeadm</p> <ul> <li>Similar to above, we can use <code>kubectl get pods -n kube-system</code> command to find the kube-apiserver pod.</li> <li>Run on control-plane (master node)<ul> <li>If you want to see the kube-apiserver pod config options, then you have to <code>cat /etc/kubernetes/manifests/kube-apiserver.yaml</code>.</li> <li>If you want to see the kube-apiserver running process, then you have to <code>ps -aux | grep kube-apiserver</code>.</li> <li>If you want to see the kube-apiserver service, then you have to <code>cat /etc/systemd/system/kube-apiserver.service</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/kubernetes/cluster-architecture/#worker-node-workers","title":"Worker node (Workers)","text":""},{"location":"docs/kubernetes/cluster-architecture/#kubelet","title":"kubelet","text":"<pre><code>flowchart RL\n  subgraph \"Master Node (Control Plane)\"\n    kube-apiserver\n  end\n  subgraph \"Node 1 (Worker Node)\"\n    kubelet --&gt; kube-apiserver\n  end</code></pre> <p>The kubelet is an agent that runs on each node in the cluster. It will register the node with the kube-apiserver when it exists in worker node. It will listen the instructions from the kube-apiserver, when it receives the instructions to deploy a container or pod on the node, it will create the pod on the node and tell the container runtime engine to pull the required image and run as an application instance. Lastly, after everything is completed, the kubelet will update the status back to the kube-apiserver and the kube-apiserver will update the information in the ETCD cluster.</p> <p>Besides, the kube-api server retrieves kubelet status reports periodically to monitor the node and container/pod state. Lastly, kubelet ensures a specific number of pods are available on a single node.</p> <p>Summary;</p> <ul> <li>Register node</li> <li>Create Pod</li> <li>Monitor node &amp; PODs</li> </ul> <p>By default, kubelet is not automatically deploy by kubeadm tool. So, you have to install it manually on the worker nodes. Refer to this link for kubelet installation.</p> <ul> <li>Deploy <code>kubelet</code> as a pod is not a good practice, as the <code>kubelet</code> is designed to run as a system service on each node (host) in the Kubernetes cluster. This will ensure that the <code>kubelet</code> can manage the lifecycle of pods and containers on the node without any dependencies on the Kubernetes control plane.</li> </ul> <p>If you want to see the kubelet running process, then you have to <code>ps -aux | grep kubelet</code>. Remember the default <code>kubelet</code> configuration file normally is located at <code>/var/lib/kubelet/config.yaml</code> and <code>/etc/kubernetes/kubelet.conf</code>. This file <code>/etc/kubernetes/kubelet.conf</code> is used by the <code>kubelet</code> to authenticate with the kube-apiserver.</p>"},{"location":"docs/kubernetes/cluster-architecture/#kube-proxy","title":"kube-proxy","text":"<p>Info</p> <p>It is a network proxy that runs on each node in the cluster. It maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster (nodes can communicate with internal and external).</p> <pre><code>flowchart LR\n  external\n\n  subgraph Cluster\n    subgraph node1\n      kube-proxy1[kube-proxy]\n    end\n\n    subgraph node2\n      kube-proxy2[kube-proxy]\n    end\n\n    subgraph node3\n      kube-proxy3[kube-proxy]\n    end\n\n    kube-proxy1 --- kube-proxy2\n    kube-proxy2 --- kube-proxy3\n  end\n\n  external --- kube-proxy1\n  external --- kube-proxy2\n  external --- kube-proxy3</code></pre> <pre><code>flowchart\n  pod-network(POD network) --- pod1\n  pod-network(POD network) --- pod2\n\n  subgraph \"Node 2\"\n    pod1[\"Pod 2 (10.106.0.2)\"]\n  end\n  subgraph \"Node 1\"\n    pod2[\"Pod 1 (10.106.0.1)\"]\n  end</code></pre> <p>Every pod can reach other pods within a Kubernetes cluster by deploying a pod networking solution. A pod network is an internal virtual network connecting all nodes in a cluster so that the pods can communicate with each other. In this example, assuming Pod 1 is an API application and Pod 2 is the database. Pod 1 can access Pod 2 using the Pod 2 IP address, but the Pod 2 IP will not be guaranteed the same as always.</p> <pre><code>flowchart\n  pod-network(POD network) --- pod1\n  pod-network(POD network) --- pod2\n\n  subgraph \"Node 2\"\n    pod2[\"Pod 2 (10.106.0.2)\"]\n  end\n  subgraph \"Node 1\"\n    pod1[\"Pod 1 (10.106.0.1)\"]\n  end\n\n  service[\"service.db (10.106.0.3)\"]\n\n  pod2 --- service\n  pod1 --- service</code></pre> <p>So, we have to create a Kubernetes service object that exposes the database across the cluster, which means Pod 1 (API application) can access Pod 2 (database) by using the service name and the service will also assign an IP address. </p> <p>Warning</p> <p>But how does the service get its IP address? Does the service join the same pod network as well?</p> <p>Unfortunately, it does not join the pod network. Here is the explanation.</p> <ul> <li>Service objects are not actually objects: they have no interfaces or active listening processes, and they are virtual components within Kubernetes that only live in memory, so they cannot join the pod network.</li> </ul> <pre><code>flowchart\n  pod-network(POD network) --- pod1\n  pod-network(POD network) --- pod2\n\n  subgraph \"Node 2\"\n    direction TB\n    pod2[\"Pod 2 (10.106.0.2)\"]\n    kube-proxy2[kube-proxy]\n    pod2 --- kube-proxy2\n    table2[IPTable: 10.106.0.3 --&gt; 10.106.0.2]\n  end\n  subgraph \"Node 1\"\n    direction TB\n    pod1[\"Pod 1 (10.106.0.1)\"]\n    kube-proxy1[kube-proxy]\n    pod1 --- kube-proxy1\n    table1[IPTable: 10.106.0.3 --&gt; 10.106.0.2]\n  end\n\n  service[\"service.db (10.106.0.3)\"]\n\n  service --- kube-proxy2\n  service --- kube-proxy1</code></pre> <p>However, we also stated that the service should be accessible from any node in the cluster. With the help of kube-proxy, we can achieve that. Each node in the Kubernetes cluster runs Kube-proxy, which looks for new services and creates appropriate routing rules on the worker nodes to forward traffic to the new services to pods whenever a new service is created. It will ensure that the containers can talk to each other via services within the cluster.</p> <p>This can be done through IPTables rules. It will create an IPTable rules on each node in the cluster to know that the Database service is actually point to Pod 2 (database), 10.106.0.3 --&gt; 10.106.0.2. This is how kube-proxy configures a new service.</p> <p>How to deploy kube-proxy?</p> <ul> <li>kubeadm<ul> <li>Similar to other Kubernetes components, we can use <code>kubectl get pods -n kube-system</code> command to find the kube-proxy pod.</li> <li>Do take note that, kube-proxy is actually deployed as a Daemonset, so the kube-proxy pod will always be deployed on each new node in the cluster.<ul> <li><code>kubectl get daemonset -n kube-system</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/kubernetes/cluster-components-security/","title":"Cluster Components Security","text":""},{"location":"docs/kubernetes/cluster-components-security/#secure-controller-manager-and-scheduler","title":"Secure controller manager and scheduler","text":"<p>In summary;</p> <ul> <li>Isolate the controller manager and scheduler on another node</li> <li>Apply Role-Based Access Control (RBAC) to limit what the controller manager and scheduler can do</li> <li>Secure communication using Transport Layer Security (TLS) between all the components in the cluster</li> <li>Enable audit logging to keep track the activities and monitor the activities of the controller manager and scheduler</li> <li>Secure default configurations and protect the configuration files.</li> <li>Update/Run the Kubernetes latest version.</li> <li>Regularly scan for vulnerabilities in the cluster components.</li> </ul> <pre><code>graph LR\n  subgraph C[Cluster]\n    subgraph n1[Node 1]\n      kube-controller-manager\n      kube-scheduler\n    end\n\n    subgraph n2[Node 2]\n      pod1[pod]\n      pod2[pod]\n    end\n\n    subgraph n3[Node 3]\n      pod3[pod]\n    end\n  end</code></pre> <p>When we want to protect the controller manager and scheduler, we have to isolate them, and we can do this by running them on a separate node. This way, if the application running on the node is compromised, the controller manager and scheduler are not affected.</p> <pre><code>graph LR\n  RBAC --&gt; n1 \n\n  subgraph C[Cluster]\n    subgraph n1[Node 1]\n      kube-controller-manager\n      kube-scheduler\n    end\n  end</code></pre> <p>We also have to apply Role-Bsaed Access Control (RBAC) to limit what the controller manager and scheduler can do. For example, replication controller only needs to manage pod replicas and the scheduler only needs to schedule pods. In this casem you can configure RBAC to only allow these actions, so if they are being compromised, the attacker can't do much due to the limited access.</p> <pre><code>graph LR\n  subgraph C[Cluster]\n    subgraph n1[Node 1]\n      kube-controller-manager --&gt; TLS\n      kube-scheduler --&gt; TLS\n    end\n  end</code></pre> <p>We have to secure communication using Transport Layer Security (TLS) between all the components in the cluster. This way, all data transferred between the components is encrypted and secure. Remember to rotate the TLS certificates regularly.</p> <p>Also, we have to enable audit logging to keep track the activities and monitor the activities of the controller manager and scheduler. This way, we can refer to the audit logs to investigate any suspicious activities. You can use Prometheus and Grafana to monitor the activities of the controller manager and scheduler.</p>"},{"location":"docs/kubernetes/cluster-components-security/#secure-kubelet","title":"Secure kubelet","text":"<p>Info</p> <p>Refer to the Kubelet section to understand the kubelet.</p> <p>In summary;</p> <ul> <li>Set the authorization mode to Webhook</li> <li>Disable anonymous access and enable supported authentication mechanims</li> </ul> <p>Before we secure the kubelet, we need to identify the kubelet configuration file. We know that kubelet is installed as a systemd service. So, we can use the following command to identify the kubelet configuration file.</p> Bash<pre><code>ps -aux | grep kubelet\ncat &lt;configuration-file-path&gt;\n</code></pre> <p><code>kubelet</code> expose two ports, 10250 and 10255.</p> Port Description 10250 Serves the kubelet API that allows full access 10255 Serves the kubelet API that allows unauthenticated/unauthorized read-only access"},{"location":"docs/kubernetes/cluster-components-security/#set-the-authorization-mode-to-webhook","title":"Set the authorization mode to Webhook","text":"<p>By default, the <code>kubelet</code> is configured to allow all requests/access from the kube-apiserver to its API. So when you do a <code>curl</code> request to the kubelet API, you can see the response. You can find the <code>kubelet API endpoints</code> in this URL.</p> Bash<pre><code>curl -k https://localhost:10250/pods\n\n# Output\n{\"kind\":\"PodList\",\"apiVersion\":\"v1\",\"metadata\":{},\"items\":[{\"metadata\":{\"name\":\"kube-scheduler-controlplane\",\"namespace\":\"kube-system\",\"uid\":\"471ac3f1f2864d9e1bca566703c10b41\",\"creationTimestamp\":null,\"labels\":{\"component\":\"kube-scheduler\",\"tier\":\"control-plane\"}},...\n</code></pre> <p>However, this is not secure because it allow full access to the kubelet API. So, we have to set authorization mode to <code>Webhook</code> in the kubelet configuration file. By setting the authorization mode to <code>Webhook</code>, the kubelet will send a request to the kube-apiserver to determine if the request is authorized.</p> /var/lib/kubelet/config.yaml<pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nauthorization:\n  mode: Webhook\n</code></pre>"},{"location":"docs/kubernetes/cluster-components-security/#authenticate-and-authorize-kubelet-requests","title":"Authenticate and authorize kubelet requests","text":"<p>Now the use of pod <code>10255</code> is deprecated and it is disabled by default in the latest version of Kubernetes. However, let's assume you are using an older version of Kubernetes that still uses the <code>10255</code> port.</p> <p>The <code>10255</code> port is similar to the <code>10250</code> port, just it only provides read-only access to any unauthorized and unauthenticated user. This exposes a risk to the cluster, as an attacker if knows the host IP address with the <code>10255</code> port open, they can get or do some malicious activities.</p> Bash<pre><code>curl -k https://localhost:10255/metrics\n</code></pre> <p>By default, the kubelet permits all requests from anonymous users (username: system:anonymous and group: system:unauthenticated) to the API without any authentication. So, to prevent that, we have to ensure all requests to the kubelet API are authenticated and authorized. The following YAML configuration file are applicable to both ports.</p> /var/lib/kubelet/config.yaml<pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nauthentication:\n  anonymous:\n    enabled: false\n  x509:\n    clientCAFile: /etc/kubernetes/pki/ca.crt # path to the client CA file\n</code></pre> <p>There are two authentication mechanims;</p> <ul> <li>Certificate based</li> <li>API bearer token based</li> </ul> <p>After you have set the authentication mechanims, you have to restart the kubelet service to apply the changes.</p> Bash<pre><code>systemctl restart kubelet.service\ncurl -k https://localhost:10250/pods \\\n  --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \\\n  --key /etc/kubernetes/pki/apiserver-kubelet-client.key\n</code></pre> <ul> <li>This time, you have to pass in the certificate and key to authenticate the request.</li> </ul>"},{"location":"docs/kubernetes/cluster-components-security/#secure-container-runtime","title":"Secure container runtime","text":"<p>Before we learn how to secure the container runtime, it's important to understand some CVE Ids.</p> CVE Id Description CVE-2019-5736 This vulnerability affects Docker and other container runtimes that use runc. It allows an attacker to overwrite the host runc binary and gain root-level code execution on the host. CVE-2020-15257 This vulnerability affects containerd, a core container runtime used by Docker and Kubernetes. It allows an attacker to gain access to the containerd API without proper authentication. CVE-2021-30465 This vulnerability affects Docker and other container runtimes that use runc. It allows an attacker to escape the container and execute arbitrary code on the host. CVE-2021-32760 This vulnerability affects containerd. It allows an attacker to cause a denial of service (DoS) by sending a specially crafted request to the containerd API. CVE-2022-23648 This vulnerability affects containerd. It allows an attacker to gain unauthorized access to the containerd API and perform actions such as creating or deleting containers. CVE-2022-0811 This vulnerability affects CRI-O, a lightweight container runtime for Kubernetes. It allows an attacker to escalate privileges and execute arbitrary code on the host <pre><code>graph LR\n  subgraph C[Cluster]\n    subgraph n1[Node 1]\n      container-runtime\n    end\n  end</code></pre> <p>Each node in the cluster has a container runtime. To secure container runtime;</p> <ul> <li>Regularly update the container runtime to the latest version</li> <li>Run container with the least privilege and avoid running containers as root YAML<pre><code>spec:\n  securityContext:\n    runAsUser: 2000 # user id\n    runAsGroup: 3000 # group id\n</code></pre></li> <li>Set read-only root filesystem to prevent the container from writing to the root filesystem   YAML<pre><code>spec:\n  securityContext:\n    readOnlyRootFilesystem: true\n</code></pre></li> <li>Limit the resources (CPU, Memory) that the container can use. So that we can prevent container from consuming all the resources on the node and can prevent denial-of-service (DoS)attacks.   YAML<pre><code>spec:\n  resources:\n    limits:\n      cpu: 1\n      memory: 1Gi\n</code></pre></li> <li> <p>Apply security profiles like AppArmor or SELinux to restrict the actions that the container can perform, also to add an extra layer of security by enforcing mandatory access control policies on containers.</p> <ul> <li>These profiles will restrict what the container can do, for example, it can restrict the container from accessing the host filesystem or network.</li> <li>SELinux (Security-Enhanced Linux) is a Linux kernel security module that provides a mechanism for supporting access control security policies. It's policies can define how processes and users can access resources on the system.   sample.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n  - name: sample-container\n    image: nginx\n    securityContext:\n      seLinuxOptions:\n        user: \"system_u\" # The SELinux user label that applies to the container\n        role: \"system_r\" # The SELinux role label that applies to the container.\n        type: \"svirt_lxc_net_t\" # The SELinux type label that applies to the container.\n        level: \"s0:c123,c456\" # The SELinux level label that applies to the container.\n</code></pre></li> <li>AppArmor (Application Armor) is a Linux security module that allows the system administrator to restrict programs' capabilities with profiles.   YAML<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\n  annotations:\n    # refers to a profile that is defined locally on the node\n    container.apparmor.security.beta.kubernetes.io/sample-container: localhost/sample-profile\nspec:\n  containers:\n  - name: sample-container\n    image: nginx\n</code></pre></li> </ul> </li> <li> <p>Transition to supported container runtime like containerd or CRI-O, as Docker is deprecated in Kubernetes, and to ensure compatibility with the latest version of Kubernetes.</p> </li> <li>Implement monitoring and logging like Prometheus, Grafana, etc to detect and investigate any suspicious activities in the container runtime. Enable audit logging to keep track of the activities in the contaienr runtime.</li> </ul>"},{"location":"docs/kubernetes/cluster-components-security/#secure-kube-proxy","title":"Secure kube-proxy","text":"<p>Info</p> <p>Refer to the kube-proxy section to understand the kube-proxy.</p> <p>In summary;</p> <ul> <li>Set kube-proxy configuration with proper permission</li> <li>Secure communication using Transport Layer Security (TLS) between the kube-proxy and kube-apiserver</li> <li>Set kube-proxy with least privilege</li> <li>Set network policies to restrict the access to the kube-proxy service</li> <li>Set logging and monitoring to detect and investigate any suspicious activities in the kube-proxy</li> <li>Regularly scan and update for vulnerabilities in the kube-proxy</li> <li>Enable audit logging to keep track the activities and monitor the activities of the kube-proxy</li> </ul> <p>Before we secure the kube-proxy, we need to identify the kube-proxy configuration file. There are two ways to identify the kube-proxy configuration file.</p> Bash<pre><code># If kube-proxy deploys as pods\nkubectl describe pod/kube-proxy-xxxx -n kube-system\n\n# YAML output\nCommand:\n  /usr/local/kube-proxy\n  --config=/var/lib/kube-proxy/config.conf\n\n# If deploy as systemd service\nps -aux | grep kube-proxy\n</code></pre> /var/lib/kube-proxy/config.conf<pre><code>apiVersion: kubeproxy.config.k8s.io/v1alpha1\nbindAddress: 0.0.0.0\nbindAddressHardFail: false\nclientConnection:\n  acceptContentTypes: \"\"\n  burst: 0\n  contentType: \"\"\n  kubeconfig: /var/lib/kube-proxy/kubeconfig.conf\n  qps: 0\nclusterCIDR: 10.244.0.0/16\nconfigSyncPeriod: 0s\nconntrack:\n  maxPerCore: 0\n  min: null\n  tcpBeLiberal: false\n  tcpCloseWaitTimeout: null\n  tcpEstablishedTimeout: null\n  udpStreamTimeout: 0s\n  udpTimeout: 0s\ndetectLocal:\n  bridgeInterface: \"\"\n  interfaceNamePrefix: \"\"\ndetectLocalMode: \"\"\nenableProfiling: false\nhealthzBindAddress: \"\"\nhostnameOverride: \"\"\niptables:\n  localhostNodePorts: null\n  masqueradeAll: false\n  masqueradeBit: null\n  minSyncPeriod: 1s\n  syncPeriod: 0s\nipvs:\n  excludeCIDRs: null\n  minSyncPeriod: 0s\n  scheduler: \"\"\n  strictARP: false\n  syncPeriod: 0s\n  tcpFinTimeout: 0s\n  tcpTimeout: 0s\n  udpTimeout: 0s\nkind: KubeProxyConfiguration\nlogging:\n  flushFrequency: 0\n  options:\n    json:\n      infoBufferSize: \"0\"\n    text:\n      infoBufferSize: \"0\"\n  verbosity: 0\nmetricsBindAddress: \"\"\nmode: iptables\nnftables:\n  masqueradeAll: false\n  masqueradeBit: null\n  minSyncPeriod: 0s\n  syncPeriod: 0s\nnodePortAddresses: null\noomScoreAdj: null\nportRange: \"\"\nshowHiddenMetricsForVersion: \"\"\nwinkernel:\n  enableDSR: false\n  forwardHealthCheckVip: false\n  networkName: \"\"\n  rootHnsEndpointName: \"\"\n  sourceVip: \"\"\n</code></pre> <p>This kube-proxy configuration file <code>/var/lib/kube-proxy/config.conf</code> contains the configuration for the kube-proxy to communicate with the kube-apiserver.</p> <p>First, we have to check the file and group permission of the kube-proxy configuration file. After that, we need to set permission to 644 or stricter and make sure the ownership is set to root:root to protect the configuration file, as it does not allow write access by other users.</p> Bash<pre><code>ls -l /var/lib/kube-proxy/config.conf\nchmod 644 /var/lib/kube-proxy/config.conf\nchown root:root /var/lib/kube-proxy/config.conf\n</code></pre> <ul> <li>r (read) = 4, w (write) = 2, x (execute) = 1</li> <li>644 = This ensures that the owner can read and modify the file, while the group and others can only read the file.</li> </ul> <p>Next, we have to secure communication using Transport Layer Security (TLS) between the kube-proxy and kube-apiserver. This way, all data transferred between the kube-proxy and kube-apiserver is encrypted and secure.</p> /var/lib/kube-proxy/kubeconfig.conf<pre><code>apiVersion: v1\nkind: Config\nclusters:\n- cluster:\n    # validate the TLS certificate of the kube-apiserver\n    certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n    server: https://kind-cluster-control-plane:6443\n  name: default\ncontexts:\n- context:\n    cluster: default\n    namespace: default\n    user: default\n  name: default\ncurrent-context: default\nusers:\n- name: default\n  user:\n    # uses the service account token to authenticate to the kube-apiserver\n    tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token\n</code></pre> <p>Also, we will need to enable audit logging to keep track the activities and monitor the activities of the kube-proxy. This way, we can refer to the audit logs to investigate any suspicious activities.</p> <p>audit.yaml<pre><code>apiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n  # Log all requests at the Metadata level.\n  - level: Metadata\n    resources:\n      - group: \"\"\n        resources: [\"pods\", \"services\", \"endpoints\"]\n      - group: \"extensions\"\n        resources: [\"ingresses\"]\n      - group: \"networking.k8s.io\"\n        resources: [\"networkpolicies\"]\n  # Log all requests at the RequestResponse level for kube-proxy.\n  - level: RequestResponse\n    users: [\"system:kube-proxy\"]\n    verbs: [\"create\", \"update\", \"patch\", \"delete\"]\n    resources:\n      - group: \"\"\n        resources: [\"pods\", \"services\", \"endpoints\"]\n      - group: \"extensions\"\n        resources: [\"ingresses\"]\n      - group: \"networking.k8s.io\"\n        resources: [\"networkpolicies\"]\n</code></pre> This policy logs all requests at the Metadata level for pods, services, endpoints, ingresses, and network policies, and logs all create, update, patch, and delete requests at the RequestResponse level for the kube-proxy user.</p>"},{"location":"docs/kubernetes/cluster-components-security/#secure-etcd","title":"Secure ETCD","text":"<p>Info</p> <p>Refer to the etcd section to understand the etcd.</p> <p>In summary;</p> <ul> <li>Encrypting data at REST (etcd and persistent volumes)</li> <li>Regular backup ETCD data</li> <li>Secure communication using Transport Layer Security (TLS) between the etcd and other components in the cluster</li> </ul>"},{"location":"docs/kubernetes/cluster-components-security/#secure-container-networking","title":"Secure container networking","text":"<pre><code>graph\n  subgraph cluster\n    subgraph node1\n      pod1[pod]\n    end\n\n    subgraph node2\n      pod2[pod]\n    end\n\n    subgraph node3\n      pod3[pod]\n    end\n\n    pod1 --- pod2 --- pod3\n  end</code></pre> <p>By default, we know that Kubernetes networking is flat and unsecured. This means that all pods can communicate with each other, and there is no network policy to restrict the communication between the pods.</p> <p>In summary;</p> <ul> <li>Implement network policies to restrict the communication (incoming and outgoing traffic) between the pods</li> <li>Use service mesh like Istio to secure the communication between the pods. Service mesh provides mutual TLS for encrypted and authenticated communication, traffic management, and observability</li> <li>Encrypt the network traffic between the pods using IPSec or WireGuard. This will ensure that all data transferred between the pods is encrypted and secure</li> <li>Isolate sensitive workloads by using namespaces and apply network policies. Segregate workloads by using namespaces and apply network policies can help to reduce impact if one of the workloads is compromised.</li> </ul>"},{"location":"docs/kubernetes/cluster-components-security/#secure-storage","title":"Secure storage","text":"<p>Before we learn how to secure storage, it is important to understand the security issues impact on Kubernetes cluster. For example, </p> <ul> <li>misconfiguration of storage access will let the attackers to access the sensitive data</li> <li>lack of encryption will lead to data leakage</li> <li>insufficient backup will lead to data loss due to system failures or attacks</li> </ul> <p>In summary;</p> <ul> <li>Encrypting data at REST (etcd and persistent volumes). For example, you can use AWS EBS, Azure Disk Storage, etc, these providers actually offer encryption options.</li> <li>Use Role-Based Access Control (RBAC) to limit the access to the storage. For example, you can grant only authorized users to access the storage.</li> <li>Use Storage class to enforce security, performance limits, and backup policies. You can enable encryption in StorageClass. This will ensure that all data stored in the persistent volume is encrypted.   sample-storage-class.yaml<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: example-st\nprovisioner: ebs.csi.aws.com\nparameters:\n  csi.storage.k8s.io/fstype: xfs\n  type: io1\n  iopsPerGB: \"50\" # performance limits\n  encrypted: \"true\"\n  tagSpecification_1: \"key1=value1\"\n  tagSpecification_2: \"key2=value2\"\n</code></pre><ul> <li>IOPS represents the number of read and write operations per second that the volume can support. So, the higher the IOPS, the better the performance.</li> </ul> </li> <li>Regularly backup and prepare for disaster recovery plan. You can use Velero, Heptio Ark, Kasten, etc to backup the data in the cluster.</li> <li>Monitor and audit the storage metrics to detect and investigate any suspicious activities in the storage. Enable audit logging to keep track the activities in the storage.</li> </ul>"},{"location":"docs/kubernetes/cluster-role/","title":"Cluster Role","text":""},{"location":"docs/kubernetes/cluster-role/#usage-and-concept-of-cluster-role","title":"Usage and Concept of Cluster Role","text":"<p>Reference</p> <p>The concept of cluster role is similar to role, but the difference is that cluster role is not namespaced. It is used to grant permissions to resources across all namespaces. Cluster roles are useful for cluster-wide permissions.</p> <ul> <li>cluster-scoped resources (like nodes, namespaces, etc.)</li> <li>non-resource endpoints (like <code>/healthz</code>, <code>/version</code>, etc.)</li> <li>namespaced resources (like pods, services, etc.) across all namespaces<ul> <li>For example, you can create a cluster role to grant access to all pods in all namespaces.</li> </ul> </li> </ul> Bash<pre><code># list all resources under namespace\nkubectl api-resources --namespaced=false\n\n# this will help you to get the verb\nkubectl api-resources --namespaced=false --sort-by name -o wide\n</code></pre> NAME APIVERSION KIND componentstatuses v1 ComponentStatus namespaces v1 Namespace nodes v1 Node persistentvolumes v1 PersistentVolume mutatingwebhookconfigurations admissionregistration.k8s.io/v1 MutatingWebhookConfiguration validatingadmissionpolicies admissionregistration.k8s.io/v1 ValidatingAdmissionPolicy validatingadmissionpolicybindings admissionregistration.k8s.io/v1 ValidatingAdmissionPolicyBinding validatingwebhookconfigurations admissionregistration.k8s.io/v1 ValidatingWebhookConfiguration customresourcedefinitions apiextensions.k8s.io/v1 CustomResourceDefinition apiservices apiregistration.k8s.io/v1 APIService selfsubjectreviews authentication.k8s.io/v1 SelfSubjectReview tokenreviews authentication.k8s.io/v1 TokenReview selfsubjectaccessreviews authorization.k8s.io/v1 SelfSubjectAccessReview selfsubjectrulesreviews authorization.k8s.io/v1 SelfSubjectRulesReview subjectaccessreviews authorization.k8s.io/v1 SubjectAccessReview certificatesigningrequests certificates.k8s.io/v1 CertificateSigningRequest flowschemas flowcontrol.apiserver.k8s.io/v1 FlowSchema prioritylevelconfigurations flowcontrol.apiserver.k8s.io/v1 PriorityLevelConfiguration ingressclasses networking.k8s.io/v1 IngressClass runtimeclasses node.k8s.io/v1 RuntimeClass clusterrolebindings rbac.authorization.k8s.io/v1 ClusterRoleBinding clusterroles rbac.authorization.k8s.io/v1 ClusterRole priorityclasses scheduling.k8s.io/v1 PriorityClass csidrivers storage.k8s.io/v1 CSIDriver csinodes storage.k8s.io/v1 CSINode storageclasses storage.k8s.io/v1 StorageClass volumeattachments storage.k8s.io/v1 VolumeAttachment <p>Now, from the above table, we can see that clusterroles and clusterrolebindings are under cluster scope, meaning that they are created within a cluster. So, we no need to specify the namespace while creating them.</p>"},{"location":"docs/kubernetes/cluster-role/#step-1-create-a-cluster-role","title":"Step 1: Create a Cluster Role","text":"<p>You can use the following command to get the verb for the resource.</p> Bash<pre><code>kubectl api-resources --namespaced=false --sort-by name -o wide\n</code></pre> clusterrole.yaml<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: admin-reader\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"] # use * to allow all resources\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"secrets\"] # accept namespaced resources as well\n    verbs: [\"get\", \"list\", \"watch\"] # use * to allow all verbs\n  - nonResourceURLs: [\"/healthz\", \"/logs/*\"]\n    verbs: [\"get\"]\n</code></pre> <ul> <li><code>apiGroups</code> - The API group of the resource. If you are not sure, you can see the above table (API Version).</li> <li><code>resources: [\"secrets\"]</code> - You can create a cluster role to grant access to all secrets in all namespaces. So, the user can access all secrets in all namespaces.</li> </ul> Bash<pre><code># You cannot create a role with different rules, as they will mix it up\nkubectl create clusterrole admin-reader --verb=get,watch,list --resource=nodes\nkubectl create clusterrole admin-reader --verb=get,watch,list --resource=secrets\nkubectl create clusterrole admin-reader --verb=get --non-resource-url=/logs/*,/healthz\n\nkubectl get clusterroles\n</code></pre>"},{"location":"docs/kubernetes/cluster-role/#step-2-create-a-cluster-role-binding","title":"Step 2: Create a Cluster Role Binding","text":"<p>Link the role to a user, group or service account.</p> rolebinding.yaml<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-binding\nsubjects: # users, groups, or service accounts\n  - kind: Group\n    name: manager\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: admin-reader # cluster role name\n</code></pre> Bash<pre><code>kubectl create clusterrolebinding admin-binding --role=admin-reader --group=manager\nkubectl get clusterrolebinding\n</code></pre>"},{"location":"docs/kubernetes/cluster-upgrade/","title":"Cluster Upgrade","text":""},{"location":"docs/kubernetes/cluster-upgrade/#introduction-of-cluster-upgrade","title":"Introduction of Cluster Upgrade","text":"<p>So, we know that the Kubernetes components can have their own versions, for example;</p> <ul> <li>kube-apiserver: v1.32.0</li> <li>kube-scheduler: v1.32.0</li> <li>kubelet: v1.32.0</li> <li>kube-proxy: v1.32.0</li> <li>controller-manager: v1.32.0</li> <li>etcd: v3.5.17</li> <li>coredns: v1.12.0</li> </ul> <p>Remember, it is not mandatory to have the same version for all the components. But, it is recommended to have the same version for all the components, except for the etcd and coredns. There is one thing to remember, kube-apiserver is the primary component, so assume that the kube-apiserver is <code>v1.32.0</code>, then the other components like kube-scheduler, kubelet, kube-proxy, controller-manager should be less than or equal to v1.32.0, so that there is no compatibility issue.</p> <p>If you used <code>kubeadm</code> tool to deploy your cluster, then the <code>kubeadm</code> tool itself can help you to plan and upgrade the cluster.</p> Bash<pre><code>kubeadm upgrade plan # gives you a lot of information\nkubeadm upgrade apply &lt;version&gt;\n</code></pre>"},{"location":"docs/kubernetes/cluster-upgrade/#process-of-cluster-upgrade","title":"Process of Cluster Upgrade","text":"<p>pkgs reference</p> <p>kubeadm upgrade</p> <p>Here is the scenario, we have a Kubernetes cluster with a master and 2 workers nodes are running. The current version of the cluster is <code>v1.21.0</code>. Now, we want to upgrade the cluster to <code>v1.22.0</code>.</p>"},{"location":"docs/kubernetes/cluster-upgrade/#step-1-upgrade-the-master-node","title":"Step 1: Upgrade the Master node","text":"<p>When the master node is being upgraded, the control plane components like <code>kube-apiserver</code>, <code>kube-scheduler</code>, and <code>kube-controller-manager</code> will go down, but your current worker nodes will continue to work without any issue. Just we cannot deploy, modify, and delete the existing resources as the control plane components are down.</p> Bash<pre><code># replace x with the version you picked for this upgrade, for example 1.29, 1.32, etc.\necho \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.x/deb/ /\" | sudo tee /etc/apt/sources.list.d/kubernetes.list\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.x/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo apt-get update\n\nsudo apt update\nsudo apt-cache madison kubeadm # you can copy the version that you want to upgrade\n\n# replace x in 1.31.x-* with the latest patch version\nsudo apt-mark unhold kubeadm &amp;&amp; \\\nsudo apt-get update &amp;&amp; sudo apt-get install -y kubeadm='1.31.x-*' &amp;&amp; \\\nsudo apt-mark hold kubeadm\n\nkubeadm version\nsudo kubeadm upgrade plan # gives you a lot of information\n\n# replace x with the patch version you picked for this upgrade\n# upgrade apply is for master side\nsudo kubeadm upgrade apply v1.31.x\n\n# after you upgrade the cluster, when you execute kubectl get nodes\n# you will still see the version is not updated, as that version is the kubelet version\n# not the kube-apiserver version\n# so you have to upgrade the kubelet version if have installed\n# So you need to drain the node first\n# replace &lt;node-to-drain&gt; with the name of your node you are draining\nkubectl drain &lt;node-to-drain&gt; --ignore-daemonsets\n\n# replace x in 1.31.x-* with the latest patch version\nsudo apt-mark unhold kubelet kubectl &amp;&amp; \\\nsudo apt-get update &amp;&amp; sudo apt-get install -y kubelet='1.31.x-*' kubectl='1.31.x-*' &amp;&amp; \\\nsudo apt-mark hold kubelet kubectl\n\nsudo systemctl daemon-reload\nsudo systemctl restart kubelet\n\nkubectl uncordon &lt;node-to-uncordon&gt;\n</code></pre>"},{"location":"docs/kubernetes/cluster-upgrade/#step-2-upgrade-the-worker-nodes","title":"Step 2: Upgrade the Worker nodes","text":"<p>There are different ways to upgrade the worker nodes;</p> <ul> <li>Upgrade all of them at once, but all the pods will be down. (Downtime)</li> <li>Upgrade one by one, but the pods will be rescheduled to the other worker nodes. (No Downtime)</li> <li>Add new worker nodes with the new version, this is very useful if you're on a cloud provider. (No Downtime)<ul> <li>Add the new worker node</li> <li>Move the pods (workload) from the old worker node to the new worker node</li> <li>Remove the old worker node</li> </ul> </li> </ul> <p>For the demo, we will upgrade one worker node at once. Repeat these steps for all the worker nodes.</p> Bash<pre><code># replace x with the version you picked for this upgrade, for example 1.29, 1.32, etc.\necho \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.x/deb/ /\" | sudo tee /etc/apt/sources.list.d/kubernetes.list\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.x/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo apt-get update\n\n# replace x in 1.31.x-* with the latest patch version\nsudo apt-mark unhold kubeadm &amp;&amp; \\\nsudo apt-get update &amp;&amp; sudo apt-get install -y kubeadm='1.31.x-*' &amp;&amp; \\\nsudo apt-mark hold kubeadm\n\n# upgrade node is for worker side\nsudo kubeadm upgrade node\n\n# execute this command on a control plane node\n# replace &lt;node-to-drain&gt; with the name of your node you are draining\nkubectl drain &lt;node-to-drain&gt; --ignore-daemonsets\n\n# replace x in 1.31.x-* with the latest patch version\nsudo apt-mark unhold kubelet kubectl &amp;&amp; \\\nsudo apt-get update &amp;&amp; sudo apt-get install -y kubelet='1.31.x-*' kubectl='1.31.x-*' &amp;&amp; \\\nsudo apt-mark hold kubelet kubectl\n\nsudo systemctl daemon-reload\nsudo systemctl restart kubelet\nkubectl uncordon &lt;node-to-uncordon&gt;\n</code></pre>"},{"location":"docs/kubernetes/command-and-args/","title":"Commands and Arguments","text":""},{"location":"docs/kubernetes/command-and-args/#usage-of-commands-and-arguments","title":"Usage of commands and arguments","text":"<pre><code>flowchart LR\n  A[Dockerfile-ENTRYPOINT] --&gt; C\n  B[Dockerfile-CMD] --&gt; D\n  C[YAML-command]\n  D[YAML-args]</code></pre> pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-name\nspec:\n  containers:\n    - name: container-name\n      image: ubuntu:22.04\n      command: [\"sleep\"] # entrypoint in Dockerfile\n      args: [\"10\"] # CMD in Dockerfile\n</code></pre>"},{"location":"docs/kubernetes/container-runtime/","title":"Container Runtime","text":"<p>Info</p> <p>Highly recommended to use crictl to debug Kubernetes nodes.</p> <p>In summary;</p> <ul> <li>containerd provides a container runtime</li> <li>ctr is a low-level debugging, mainly for debugging contained</li> <li>nerdctl is a general purpose, Docker-like CLI (alternative to Docker) to containerd</li> <li>crictl interacts with any CRI runtime, mainly for inspecting and debugging the container runtimes, it's good for Kubernetes users.</li> </ul>"},{"location":"docs/kubernetes/container-runtime/#why-kubernetes-no-need-to-install-docker","title":"Why Kubernetes no need to install Docker?","text":"<pre><code>flowchart TD\n  cri[\"Container Runtime Interface (CRI)\"] ---&gt; Kubernetes\n\n  subgraph vendor[Container Runtime]\n    rkt\n    containerd\n    docker[Docker Engine]\n    crio[CRI-O]\n  end\n\n  vendor ---&gt; cri</code></pre> <p>In summary, Kubernetes supports other runtime engines that adhere to the OCI standards, like containerd or Rocket, so it is not necessary to install Docker before installing Kubernetes.</p> <p>Kubernetes only needs the Container Runtime Interface (CRI) to work as a container runtime to perform the container operations as long as they adhere to the OCI standards. Therefore, Kubernetes is compatible with any runtime engines via CRI, such as Rocket, containerd, etc.</p>"},{"location":"docs/kubernetes/container-runtime/#containerd","title":"containerD","text":"<p>Reference</p> <ul> <li>https://containerd.io/</li> <li>https://github.com/containerd/containerd</li> <li>https://github.com/containerd/containerd/tree/main/cmd/ctr</li> <li>https://labs.iximiuz.com/courses/containerd-cli/ctr/container-management#recap-what-is-ctr</li> </ul> <pre><code>flowchart LR\n  subgraph CLI[Command-line containerd clients]\n    ctr\n    nerdctl\n    crictl\n  end\n\n  ctr ---&gt; containerd\n  nerdctl ---&gt; containerd\n  crictl ---&gt; containerd\n\n  subgraph vendor[Container Runtime]\n    containerd\n  end\n\n  vendor ---&gt; cri[\"Container Runtime Interface (CRI)\"] ---&gt; Kubernetes</code></pre> <p>If you don't need Docker's other features, you can just install containerD without installing Docker. When you install containerD, it will auto install it's command line called \"ctr\". This tool is solely made for debugging containerd and it's not user-friendly, but it gives you a great way to understand how containers work under the hood.</p> <p>You can refer this link ctr to understand more ctr CLI options.</p> Bash<pre><code>ctr --help\nctr images pull docker.io/library/redis:alpine\nctr run docker.io/library/redis:alpine redis\n</code></pre> <p>Therefore, there is an alternative tool called nerdctl that provides a stable, human-friendly user experience, and it is mainly used for general purposes. It's very similar to Docker command.</p> <ul> <li>Same UI/UX (Docker-like CLI) as docker</li> <li>Supports Docker Compose <code>nerdctl compose up</code></li> <li>Supports rootless mode, without slirp overhead</li> <li>Supports lazy-pulling</li> <li>Supports encrypted container images</li> <li>Supports P2P image distribution</li> <li>Supports container image image signing and verifying</li> <li>Apply Kubernetes namespaces for containers, etc</li> </ul> Bash<pre><code>nerdctl\nnerdctl run -it ubuntu\nnerdctl run -p 8080:8080 -d webapplication\nnerdctl compose -f ./docker-compose.yaml up\n</code></pre> <p>There is another tool called crictl, which provides a CLI for CRI-compatible container runtimes like \"containerd\".</p> <ul> <li>This tool need to install separately</li> <li>It can interact with any CRI runtime, so the developers can debug the runtime without having to install Kubernetes components</li> <li>It's used to inspect and debug container runtimes and it is ideal not to create containers</li> </ul> Bash<pre><code>crictl help\ncrictl pull busybox\ncrictl images\ncrictl ps -a\ncrictl pods\n</code></pre> <p>If you try to use crictl to create containers on the Kubernetes environment, kubelet will be going to delete them as kubelet ensures a specific number of pods are available on a single node, that means containers created outside Kubelet's environment are unknown to it. So this tool is mainly used for debugging purposes (exec into container).</p>"},{"location":"docs/kubernetes/cron-job/","title":"Cron Job","text":""},{"location":"docs/kubernetes/cron-job/#concept-and-usage-of-cronjob","title":"Concept and Usage of CronJob","text":"<p>CronJob will create jobs on a time-based schedule. It is useful when you want to run a job at a specific time or at regular intervals.</p> <p>You can use this tool crontab.guru to generate the cron expression.</p> Field Allowed Values minute 0-59 hour 0-23 day-of-month 1-31 month 1-12 day-of-week 0-6 (0 is Sunday) cronjob.yaml<pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: backup-cron-job\nspec:\n  # minute, hour, day-of-month, month, day-of-week\n  schedule: \"0 0 1 * *\" # At 00:00 on day-of-month 1\n  jobTemplate:\n    spec: # Job spec\n      completions: 3 # run 3 pods\n      parallelism: 2 # parallel run 2 pods at a time\n      template: # pod template\n        spec:\n          containers:\n            - name: backup\n              image: busybox\n              command: [\"echo\", \"Backup operation\"]\n          restartPolicy: Never\n</code></pre> Bash<pre><code>kubectl get cronjob\n</code></pre>"},{"location":"docs/kubernetes/daemon-set/","title":"DaemonSet","text":""},{"location":"docs/kubernetes/daemon-set/#concept-and-usage-of-daemonset","title":"Concept and Usage of DaemonSet","text":"<p>DaemonSet will ensure that all or some Nodes will run a copy of a Pod. Whenever a new Node is added to the cluster, a Pod is automatically added to that Node. When a Node is removed from the cluster, the Pod will be automatically removed as well. Remember, DaemonSet uses the default scheduler and node affinity rules to schedule the pods on the nodes.</p> <p>Use cases;</p> <ul> <li>Running a logging agent (collector) on all nodes</li> <li>Running a monitoring agent on all nodes</li> <li>Setting up network services like firewall, load-balancer, network proxies, VPN on all nodes</li> </ul> <p>For example, we know that <code>kube-proxy</code> is required on every node to provide network services. Therefore, <code>kube-proxy</code> component is actually deployed as a DaemonSet in Kubernetes.</p> <pre><code>flowchart TB\n    subgraph Cluster\n        direction TB\n        N1[Node 1] --&gt; P1(Pod)\n        N2[Node 2] --&gt; P2(Pod)\n    end\n\n    P1:::daemonset\n    P2:::daemonset\n    subgraph DaemonSet\n      direction LR\n      PDS(Pod Definition)\n      PDS --&gt; P1\n      PDS --&gt; P2\n    end</code></pre> daemonsets.yaml<pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: sample-daemonset\nspec:\n  selector:\n    matchLabels:\n      app: sample-agent\n  template:\n    metadata:\n      labels:\n        app: sample-agent\n    spec:\n      containers:\n        - name: sample-monitoring-agent\n          image: sample-monitoring-agent-image\n</code></pre> Bash<pre><code>kubectl get daemonsets\n</code></pre>"},{"location":"docs/kubernetes/deployment/","title":"Deployment","text":""},{"location":"docs/kubernetes/deployment/#what-is-deployment","title":"What is Deployment?","text":"<pre><code>---\ntitle: Deployment\n---\nflowchart\n  subgraph \"Deployment\"\n    subgraph \"ReplicaSet\"\n      pod1[Pod]\n      pod2[Pod]\n      pod3[Pod]\n      pod4[Pod]\n      pod5[Pod]\n    end\n  end</code></pre> <p>Deployment is a higher-level abstraction that manages a set of replicas of the application. It will ensure the desired number of replicas are running and available, just like ReplicaSet, but it offers more features like rolling updates and rollbacks.</p> <pre><code>flowchart LR\n  deployment --manages--&gt; replicaset\n  replicaset[ReplicaSet] --manages--&gt; container\n  container</code></pre> <p>Here is an example, currently you have 3 instances of an application, you want to upgrade them, but you don't want to upgrade all of them at once and you want to update it one by one. This is where Deployment comes into play and this upgrade process is called rolling update.</p> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: backend\n  template:\n    metadata:\n      name: app-pod\n      labels:\n        app: backend\n    spec:\n      containers:\n        - name: backend-container\n          image: backend:1.0\n</code></pre> Bash<pre><code>kubectl get deployments\nkubectl create deployment &lt;deployment-name&gt; --image=&lt;image-name&gt; --replicas=&lt;number-of-replicas&gt;\nkubectl create deployment backend --image=web:1.0 --replicas=3\n</code></pre>"},{"location":"docs/kubernetes/deployment/#rollout-and-rollback-revertundo","title":"Rollout and Rollback (Revert/Undo)","text":"<p>Deployment rollout and rollback allows the developer to update the application without downtime and can revert back to previous versions as needed.</p> <p>Rollout</p> <ul> <li>A process of updating the application to a new version.</li> <li>It will gradually replace the old pods with new ones, ensuring that the application remains available during the update.</li> </ul> <p>Rollback</p> <ul> <li>A process of reverting to a previous version of the application if something goes wrong during the rollout.</li> </ul> <p>Here is an process, when you first create a deployment, it will trigger a rollout. When you update the deployment, it will trigger another rollout. If something goes wrong, you can rollback to the previous version.</p> Bash<pre><code># get the status of rollout\nkubectl rollout status deployment/&lt;deployment-name&gt;\n\n# get the revisions and history of rollout\nkubectl rollout history deployment/&lt;deployment-name&gt;\n\n# Check the status of each revision individually\nkubectl rollout history deployment/&lt;deployment-name&gt; --revision=&lt;revision-number&gt;\nkubectl rollout history deployment/app-deploy --revision=1\n\n# rolling update (rollout)\nkubectl set image deployment/&lt;deployment-name&gt; &lt;container-name&gt;=&lt;new-image-name&gt;\nkubectl set image deployment/app-deployment backend-container=backend:2.0\n\n# in some cases, when you perform rollout history, you will see \"change-cause\" field is empty\n# so, you can use --record flag to save the command used to create/update the deployment against the revision number\nkubectl set image deployment/&lt;deployment-name&gt; &lt;container-name&gt;=&lt;new-image-name&gt; --record\n\n\n# rollback (undo a change)\nkubectl rollout undo deployment/&lt;deployment-name&gt;\n\n# rollback to specific revision\nkubectl rollout undo deployment/&lt;deployment-name&gt; --to-revision=&lt;revision-number&gt;\nkubectl rollout undo deployment/app-deploy --to-revision=1\n</code></pre>"},{"location":"docs/kubernetes/deployment/#deployment-strategy","title":"Deployment Strategy","text":""},{"location":"docs/kubernetes/deployment/#recreate-strategy","title":"Recreate Strategy","text":"<pre><code>flowchart LR\n  A[Start Update] --&gt; B[Terminate Old Pods]\n  B --&gt; C[Create New Pods]\n  C --&gt; D[Update Complete]</code></pre> <p>For recreate strategy, all existing pods will be terminated and new pods will be created with the new configuration. This will cause downtime during the update.</p> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-deployment\nspec:\n  replicas: 3\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: backend\n  template:\n    metadata:\n      name: app-pod\n      labels:\n        app: backend\n    spec:\n      containers:\n        - name: backend-container\n          image: backend:1.0\n</code></pre>"},{"location":"docs/kubernetes/deployment/#rolling-update","title":"Rolling update","text":"<pre><code>flowchart LR\n  A[Start Rolling Update] --&gt; B[Create New Pod]\n  B --&gt; C[Wait for New Pod to be Ready]\n  C --&gt; D[Terminate Old Pod]\n  D --&gt; E{More Pods to Update?}\n  E --&gt;|Yes| B\n  E --&gt;|No| F[Update Complete]</code></pre> <p>Rolling update is the default deployment strategy. It will gradually replaces old pods with new ones. Therefore, it will ensure the application available during the update process, minimizing downtime.</p> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-deployment\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1 # ensures that at most 1 pod can be unavailable during the update\n      maxSurge: 1 # allows the deployment to create 1 additional pod above the desired number of replicas during the update.\n  selector:\n    matchLabels:\n      app: backend\n  template:\n    metadata:\n      name: app-pod\n      labels:\n        app: backend\n    spec:\n      containers:\n        - name: backend-container\n          image: backend:1.0\n</code></pre> Bash<pre><code># rolling update (rollout)\nkubectl set image deployment/&lt;deployment-name&gt; &lt;container-name&gt;=&lt;new-image-name&gt;\nkubectl set image deployment/app-deployment backend-container=backend:2.0\n</code></pre>"},{"location":"docs/kubernetes/encrypting-secret-data-at-rest/","title":"Encrypting Secret Data at REST","text":""},{"location":"docs/kubernetes/encrypting-secret-data-at-rest/#steps-of-encrypting-secret-data-at-rest","title":"Steps of Encrypting Secret Data at REST","text":"<p>Reference</p>"},{"location":"docs/kubernetes/encrypting-secret-data-at-rest/#step-1-check-encryption-at-rest-is-already-enabled-on-the-cluster","title":"Step 1: Check encryption at REST is already enabled on the cluster","text":"Bash<pre><code># method 1\nps -aux | grep kube-apiserver | grep \"encryption-provider-config\"\n# if it returns empty, that means is no encryption at REST enabled\n\n# method 2\ncat /etc/kubernetes/manifests/kube-apiserver.yaml\n# check got option --encryption-provider-config\n</code></pre>"},{"location":"docs/kubernetes/encrypting-secret-data-at-rest/#step-2-understand-the-encryption-configuration","title":"Step 2: Understand the encryption configuration","text":"sample-encryption-config.yaml<pre><code>---\n#\n# CAUTION: this is an example configuration.\n#          Do not use this for your own cluster!\n#\napiVersion: apiserver.config.k8s.io/v1\nkind: EncryptionConfiguration\nresources:\n  - resources: # You can pick which resources you want to encrypt\n      - secrets\n      - configmaps\n      - pandas.awesome.bears.example # a custom resource API\n\n    # Here got a lot of providers, this order matters, as the first provider which is identity will encrypt the data first, then it could use any of these (aesgcm, aescbc, secretbox, etc) to decrypt.\n    # So since the identity it empty, meaning no encryption at all, so if you want to encrypt your data, then you should choose and place either one (aesgcm, aescbc, secretbox) at the first place.\n    providers:\n      # This configuration does not provide data confidentiality. The first\n      # configured provider is specifying the \"identity\" mechanism, which\n      # stores resources as plain text.\n      #\n      - identity: {} # plain text, in other words NO encryption\n      - aesgcm:\n          keys:\n            - name: key1\n              secret: c2VjcmV0IGlzIHNlY3VyZQ== # this secret will be used for the encryption by the encryption algorithm\n            - name: key2\n              secret: dGhpcyBpcyBwYXNzd29yZA==\n      - aescbc:\n          keys:\n            - name: key1\n              secret: c2VjcmV0IGlzIHNlY3VyZQ==\n            - name: key2\n              secret: dGhpcyBpcyBwYXNzd29yZA==\n      - secretbox:\n          keys:\n            - name: key1\n              secret: YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=\n  - resources:\n      - events\n    providers:\n      - identity: {} # do not encrypt Events even though *.* is specified below\n  - resources:\n      - '*.apps' # wildcard match requires Kubernetes 1.27 or later\n    providers:\n      - aescbc:\n          keys:\n          - name: key2\n            secret: c2VjcmV0IGlzIHNlY3VyZSwgb3IgaXMgaXQ/Cg==\n  - resources:\n      - '*.*' # wildcard match requires Kubernetes 1.27 or later\n    providers:\n      - aescbc:\n          keys:\n          - name: key3\n            secret: c2VjcmV0IGlzIHNlY3VyZSwgSSB0aGluaw==\n</code></pre> <ul> <li>Here got a lot of providers, this order matters, as the first provider which is identity will encrypt the data first, then it could use any of these (aesgcm, aescbc, secretbox, etc) to decrypt.</li> <li>So since the identity it empty, meaning no encryption at all, so if you want to encrypt your data, then you should choose and place either one (aesgcm, aescbc, secretbox) at the first place.</li> </ul>"},{"location":"docs/kubernetes/encrypting-secret-data-at-rest/#step-3-generate-a-new-encryption-key","title":"Step 3: Generate a new encryption key","text":"Bash<pre><code># Linux\nhead -c 32 /dev/urandom | base64\n\n# Windows\n# Do not run this in a session where you have set a random number\n# generator seed.\n[Convert]::ToBase64String((1..32|%{[byte](Get-Random -Max 256)}))\n</code></pre>"},{"location":"docs/kubernetes/encrypting-secret-data-at-rest/#step-4-create-a-new-encryption-configuration-file","title":"Step 4: Create a new encryption configuration file","text":"<p>Create a new file called <code>enc.yaml</code> and replace the secret value with the newly generated encryption key.</p> enc.yaml<pre><code>---\napiVersion: apiserver.config.k8s.io/v1\nkind: EncryptionConfiguration\nresources:\n  - resources:\n      - secrets\n      - configmaps\n      - pandas.awesome.bears.example\n    providers:\n      - aescbc:\n          keys:\n            - name: key1\n              # See the following text for more details about the secret value\n              secret: &lt;BASE 64 ENCODED SECRET&gt; # Replace your newly generated encryption key here\n      - identity: {} # this fallback allows reading unencrypted secrets;\n                     # for example, during initial migration\n</code></pre>"},{"location":"docs/kubernetes/encrypting-secret-data-at-rest/#step-5-apply-this-config-file-to-kube-apiserver-static-pod","title":"Step 5: Apply this config file to <code>kube-apiserver</code> static pod","text":"Bash<pre><code># save the new encryption configuration file to /etc/kubernetes/enc\nmkdir /etc/kubernetes/enc\nmv enc.yaml /etc/kubernetes/enc\n\n# edit the kube-apiserver manifest file\nvim /etc/kubernetes/manifests/kube-apiserver.yaml\n</code></pre> /etc/kubernetes/manifests/kube-apiserver.yaml<pre><code>---\n#\n# This is a fragment of a manifest for a static Pod.\n# Check whether this is correct for your cluster and for your API server.\n#\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.20.30.40:443\n  creationTimestamp: null\n  labels:\n    app.kubernetes.io/component: kube-apiserver\n    tier: control-plane\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    ...\n    - --encryption-provider-config=/etc/kubernetes/enc/enc.yaml  # add this line\n    volumeMounts:\n    ...\n    - name: enc                           # add this line\n      mountPath: /etc/kubernetes/enc      # add this line\n      readOnly: true                      # add this line\n    ...\n  volumes:\n  ...\n  - name: enc                             # add this line\n    hostPath:                             # add this line\n      path: /etc/kubernetes/enc           # add this line\n      type: DirectoryOrCreate             # add this line\n  ... \n</code></pre>"},{"location":"docs/kubernetes/encrypting-secret-data-at-rest/#step-6-restart-the-kube-apiserver","title":"Step 6: Restart the <code>kube-apiserver</code>","text":"<p>If not mistaken, it will auto restart once you save the file, if not, you can restart it manually.</p> Bash<pre><code># method 1\ndocker ps | grep kube-apiserver\ndokcer restart &lt;kube-apiserver-container-id&gt;\n\n# method 2\nsudo systemctl restart kube-apiserver\n\n# method 3\nkubectl delete pod -n kube-system -l component=kube-apiserver\n</code></pre>"},{"location":"docs/kubernetes/encrypting-secret-data-at-rest/#step-7-test-the-encryption","title":"Step 7: Test the encryption","text":""},{"location":"docs/kubernetes/encrypting-secret-data-at-rest/#71-check-whether-the-etcd-cmd-exists","title":"7.1 Check whether the etcd cmd exists","text":"Bash<pre><code>etcdctl\n# if not exists then install it\nsudo apt-get install etcd-client\n</code></pre>"},{"location":"docs/kubernetes/encrypting-secret-data-at-rest/#72-create-a-new-secret","title":"7.2 Create a new secret","text":"Bash<pre><code>kubectl create secret generic secret1 -n default --from-literal=mykey=mydata\n</code></pre>"},{"location":"docs/kubernetes/encrypting-secret-data-at-rest/#73-read-the-secret-out-of-etcd","title":"7.3 Read the secret out of etcd","text":"Bash<pre><code>ETCDCTL_API=3 etcdctl \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt   \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key  \\\n  get /registry/secrets/default/&lt;your-secret&gt; | hexdump -C\n\n# example\nETCDCTL_API=3 etcdctl \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt   \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key  \\\n  get /registry/secrets/default/secret1  | hexdump -C\n</code></pre> <p>The output is similar to this (abbreviated): Text Only<pre><code>00000000  2f 72 65 67 69 73 74 72  79 2f 73 65 63 72 65 74  |/registry/secret|\n00000010  73 2f 64 65 66 61 75 6c  74 2f 73 65 63 72 65 74  |s/default/secret|\n00000020  31 0a 6b 38 73 3a 65 6e  63 3a 61 65 73 63 62 63  |1.k8s:enc:aescbc|\n00000030  3a 76 31 3a 6b 65 79 31  3a c7 6c e7 d3 09 bc 06  |:v1:key1:.l.....|\n00000040  25 51 91 e4 e0 6c e5 b1  4d 7a 8b 3d b9 c2 7c 6e  |%Q...l..Mz.=..|n|\n00000050  b4 79 df 05 28 ae 0d 8e  5f 35 13 2c c0 18 99 3e  |.y..(..._5.,...&gt;|\n[...]\n00000110  23 3a 0d fc 28 ca 48 2d  6b 2d 46 cc 72 0b 70 4c  |#:..(.H-k-F.r.pL|\n00000120  a5 fc 35 43 12 4e 60 ef  bf 6f fe cf df 0b ad 1f  |..5C.N`..o......|\n00000130  82 c4 88 53 02 da 3e 66  ff 0a                    |...S..&gt;f..|\n0000013a\n</code></pre></p> <p>Now your data is encrypted at REST. Congratulations!</p>"},{"location":"docs/kubernetes/encrypting-secret-data-at-rest/#step-8-optional-ensure-all-previous-secrets-are-encrypted","title":"Step 8 (Optional): Ensure all previous secrets are encrypted","text":"Bash<pre><code># Run this as an administrator that can read and write all Secrets\nkubectl get secrets --all-namespaces -o json | kubectl replace -f -\n</code></pre>"},{"location":"docs/kubernetes/env-and-secrets-setup/","title":"Environments (ConfigMap) and Secrets","text":"<p>We have 3 ways to set environment variables and secrets;</p> <ul> <li>key-value pair format</li> <li>ConfigMap</li> <li>Secrets</li> </ul>"},{"location":"docs/kubernetes/env-and-secrets-setup/#key-value-pair-format","title":"key-value pair format","text":"pod.yml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-ubuntu\nspec:\n  container:\n    - name: my-ubuntu\n      image: ubuntu\n      env:\n        - name: URL\n          value: https://karchunt.com\n        - name: APP_NAME\n          value: karchunt-ubuntu\n</code></pre>"},{"location":"docs/kubernetes/env-and-secrets-setup/#configmap","title":"ConfigMap","text":"<p>The concept of ConfigMap is actually the same as key-value pair format, just it stores the configuration data in a Kubernetes API object.</p> <p>There are two ways of creating ConfigMap;</p> <ul> <li>Imperative</li> <li>Declarative</li> </ul> Bash<pre><code># get configmap\nkubectl get configmap\n</code></pre>"},{"location":"docs/kubernetes/env-and-secrets-setup/#ways-of-creating-configmap","title":"Ways of creating ConfigMap","text":""},{"location":"docs/kubernetes/env-and-secrets-setup/#imperative","title":"Imperative","text":"Bash<pre><code># create configmap from key-valuye pairs\nkubectl create configmap &lt;config-name&gt; --from-literal=&lt;key&gt;=&lt;value&gt;\nkubectl create configmap api-config --from-literal=port=8080\n\n# create configmap from file path\nkubectl create configmap &lt;config-name&gt; --from-file=&lt;path-to-file&gt;\nkubectl create configmap api-config --from-file=.env\n</code></pre> <p>When you create configmap from file path, make sure each line adhere to the format like this <code>&lt;name&gt;=&lt;value&gt;</code></p> .env<pre><code>port=8080\nenv=prod\n</code></pre>"},{"location":"docs/kubernetes/env-and-secrets-setup/#declarative","title":"Declarative","text":"api-config.yaml<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: api-config\ndata:\n  port: 8080\n  env: prod\n</code></pre>"},{"location":"docs/kubernetes/env-and-secrets-setup/#apply-entire-configmap-to-pod","title":"Apply entire ConfigMap to Pod","text":"<p>Make sure the configMapRef name is equal to ConfigMap file metadata name.</p> api-party.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-party\nspec:\n  containers:\n    - name: api-party\n      image: api-party:0.1.0\n      ports:\n        - containerPort: 8080\n      envFrom:\n        - configMapRef:\n            name: api-config # ConfigMap file metadata name\n</code></pre>"},{"location":"docs/kubernetes/env-and-secrets-setup/#apply-only-1-value-from-configmap-to-pod","title":"Apply only 1 value from ConfigMap to Pod","text":"<p>We can also apply only 1 value from ConfigMap to Pod.</p> api-party.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-party\nspec:\n  containers:\n    - name: api-party\n      image: api-party:0.1.0\n      ports:\n        - containerPort: 8080\n      env:\n        - name: port\n          valueFrom:\n            configMapKeyRef:\n              name: api-config # ConfigMap file metadata name\n              key: port # ConfigMap file under data section\n</code></pre>"},{"location":"docs/kubernetes/env-and-secrets-setup/#apply-configmap-to-volume","title":"Apply ConfigMap to volume","text":"<p>Note</p> <p>A container using a ConfigMap as a subPath volume will not receive ConfigMap updates.</p> <p>In some cases, you might want to create files with the contents during runtime, but you can actually do that using ConfigMap.</p> special-cm.yaml<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: model-config\ndata:\n  config.py: |\n    config = {\n      'url': 'http://localhost:8080'\n    }\n  special_file.txt: specialData\n</code></pre> api-party.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-party\nspec:\n  containers:\n    - name: api-party\n      image: api-party:0.1.0\n      ports:\n        - containerPort: 8080\n      volumeMounts:\n        - name: config-volume\n          mountPath: /app/src\n          # subPath: src\n  volumes:\n    - name: config-volume\n      configMap:\n        name: model-config\n</code></pre> <p>In this case, two files called will be created in <code>/app/src</code>, so these files will present at this path:</p> <ul> <li><code>/app/src/config.py</code></li> <li><code>/app/src/special_file.txt</code></li> </ul>"},{"location":"docs/kubernetes/env-and-secrets-setup/#secrets","title":"Secrets","text":"<p>Info</p> <p>Remember, the way of mapping secrets and how it works are very similar to ConfigMap. Refer to link to learn understand more about Kubernetes secret. Addtional info:</p> <ul> <li>Secrets only encoded and not encrypted in etcd. Therefore, we should enable encryption at kube-apiserver (REST)</li> <li>Consider external secrets providers like HashiCorp Vault and AWS Secrets Manager</li> <li>Don't check in secret objects with data to SCM like GitHub</li> <li>Remember to setup RBAC (least-privileged access) to secrets as anyone can access those Kubernetes objects within the same namespace</li> </ul> <p>Secrets are used to store confidential information like a password, key, or token.</p> <p>Additional information on how Kubernetes manage secrets:</p> <ul> <li>A secret will be only sent to a node if a pod requires it</li> <li>When we mount secrets into pods, the kubelet will store a copy of the secret into a <code>tmpfs</code> (aka a disk in RAM) so that the confidential data is not written to disk storage.</li> <li>The kubelet will delete its local copy of the secret when the pod depends on the secret is deleted.</li> </ul> Bash<pre><code>kubectl get secrets\n</code></pre> Built-in Type Usage Opaque arbitrary user-defined data kubernetes.io/service-account-token ServiceAccount token kubernetes.io/dockerconfigjson serialized ~/.docker/config.json file kubernetes.io/basic-auth credentials for basic authentication kubernetes.io/ssh-auth credentials for SSH authentication kubernetes.io/tls data for a TLS client or server bootstrap.kubernetes.io/token bootstrap token data"},{"location":"docs/kubernetes/env-and-secrets-setup/#ways-of-creating-secrets","title":"Ways of creating Secrets","text":""},{"location":"docs/kubernetes/env-and-secrets-setup/#imperative_1","title":"Imperative","text":"<p>When you use imperative to create secret, you no need to encode your data, as the command will automatically help you to encode.</p> Bash<pre><code># create secret generic (opaque) from key-value pairs\nkubectl create secret generic &lt;secret-name&gt; --from-literal=&lt;key&gt;=&lt;value&gt;\nkubectl create secret generic api-secret --from-literal=token=yourtoken\n\n# create secret generic from file path\nkubectl create secret generic &lt;secret-name&gt; --from-file=&lt;path-to-file&gt;\nkubectl create secret generic api-secret --from-file=.env\n\n# create container registry secret\nkubectl create secret docker-registry secret-harbor \\\n  --docker-email=karchun@gmail.com \\\n  --docker-username=karchun \\\n  --docker-password=mypassword \\\n  --docker-server=harbor.registry.com\n\n# retrieve and decode the data for registry secret\nkubectl get secret secret-harbor -o jsonpath='{.data.*}' | base64 -d\n\n# Create TLS secret\n# the certificate for --cert must be .PEM encoded\nkubectl create secret tls &lt;secret-name&gt; --cert=&lt;path-to-cert-file&gt; --key=&lt;path-to-key-file&gt;\nkubectl create secret tls tls-domain --cert=\"path.crt\" --key=\"key.key\"\n</code></pre> <p>When you create secret from file path, make sure each line adhere to the format like this <code>&lt;name&gt;=&lt;value&gt;</code></p> .env<pre><code>token=yourtoken\n</code></pre>"},{"location":"docs/kubernetes/env-and-secrets-setup/#declarative_1","title":"Declarative","text":"<p>Make sure your secrets store with base64 format</p> Bash<pre><code># encode\necho -n '&lt;data&gt;' | base64\necho -n 'password' | base64\n\n# decode\necho -n 'bXlzcWw=' | base64 --decode\n</code></pre>"},{"location":"docs/kubernetes/env-and-secrets-setup/#opaque","title":"Opaque","text":"api-secret.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: api-secret\ndata:\n  token: bXlzcWw=\n</code></pre>"},{"location":"docs/kubernetes/env-and-secrets-setup/#docker-config","title":"Docker config","text":"docker-config.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: secret-dockercfg\ntype: kubernetes.io/dockercfg\ndata:\n  .dockerconfigjson: |\n    eqJhdXRocyd6eyJodHRwczoaL2V4YW1wbGUvdjEvcjP7ImF1dGgiOiJvcGVuc2VzYW1lIn19fQo=    \n</code></pre>"},{"location":"docs/kubernetes/env-and-secrets-setup/#basic-authentication","title":"Basic authentication","text":"<p>The <code>data</code> or <code>stringData</code> (clear text content) field of the Secret must contain one of the following two keys:</p> <ul> <li><code>username</code></li> <li><code>password</code></li> </ul> basic-authentication.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: secret-basic-auth\ntype: kubernetes.io/basic-auth\ndata:\n  username: bXlzcWw=\n  password: bXlzcWw=\n</code></pre>"},{"location":"docs/kubernetes/env-and-secrets-setup/#ssh-authentication","title":"SSH authentication","text":"<p>Mandatory field: <code>ssh-privatekey</code></p> ssh-authentication.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: secret-ssh-auth\ntype: kubernetes.io/ssh-auth\ndata:\n  ssh-privatekey: |\n    abcoAI812hcnaik\n</code></pre>"},{"location":"docs/kubernetes/env-and-secrets-setup/#tls","title":"TLS","text":"<p>Mandatory fields: <code>tls.crt</code> and <code>tls.key</code></p> tls.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: secret-tls\ntype: kubernetes.io/tls\ndata:\n  tls.crt: |\n    abcoAI812hcnaik\n    UUVCQlFVQU1Jada\n    bkkxWHgKRHanca1\n  tls.key: |\n    abcoAI812hcnaikabcoAI812hcnaikabcoAI812hcnaik\n</code></pre>"},{"location":"docs/kubernetes/env-and-secrets-setup/#apply-entire-secret-to-pod","title":"Apply entire Secret to Pod","text":"<p>Make sure the secretRef name is equal to Secret file metadata name.</p> api-party.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-party\nspec:\n  containers:\n    - name: api-party\n      image: api-party:0.1.0\n      ports:\n        - containerPort: 8080\n      envFrom:\n        - secretRef:\n            name: api-secret # Secret file metadata name\n</code></pre>"},{"location":"docs/kubernetes/env-and-secrets-setup/#apply-only-1-value-from-secret-to-pod","title":"Apply only 1 value from Secret to Pod","text":"<p>We can also apply only 1 value from Secret to Pod.</p> api-party.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-party\nspec:\n  containers:\n    - name: api-party\n      image: api-party:0.1.0\n      ports:\n        - containerPort: 8080\n      env:\n        - name: token\n          valueFrom:\n            secretKeyRef:\n              name: api-secret # Secret file metadata name\n              key: token # Secret file under data section\n</code></pre>"},{"location":"docs/kubernetes/env-and-secrets-setup/#apply-secret-to-volume","title":"Apply Secret to volume","text":"<p>Info</p> <p>A container using a Secret as a subPath volume will not receive Secret updates.</p> <p>In some cases, you might want to create files with the contents during runtime, but you can actually do that using Secret.</p> special-cm.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: sample-secret\ndata:\n  .secret-file: bXlzcWw=\n</code></pre> api-party.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-party\nspec:\n  containers:\n    - name: api-party\n      image: api-party:0.1.0\n      ports:\n        - containerPort: 8080\n      volumeMounts:\n        - name: secret-volume\n          mountPath: /app/src\n          readOnly: true\n          # subPath: src\n  volumes:\n    - name: secret-volume\n      configMap:\n        name: sample-secret\n</code></pre> <p>In this case, a file called <code>.secret-file</code> will be created in <code>/app/src</code>, so this file will present at this path <code>/app/src/.secret-file</code>.</p>"},{"location":"docs/kubernetes/exam-tips/","title":"Exam Tips","text":""},{"location":"docs/kubernetes/exam-tips/#imperative-commands-with-dry-run","title":"Imperative Commands with dry-run","text":"<p>Use <code>--dry-run=client</code> to generate the YAML file for the resource without actually creating it.</p> Bash<pre><code># pod\nkubectl run nginx --image=nginx --dry-run=client -o yaml\nkubectl run nginx --image=nginx --port=80 --expose # expose the pod as a service\n\n# deployment\nkubectl create deployment --image=nginx nginx --dry-run=client -o yaml &gt; sample-deployment.yaml\n\n# expose pod as service\nkubectl expose pod nginx --port=80 --name=nginx-service --type=NodePort --dry-run=client -o yaml\n\n# service\n# the type of service is clusterip\nkubectl create service clusterip nginx --tcp=80:80 --dry-run=client -o yaml\nkubectl create service nodeport nginx --tcp=80:80 --node-port=31080 --dry-run=client -o yaml\n</code></pre>"},{"location":"docs/kubernetes/exam-tips/#format-output-with-kubectl","title":"Format output with <code>kubectl</code>","text":"<p>For better readability, you can use this command <code>kubectl [command] [TYPE] [NAME] -o &lt;output_format&gt;</code> to format the output.</p> <p>Commonly used output formats are:</p> <ul> <li><code>-o json</code> - Output a JSON formatted API object</li> <li><code>-o name</code> - Only display resource name</li> <li><code>-o wide</code> - Output in the plain-text format with any additional information</li> <li><code>-o yaml</code> - Output a YAML formatted API object</li> </ul> Bash<pre><code>kubectl get svc -o wide\n</code></pre>"},{"location":"docs/kubernetes/exam-tips/#edit-a-deployment","title":"Edit a deployment","text":"<p><code>kubectl edit deployment &lt;deployment-name&gt;</code> will automatically delete the existing deployment and create a new one with the changes.</p>"},{"location":"docs/kubernetes/exam-tips/#edit-a-pod","title":"Edit a pod","text":"<p>When you edit an existing pod, there are some specifications you cannot edit. For example,</p> <ul> <li><code>spec.containers[*].image</code></li> <li><code>spec.initContainers[*].image</code></li> <li><code>spec.activeDeadlineSeconds</code></li> <li><code>spec.tolerations</code></li> </ul> <p>If you use this command <code>kubectl edit pod &lt;pod-name&gt;</code>, you cannot save the file after editing it, instead it will save the file with the changes in a temporary location.</p> <p>So, in this case, we have multiple options to edit the pod:</p> <ol> <li> <p>Delete and apply the temporary file     Bash<pre><code>kubectl delete pod &lt;pod-name&gt;\nkubectl apply -f &lt;temporary-file&gt;\n</code></pre></p> </li> <li> <p>Save existing pod, edit it, delete existing, and apply the edited file     Bash<pre><code>kubectl get pod &lt;pod-name&gt; -o yaml &gt; new-pod.yaml\nvi new-pod.yaml\nkubectl delete pod &lt;pod-name&gt;\nkubectl apply -f new-pod.yaml\n</code></pre></p> </li> </ol>"},{"location":"docs/kubernetes/exam-tips/#exec-without-enter-the-pod-shell","title":"Exec without enter the pod shell","text":"Bash<pre><code>kubectl exec &lt;pod-name&gt; -- &lt;command&gt;\nkubectl exec &lt;pod-name&gt; -- ls /var/log\n</code></pre>"},{"location":"docs/kubernetes/exam-tips/#enter-the-pod-shell","title":"Enter the pod shell","text":"Bash<pre><code>kubectl exec -it &lt;pod-name&gt; -- &lt;command&gt;\nkubectl exec -it &lt;pod-name&gt; -- /bin/bash\nkubectl exec --stdin --tty &lt;pod-name&gt; -- /bin/bash\n</code></pre>"},{"location":"docs/kubernetes/exam-tips/#delete-the-pod-faster","title":"Delete the pod faster","text":"Bash<pre><code>kubectl delete pod/&lt;pod-name&gt; --force --grace-period=0\n</code></pre>"},{"location":"docs/kubernetes/ingress/","title":"Ingress","text":""},{"location":"docs/kubernetes/ingress/#what-is-ingress","title":"What is Ingress?","text":"<pre><code>graph TD\n  A[External Client] --&gt;|HTTP/HTTPS Request| B[Ingress]\n  B --&gt;|Route 1: Host/Path A| C[Service A]\n  B --&gt;|Route 2: Host/Path B| D[Service B]\n  B --&gt;|Route 3: Host/Path C| E[Service C]\n  C --&gt;|Forwards Traffic| F[Pod A1, Pod A2]\n  D --&gt;|Forwards Traffic| G[Pod B1, Pod B2]\n  E --&gt;|Forwards Traffic| H[Pod C1, Pod C2]</code></pre> <pre><code>---\ntitle: Flow\n---\ngraph LR\n  A[External Client] --&gt; B[Ingress] --&gt; C[Load Balancer] --&gt; D[Service] --&gt; E[Pods]</code></pre> <p>Ingress is a Kubernetes resource that provides HTTP and HTTPS routing to services within a cluster. It exposes HTTP/S routes from outside the cluster to services within the cluster by defining rules for routing traffic based on the request's host and path.</p> <p>Why we need Ingress?</p> <p><pre><code>graph LR\n  A[\"karchunt.com\"] --&gt; B[\"172.17.0.5:30100\"]</code></pre> If you ask the users to enter the IP address every time in the browser, the user won't be happy. So, we need a way to route the traffic to the correct service/IP address. Ingress will help us with that.</p> <p>In this case, <code>karchunt.com</code> is our DNS name, now we can point this DNS name to the IP address, therefore the users can access the service using the DNS name instead of the IP address.</p> <p>Important</p> <p>Remember, you will need to expose the Ingress controller to the external world, otherwise, the Ingress resource will not work. You can do this by using a Cloud Native Load Balancer or NodePort.</p> <p>Ingress controller is not deployed by default in Kubernetes cluster.</p> <p>Ingress contains two main components:</p> <ul> <li>Ingress controller</li> <li>Ingress resource</li> </ul>"},{"location":"docs/kubernetes/ingress/#ingress-controller","title":"Ingress controller","text":"<p>More Information</p> <ul> <li>https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/</li> <li>https://docs.nginx.com/nginx-ingress-controller/installation</li> <li>https://github.com/nginx/kubernetes-ingress</li> <li>https://hub.docker.com/r/nginx/nginx-ingress</li> </ul> <p>We will be using NGINX Ingress Controller in this example. I highly recomend you to use helm to install the Ingress controller.</p> <p>Link:</p> <ul> <li>manifests installation </li> <li>helm installation</li> </ul>"},{"location":"docs/kubernetes/ingress/#ingress-resource","title":"Ingress resource","text":"<p>More Information</p> <ul> <li>https://kubernetes.io/docs/concepts/services-networking/ingress/</li> <li>https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-ingress-em-</li> <li>https://github.com/kubernetes/ingress-nginx/blob/main/docs/examples/rewrite/README.md</li> </ul> <p>Ingress resource is a Kubernetes resource that defines the rules for routing traffic to the services.</p>"},{"location":"docs/kubernetes/ingress/#ingress-backed-by-a-single-service","title":"Ingress backed by a single service","text":"<p>The following example will route all incoming traffic to the <code>example-service</code> on  port <code>80</code>. See <code>Path based routing</code> for more details about <code>defaultBackend</code>.</p> ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\nspec:\n  defaultBackend: # This is the default backend for all incoming traffic\n    service:\n      name: example-service\n      port:\n        number: 80\n</code></pre> Bash<pre><code>kubectl get ingress example-ingress\n</code></pre>"},{"location":"docs/kubernetes/ingress/#path-based-routing","title":"Path based routing","text":"<p>If the user tries to access the URL that does not match the below rules, the request will be routed to the <code>defaultBackend</code> service.</p> ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\nspec:\n  defaultBackend: # This is the default backend for all incoming traffic\n    service:\n      name: example-service\n      port:\n        number: 80\n  rules:\n    - http:\n        paths:\n          - path: /api\n            pathType: Prefix\n            backend:\n              service:\n                name: api-service\n                port:\n                  number: 8000\n          - path: /docs\n            pathType: Prefix\n            backend:\n              service:  \n                name: docs-service\n                port:\n                  number: 8080\n</code></pre> Bash<pre><code>kubectl create ingress example-ingress --default-backend=example-service:80 \\\n  --rule=\"/api=api-service:8000\" \\\n  --rule=\"/docs=docs-service:8080\"\n</code></pre>"},{"location":"docs/kubernetes/ingress/#name-based-routing","title":"Name based routing","text":"ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\nspec:\n  rules:\n    - host: karchunt.com\n      http:\n        paths:\n          - path: \"/api\"\n            pathType: Prefix\n            backend:\n              service:\n                name: api-service\n                port:\n                  number: 8000\n    - host: karchuntan.com\n      http:\n        paths:\n          - path: \"/docs\"\n            pathType: Prefix\n            backend:\n              service:\n                name: docs-service\n                port:\n                  number: 8080\n</code></pre>"},{"location":"docs/kubernetes/ingress/#rewrite-target","title":"Rewrite target","text":"<p>Info</p> <p>https://kubernetes.github.io/ingress-nginx/examples/rewrite/</p> <p>In some cases, you may want to rewrite the URL path before forwarding the request to the backend service. For example, if the pay application expects requests at <code>/</code> rather than <code>/pay</code>, you can use the <code>nginx.ingress.kubernetes.io/rewrite-target</code> annotation to achieve this.</p> IngressPython ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n    - host: pay-karchunt.com\n      http:\n        paths:\n          - path: /pay\n            pathType: Exact\n            backend:\n              service:\n                name: pay-service\n                port:\n                  number: 8000\n</code></pre> pay.py<pre><code>from fastapi import FastAPI\n\napp = FastAPI(\n  title='Pay',\n  description='This is pay API',\n  version='0.1.0'\n)\n\n@app.post(\"/\")\ndef pay():\n  return {'success': True}\n</code></pre> <ul> <li>Meaning that when you access <code>http://pay-karchunt.com/pay</code>, it will be rewritten to <code>http://pay-karchunt.com/</code> before being forwarded to the <code>pay-service</code>. This means that the <code>pay-service</code> will receive the request at the root path <code>/</code> instead of <code>/pay</code>, as <code>pay-service</code> only expose the root path <code>/</code>.</li> </ul> <p>Format: replace(path, rewrite-target) In our case: replace(\"/pay\". \"/\")</p> ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/use-regex: \"true\"\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\n  name: rewrite\n  namespace: default\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: rewrite.bar.com\n      http:\n        paths:\n          - path: /something(/|$)(.*)\n            pathType: ImplementationSpecific\n            backend:\n              service:\n                name: http-svc\n                port: \n                  number: 80\n</code></pre> <p>replace(\"/something(/|$)(.*)\", \"/$2\")</p> <p>Any characters captured by (.*) will be assigned to the placeholder $2, which is then used as a parameter in the <code>rewrite-target</code> annotation.</p> <ul> <li><code>rewrite.bar.com/something</code> rewrites to <code>rewrite.bar.com/</code></li> <li><code>rewrite.bar.com/something/</code> rewrites to <code>rewrite.bar.com/</code></li> <li><code>rewrite.bar.com/something/new</code> rewrites to <code>rewrite.bar.com/new</code></li> </ul>"},{"location":"docs/kubernetes/ingress/#tls","title":"TLS","text":""},{"location":"docs/kubernetes/ingress/#step-1-create-a-tls-secret","title":"Step 1: Create a TLS secret","text":"<p>Refer this link for more information about how to create a TLS secret.</p>"},{"location":"docs/kubernetes/ingress/#step-2-reference-the-tls-secret","title":"Step 2: Reference the TLS secret","text":"ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\nspec:\n  tls:\n    - hosts:\n        - karchunt.com\n      secretName: karchunt-tls # This is the name of the TLS secret\n  rules:\n    - host: karchunt.com\n      http:\n        paths:\n          - path: \"/api\"\n            pathType: Prefix\n            backend:\n              service:\n                name: api-service\n                port:\n                  number: 8000\n</code></pre>"},{"location":"docs/kubernetes/job/","title":"Job","text":""},{"location":"docs/kubernetes/job/#concept-and-usage-of-job","title":"Concept and Usage of Job","text":"<p>Job will create one or more pods. The job will be considered as completed when all the pods inside the job are completed. When you delete a job, the pods created by the job will be deleted too.</p> <p>Use case;</p> <ul> <li>Running a batch Job</li> <li>Backup operation</li> </ul> job.yaml<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: backup-job\nspec:\n  completions: 3 # run 3 pods\n  parallelism: 2 # parallel run 2 pods at a time\n  template: # pod template\n    spec:\n      containers:\n        - name: backup\n          image: busybox\n          command: [\"echo\", \"Backup operation\"]\n      restartPolicy: Never\n</code></pre> <p>If some of the pods fail, the job will create new pods to replace the failed ones until the specified number of completions (3) is achieved. The job will keep retrying until all Pods have successfully completed or the job is deleted. However, since the <code>restartPolicy</code> is set to <code>Never</code>, the pod will not be restarted if it fails, instead, new Pod will be created to replace them.</p> Bash<pre><code>kubectl get jobs\nkubectl delete job &lt;job-name&gt;\n</code></pre>"},{"location":"docs/kubernetes/kubeconfig/","title":"KubeConfig","text":""},{"location":"docs/kubernetes/kubeconfig/#concept-usage-of-kubeconfig","title":"Concept &amp; Usage of kubeconfig","text":"<p>The kubeconfig file is a configuration file used to configure access to Kubernetes clusters. It is used to specify the cluster, user, and context information required to connect to a Kubernetes cluster.</p> Bash<pre><code>kubectl get pods \\\n  --server=https://&lt;cluster-ip&gt;:&lt;port&gt; \\\n  --client-certificate=&lt;path-to-client-certificate&gt; \\\n  --client-key=&lt;path-to-client-key&gt; \\\n  --certificate-authority=&lt;path-to-ca-certificate&gt; \\\n</code></pre> <p>Let me give you a scenario. When you do not have a kubeconfig file, you can still access the Kubernetes cluster using the <code>kubectl</code> command, but you have to provide the authentication details every time you run a command. This is not a good practice and it is not feasible. So, the kubeconfig file will help in this case by specifying the configuration details into a file. For example, <code>config</code>.</p>"},{"location":"docs/kubernetes/kubeconfig/#method-1-using-the-kubeconfig-flag-not-recommended","title":"Method 1: Using the <code>--kubeconfig</code> flag (not recommended)","text":"config<pre><code>--server https://&lt;cluster-ip&gt;:&lt;port&gt;\n--client-certificate &lt;path-to-client-certificate&gt;\n--client-key &lt;path-to-client-key&gt;\n--certificate-authority &lt;path-to-ca-certificate&gt;\n</code></pre> <p>Here is an example of a config file looks like, but with this method, you have to specify the config file every time you run a command. So it is not recommended, instead you should use the kubeconfig file.</p> Bash<pre><code>kubectl get pods --kubeconfig=config\n</code></pre>"},{"location":"docs/kubernetes/kubeconfig/#method-2-using-the-kubeconfig-file-recommended","title":"Method 2: Using the kubeconfig file (recommended)","text":"<pre><code>flowchart LR\n  subgraph cluster[Clusters]\n    development\n    production\n    stage\n  end\n\n  subgraph context[Contexts]\n    c1[\"dev@development\"]\n    c2[\"prod@production\"]\n    c3[\"staging@stage\"]\n  end\n\n  subgraph users[Users]\n    dev[dev user]\n    prod[prod user]\n    staging[staging user]\n  end\n\n  development --- c1 --- dev\n  production --- c2 --- prod\n  stage --- c3 --- staging</code></pre> <p>By default, <code>kubectl</code> looks for a file named <code>config</code> in the <code>$HOME/.kube</code> directory. The <code>config</code> file has the following structure:</p> <ul> <li>clusters - information about the Kubernetes cluster, like the server URL, certificate authority, etc.</li> <li>contexts - it defines which user account can access which cluster, so you no need to specify the user certificate or server configuration in <code>kubectl</code> command. For example, you create a context called <code>dev@development</code> that will use the <code>dev</code> user to access the <code>development</code> cluster.</li> <li>users - user information like the client certificate, client key, etc where the user is the one who is accessing the cluster.</li> </ul> <p>Here are the commands, where you can use to view the kubeconfig configuration:</p> Bash<pre><code>kubectl config -h\nkubectl config view # view the current kubeconfig configuration\nkubectl config view --kubeconfig=config # view the kubeconfig configuration based on the file path\n</code></pre> <p>Now, you can also specify the kubeconfig file as default kubeconfig file by setting the <code>KUBECONFIG</code> environment variable.</p> Bash<pre><code>export KUBECONFIG=&lt;file-path&gt;\nexport KUBECONFIG=/new-kube-config\n\n# or you can add it to the .bashrc file\necho \"export KUBECONFIG=/root/my-kube-config\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> config<pre><code>apiVersion: v1\nkind: Config\nclusters:\n  - name: production\n    cluster:\n      server: https://&lt;cluster-ip&gt;:&lt;port&gt;\n      certificate-authority: &lt;path-to-ca-certificate&gt; # normally is /etc/kubernetes/pki/ca.crt\ncontexts:\n  - name: prod@production\n    context:\n      cluster: production\n      user: prod\n      namespace: prod1 # optional field: the default namespace to use\nusers:\n  - name: prod\n    user:\n      client-certificate: &lt;path-to-client-certificiate&gt; # normally is /etc/kubernetes/pki/apiserver-kubelet-client.crt\n      client-key: &lt;path-to-client-key&gt; # normally is /etc/kubernetes/pki/apiserver-kubelet-client.key\n</code></pre> <p>Besides, you can also specify the default context to use by setting the <code>current-context</code> field in the kubeconfig file.</p> config<pre><code>apiVersion: v1\nkind: Config\ncurrent-context: prod@production # context name\n...\n</code></pre> <p>Of course, you can change the context as well.</p> Bash<pre><code>kubectl config use-context &lt;context-name&gt;\nkubectl config use-context prod@production\n</code></pre> <p>Now there is another option for cluster certificate side if you don't want to use <code>certificate-authority</code> field in the kubeconfig file.</p> config<pre><code>apiVersion: v1\nkind: Config\nclusters:\n  - name: production\n    cluster:\n      server: https://production:6443\n      certificate-authority-data: &lt;base64-encoded-ca-certificate&gt;\n</code></pre> <ul> <li>The <code>certificate-authority-data</code> field is the base64-encoded certificate authority data.<ul> <li>Convert the certificate authority file content to base64-encoded format.<ul> <li><code>cat /etc/kubernetes/pki/ca.crt | base64 -w 0</code></li> </ul> </li> <li>You can decode the base64-encoded data using the following command.<ul> <li><code>echo \"&lt;base64-encoded-ca-certificate&gt;\" | base64 -d</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/kubernetes/kubernetes-threat-model/","title":"Kubernetes Threat Model","text":"<p>Threat modeling is a structured approach that helps identify and mitigate potential security threats in a system. In the context of Kubernetes, it involves analyzing the architecture, components, and interactions within a Kubernetes cluster to identify vulnerabilities and risks.</p>"},{"location":"docs/kubernetes/kubernetes-threat-model/#kubernetes-trust-boundaries-and-data-flows","title":"Kubernetes Trust Boundaries and Data Flows","text":""},{"location":"docs/kubernetes/kubernetes-threat-model/#trust-boundaries","title":"Trust Boundaries","text":"<pre><code>graph LR\n  user --&gt; ingress\n\n  subgraph cluster\n    ingress --&gt; p1[Frontend Pods]\n    p1 --&gt; p2[Backend Pods]\n    p2 --&gt; db[Database Pods]\n  end</code></pre> <p>Let's take a look at this diagram. The user interacts with the cluster through an ingress controller. The ingress controller routes the traffic to the frontend pod, which in turn communicates with the backend pod. The backend pod interacts with the database pod.</p> <p>Now, let's say that the hacker compromises the frontend pod. The hacker can then use the compromised frontend pod to access the backend pod and potentially the database pod as well, because the diagram here does not isolate the pods and no enforce specific security measures between them.</p> <pre><code>graph LR\n  user --&gt; ingress\n\n  subgraph cluster\n    subgraph g1\n      ingress --&gt; frontend\n      frontend ---&gt; backend --&gt; db\n      subgraph frontend[Frontend Pods]\n        subgraph p1\n          pp1[pod]\n        end\n\n        subgraph p2\n          pp2[pod]\n        end\n      end\n\n      subgraph backend[Backend Pods]\n        subgraph b1\n          bp1[pod]\n        end\n\n        subgraph b2\n          bp2[pod]\n        end\n      end\n\n      subgraph db[Database Pods]\n        subgraph d1\n          dp1[pod]\n        end\n\n        subgraph d2\n          dp2[pod]\n        end\n      end\n    end\n  end</code></pre> <p>Important</p> <p>These isolated areas are called trust boundaries. We have multiple trust boundaries.</p> <ul> <li>Cluster boundary</li> <li>Node boundary</li> <li>Pod boundary</li> <li>Namespace boundary</li> <li>Container boundary</li> </ul> <p>In this diagram, we have isolated them different groups of pods and enforce specific security measures between them (Pod). So, if the hacker compromise one part of the system, they will not be able to compromise the other parts of the system.</p>"},{"location":"docs/kubernetes/kubernetes-threat-model/#cluster-boundary","title":"Cluster Boundary","text":"<pre><code>graph LR\n  subgraph cluster1[Cluster Production]\n    ingress --&gt; p1[Frontend Pods]\n    p1 --&gt; p2[Backend Pods]\n    p2 --&gt; db[Database Pods]\n  end\n\n  subgraph cluster2[Cluster Development]\n    ingress2[ingress] --&gt; b1[Frontend Pods]\n    b1 --&gt; b2[Backend Pods]\n    b2 --&gt; dbb[Database Pods]\n  end\n\n  subgraph cluster3[Cluster Testing]\n    ingress3[ingress] --&gt; c1[Frontend Pods]\n    c1 --&gt; c2[Backend Pods]\n    c2 --&gt; dbc[Database Pods]\n  end\n\n  cluster1 -.-&gt;|isolated from| cluster2\n  cluster2 -.-&gt;|isolated from| cluster3</code></pre> <p>important</p> <p>This setup will provide top-level security for your cluster. For example, the network traffic between the clusters will be isolated.</p> <p>The cluster boundary is the boundary between the clusters in a Kubernetes setup. We know that the Kubernetes setup involves control-plane components and worker nodes, these items will be part of the cluster boundary. So, the best is to separate or isolate the clusters for different environments. For example, we can have a cluster for production, a cluster for development, and a cluster for testing. With this setup, we can ensure that the issues in one cluster will not affect the other clusters.</p>"},{"location":"docs/kubernetes/kubernetes-threat-model/#node-boundary","title":"Node Boundary","text":"<pre><code>graph\n  subgraph cluster[Cluster]\n    direction LR\n    subgraph node1[Node 1]\n      pod1[Frontend pod]\n      pod2[Frontend pod]\n    end\n    subgraph node2[Node 2]\n      pod3[Backend pod]\n      pod4[Backend pod]\n    end\n    subgraph node3[Node 3]\n      pod5[Database pod]\n      pod6[Database pod]\n    end\n\n    node1 -.-&gt;|isolated from| node2\n    node2 -.-&gt;|isolated from| node3\n  end</code></pre> <p>The node boundary is the boundary between the nodes in a Kubernetes cluster. It ensures that the pods running on one node are isolated from the pods running on another node. For example, if a frontend pod running on node 1 is compromised, the hacker will not be able to access the backend pods running on node 2 or the database pods running on node 3. This is because the network traffic between the nodes is isolated.</p>"},{"location":"docs/kubernetes/kubernetes-threat-model/#pod-boundary","title":"Pod Boundary","text":"<pre><code>graph TD\n  subgraph cluster[Cluster]\n    pod1[Frontend Pod 1]\n    pod2[Frontend Pod 2]\n    pod3[Backend Pod]\n    pod5[Database Pod]\n\n    pod1 -.-&gt;|cannot interact| pod2\n    pod1 -.-&gt;|cannot interact| pod3\n    pod1 -.-&gt;|cannot interact| pod5\n  end</code></pre> <p>The pod boundary is the boundary between the pods in a Kubernetes cluster. Each pod typically runs one or more containers. Pods are isolated from each other using its own network policies, runtime environment, and security context, as these can be defined at the pod level.</p> <p>By default, pods within the same namespace or cluster can communicate freely unless network policies are explicitly applied to restrict communication.</p> <p>This ensures that pods are not able to access each other unless explicitly allowed by these policies.</p>"},{"location":"docs/kubernetes/kubernetes-threat-model/#namespace-boundary","title":"Namespace Boundary","text":"<pre><code>graph LR\n  subgraph namespace1[Frontend NS]\n    pod1[Frontend Pod 1]\n    pod2[Frontend Pod 2]\n  end\n\n  subgraph namespace2[Backend NS]\n    pod3[Backend Pod]\n  end\n\n  subgraph namespace3[Database NS]\n    pod5[Database Pod]\n  end\n\n  namespace1 -.-&gt;|isolated from| namespace2\n  namespace2 -.-&gt;|isolated from| namespace3</code></pre> <p>The namespace boundary is the boundary between the namespaces in a Kubernetes cluster. Namespaces are used to organize and isolate resources within a cluster. Each namespace can have its own set of resources, such as pods, services, and config maps.</p>"},{"location":"docs/kubernetes/kubernetes-threat-model/#container-boundary","title":"Container Boundary","text":"<pre><code>graph TD\n  subgraph pod1[Frontend Pod]\n    container1[Frontend Container 1]\n    container2[Frontend Container 2]\n  end\n\n  container1 -.-&gt;|isolated from| container2</code></pre> <p>The container boundary is the boundary between containers within a pod. Containers in the same pod share the same network namespace and can communicate with each other via <code>localhost</code>. It provides application-level isolation. For example, if a container in a pod is compromised, the damange is limited to that container.</p>"},{"location":"docs/kubernetes/kubernetes-threat-model/#data-flows","title":"Data Flows","text":"<p>Data flow is the movement of data between different components in a Kubernetes cluster. Understanding data flows is crucial for identifying potential vulnerabilities and attack vectors.</p> <p>To protect the data flows, we can take the following measures:</p> <ul> <li>implement network policies to restrict communication between pods and namespaces</li> <li>use encryption (TLS) for data in transit and at REST</li> <li>secure the ingress and egress traffic</li> <li>implement service mesh, etc</li> </ul>"},{"location":"docs/kubernetes/kubernetes-threat-model/#persistence","title":"Persistence","text":"<p>Persistence is the ability of a system to maintain its state and data across different attacks by hackers. In Kubernetes, hacker can achieve persistence by exploiting misconfigurations, reading secrets or using container vulnerabilities.</p> <p>There are several ways to mitigate Persistence risks:</p> <ul> <li>Use RBAC to restrict access to service accounts or sensitive resources like secrets.</li> <li>Use PodSecurityPolicies to <ul> <li>prevent containers from running as root or with privileged access</li> <li>enforce security contexts for pods and containers</li> <li>enforce read-only root filesystem</li> </ul> </li> <li>Regular updates and patching of container images.</li> <li>Implement monitoring and auditing of Kubernetes cluster activity to detect any suspicious activities and audit Kubernetes events regularly.<ul> <li>Track changes to RBAC policies, secrets, creation of new pods, network policies, and other security-related configurations.</li> <li>Alert on any suspicious activities or changes to the cluster.</li> </ul> </li> </ul>"},{"location":"docs/kubernetes/kubernetes-threat-model/#denial-of-service","title":"Denial of Service","text":"<p>More Information</p> <p>The hacker can spam the API server with requests, which can lead the service to become unavailable.</p> <p>Denial of Service (DoS) attack is an attack that aims to make a service unavailable to its intended users. In Kubernetes, DoS attacks can be achieved by overloading the cluster with requests or by exploiting vulnerabilities in the cluster components.</p> <p>We know that by default pod has a service account, which is used to authenticate the pod to the API server. If the hacker compromises the pod, they can use the service account token to authenticate to the API server and start many containers or send many requests in the cluster. This can lead to resource (CPU &amp; memory) exhaustion.</p> <p>There are several ways to mitigate Denial of Service risks:</p> <ul> <li>Setup resource quota and limit range for each namespace, which will limit the number of resources that can be used by the pods in that namespace.</li> <li>Setup resource limits and requests for each pod</li> <li>Setup network policies and proper firewall configurations to protect control plane endpoints and to control traffic flow within the cluster. For example, only allow trusted IP addresses to access the API server.</li> <li>Secure service accounts using RBAC to restrict the permissions of the service accounts. For example, can limit the service account to only be able to0 read the resources in the cluster, but not to create or delete them.</li> <li>Implement monitoring and alerting for unusual traffic patterns or spikes in resource.</li> </ul>"},{"location":"docs/kubernetes/kubernetes-threat-model/#malicious-code-execution","title":"Malicious Code Execution","text":"<p>Malicious code execution is an attack that aims to execute arbitrary (malicious) code on a system. In Kubernetes, this can be achieved by exploiting vulnerabilities in the cluster components or by compromising the pods running in the cluster.</p> <p>Here are the common steps to achieve this:</p> <ol> <li>The hacker compromises a pod by exploiting a vulnerability in the application running in the pod.</li> <li>The hacker executes malicious code in the compromised pod to install a malicious software to steal sensitive data or perform other malicious activities.</li> <li>The hacker can poison image repository by uploading a malicious image to the repository. So if other clusters pull the image, they will be compromised as well.</li> </ol> <p>Important</p> <p>I want to emphasize more on this topic.</p> sample-pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: example-hostpid-pod\n  namespace: default\nspec:\n  hostPID: true\n  containers:\n  - name: example-container\n    image: busybox\n    command: [\"sh\", \"-c\", \"ps aux\"]\n</code></pre> <p>Hacker will primarily focus on finding the pod that enables <code>hostPID</code> and <code>SYS_PTRACE</code> capabilities. This is because these two features can be used to compromise the host.</p> <p>If the container is running with <code>hostPID</code>, the hacker can access the host process, meaning that the hacker can see all the processes running on the host,  interact with the host processes, and potentially compromise the host.</p> <p>If the container enables <code>SYS_PTRACE</code> capability, the hacker can inspect the host processes and attach to them. This can be used to debug the processes or inject malicious code into the host processes.</p> <p>There are several ways to mitigate Malicious Code Execution risks:</p> <ul> <li>Secure container image repositories by ensuring that only trusted images are uploaded to the repository.<ul> <li>Use image signing to ensure that the images are authentic and untampered.</li> </ul> </li> <li>Secure image pull secrets and restrict access to only necessary pods using RBAC.</li> <li>Use PodSecurityPolicies to prevent containers from running with <code>hostPID</code> and <code>SYS_PTRACE</code> capabilities.</li> <li>Use signed images to verify the integrity of the images before deploying them to the cluster.</li> <li>Regular updates and patching of container images.</li> </ul>"},{"location":"docs/kubernetes/kubernetes-threat-model/#access-to-sensitive-data","title":"Access to sensitive data","text":"<p>Hacker can access to sensitive data can through multiple ways:</p> <ul> <li>One is misconfigured RBAC policies. The hacker can exploit (use) the misconfigured RBAC policies to gain access to sensitive data in the cluster.</li> <li>Second is accessing and viewing sensitive data in logs.</li> <li>Third is eavesdropping (spy) on the network traffic between the pods. The attacker can intercept the network traffic and read the sensitive data being transmitted between the pods if the network traffic is not encrypted.</li> </ul> <p>There are several ways to mitigate Access to sensitive data risks:</p> <ul> <li>Ensure that RBAC policies are properly configured and restrict access to sensitive data.</li> <li>Ensure the logs do not log sensitive data. Restrict access to logs and use centralized logging solutions to provide fine-grained access control and monitor access to logs.</li> <li>Use encryption (TLS) for data in transit to protect sensitive data being transmitted between the pods.</li> </ul>"},{"location":"docs/kubernetes/kubernetes-threat-model/#privilege-escalation","title":"Privilege Escalation","text":"<p>Privilege escalation is an attack that aims to gain elevated privileges in a system. Here is the scenario, we might want to run a command using the root user privileges.</p> <p>How can we do that if the root user login is disabled?</p> <p>Well, actually the preferred way is to use <code>sudo</code>. So, we will need to configure the user to have <code>sudo</code> privileges. This can be done by adding the user to the <code>sudo</code> group or by modifying the <code>/etc/sudoers</code> file.</p> Bash<pre><code>visudo\n</code></pre> <p>/etc/sudoers<pre><code># User privilege specification\nroot    ALL=(ALL:ALL) ALL\n\n# Members of the admin group may gain root privileges\n%admin ALL=(ALL) ALL\n\n# Allow members of group sudo to execute any command\n%sudo   ALL=(ALL:ALL) ALL\n\n# See sudoers(5) for more information on \"@include\" directives:\n\nanson   ALL=(ALL:ALL) ALL\n\n# Allow KC to reboot the system\n@includedir /etc/sudoers.d\nkc      localhost=NOPASSWD: /sbin/reboot\n</code></pre> Only users listed in the <code>/etc/sudoers</code> file can run commands with <code>sudo</code>. In this case, <code>anson</code> has complete privileges while <code>kc</code> can only run the <code>/sbin/reboot</code> command without a password.</p> Field Description kc, %sudo (group) user or group ALL (default), localhost Hosts (ALL) (default), (All:All) Users:Groups ALL (default), /sbin/reboot Commands"},{"location":"docs/kubernetes/labels-selectors-and-annotations/","title":"Labels, selectors, and annotations","text":""},{"location":"docs/kubernetes/labels-selectors-and-annotations/#concept-and-usage","title":"Concept and usage","text":"<ul> <li>Labels - Properties attached to each item, basically it will match to pod.  </li> <li>Selectors - Filter the items based on labels </li> <li>Annotations - Record other information for your records like buildVersion, etc.</li> </ul> <p>Normally, we will use labels and selectors to group and select objects.</p> Bash<pre><code>kubectl get deployments --selector env=prod\nkubectl get deployments --selector env=prod,tier=backend\n</code></pre> YAML<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-sample-deployment\n  labels:\n    env: prod\n    tier: backend\n  annotations: # additional information\n    buildVersion: 1.0.0\n    author: KarChun\nspec:\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      labels:\n        app: sample-app\n    spec:\n      containers:\n        - name: my-ubuntu-app\n          image: ubuntu\n</code></pre>"},{"location":"docs/kubernetes/liveness-probes/","title":"Liveness Probes","text":"<p>Info</p> <p>All command is same as Readiness Probes, but the only difference is the <code>livenessProbe</code> field.</p> <p>Reference</p> <p>The concept of Liveness probes basically is to determine whether a container is alive or not by performing some tests. If the liveness probes are successful, then we know that container is going to run (healthy), else, it (kubelet) will restart the container (unhealthy).</p> <pre><code>flowchart LR\n  A[Container] --&gt; B{Check container is alive?}\n  B --&gt;|Yes| C[Healthy]\n  B --&gt;|No| D[Unhealthy]\n  D --&gt; E[Restart the container]\n  E --&gt; B</code></pre> <p>Let me give you one scenario, suppose you have a container running an API server, and the API server is not responding to the requests, but the container still stay alive. In this case, the liveness probe will help you to restart the container, so that a new container can be created and the API server can start responding to the requests.</p> <p>HTTP liveness-probes.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: web-app\n      image: webapp\n      livenessProbe:\n        httpGet:\n          path: /api/v1/health\n          port: 8000\n        initialDelaySeconds: 5 # Number of seconds after the container has started before startup, that means it should wait for 5 seconds before performing the 1st probe\n        periodSeconds: 3 # how often to perform the probe\n        failureThreshold: 5 # default 3, if the application is not ready after 3 attempts, then the probe will stop\n</code></pre></p> <p>TCP liveness-probes.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: web-app\n      image: webapp\n      livenessProbe:\n        tcpSocket:\n          port: 8080\n        initialDelaySeconds: 5 # Number of seconds after the container has started before startup, that means it should wait for 5 seconds before performing the 1st probe\n        periodSeconds: 3 # how often to perform the probe\n        failureThreshold: 5 # default 3, if the application is not ready after 3 attempts, then the probe will stop\n</code></pre></p> <p>kubelet will try to open a socket to the container on the specified port <code>8080</code>. If it can establish the connection, then the state is healthy.</p> <p>exec liveness-probes.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: web-app\n      image: webapp\n      livenessProbe:\n        exec:\n          command:\n            - cat\n            - /app/healthy\n        initialDelaySeconds: 5 # Number of seconds after the container has started before startup, that means it should wait for 5 seconds before performing the 1st probe\n        periodSeconds: 3 # how often to perform the probe\n        failureThreshold: 5 # default 3, if the application is not ready after 3 attempts, then the probe will stop\n</code></pre></p> <p>It will return 0 status code if the command succeed.</p>"},{"location":"docs/kubernetes/logging/","title":"Logging","text":"<p>Logging is a way to monitor and troubleshoot the applications running in a Kubernetes cluster. It is important to have a logging strategy in place to monitor the health of the applications and to troubleshoot issues when they arise.</p> sample-pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: web-app\n      image: webapp\n    - name: db:\n      image: webapp-db\n</code></pre> Bash<pre><code># View the live logs of a pod\nkubectl logs -f &lt;pod-name&gt;\nkubectl logs -f sample-pod\n\n# View the container logs if there are multiple containers in a pod\nkubectl logs -f &lt;pod-name&gt; -c &lt;container-name&gt;\nkubectl logs -f sample-pod -c webapp-db\n</code></pre>"},{"location":"docs/kubernetes/manual-scheduling/","title":"Manual Scheduling","text":"<p>There are different ways to manual schedule a pod on a node. Now for some cases, you don't want to rely on the Kubernetes built-in scheduler or you don't have a scheduler in your cluster, or you want to schedule the pod by yourself. How would you do that?</p> <p>So we know that the scheduling works like this</p> <ol> <li>What to schedule? -&gt; Pod</li> <li>Which node to schedule?</li> <li>Schedule/Bind Pod to Node</li> </ol> <p>Here is an example of Pod definition YAML file.</p> sample.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: ubuntu\n      image: ubuntu\n  nodeName: node01\n</code></pre> <p>Every pod has a field called <code>nodeName</code> and by default is not set. Normally you won't set this field, as this field will auto being added by Kubernetes. Here are the steps of how Kubernetes will auto add this field.</p> <ol> <li>First, the Scheduler will go through all the pods and check which pods do not have this property set: <code>nodeName</code>.</li> <li>The Scheduler will run the scheduling algorithm to determine and identify the right node for the pod.</li> <li>Lastly, the scheduler will schedule the pod on the node by setting this property set: <code>nodeName</code> to the name of the node by creating a binding object.</li> </ol> <p>If you don't have a scheduler in your cluster, then when you deployed the pods, those pods status will be in Pending state, as nobody monitor and schedule nodes for those deployed pods.</p> <p>In this case, you can manually assign those pods to the specified node by yourself without a scheduler (set the <code>nodeName</code> field to the node name). Note that, you can only specify the <code>nodeName</code> at the Pod creation time.</p> sample.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: ubuntu\n      image: ubuntu\n  nodeName: node01\n</code></pre> <p>Here is the another scenario, assume your pod already created and you would like to assign the deployed pod to a node. Now, we know that once you deployed the Pod, Kubernetes don't allow you to modify the <code>nodeName</code> property field. How would you do that?</p> <p>Well, in this case, we can create a Binding object and send a POST request to the Pod's binding API, with this method, we can assign a node to the existing deployed pod.</p> binding.yaml<pre><code>apiVersion: v1\nkind: Binding\nmetadata:\n  name: sample-binding\ntarget:\n  apiVersion: v1\n  kind: Node\n  name: node03\n</code></pre> <p>Remember, you must convert the above YAML file to JSON format for POST request. Reference</p> Bash<pre><code>curl -H \"Content-Type: application/json\" \\\n  -X POST \\\n  -d '{\"apiVersion\": \"v1\",\"kind\": \"Binding\",\"metadata\": {\"name\": \"sample-binding\"},\"target\": {\"apiVersion\": \"v1\",\"kind\": \"Node\",\"name\": \"node03\"}}' \\ \n  http://{server-url}/api/v1/namespaces/{namespace}/pods/{pod-name}/binding\n</code></pre>"},{"location":"docs/kubernetes/monitoring/","title":"Monitoring","text":""},{"location":"docs/kubernetes/monitoring/#what-kind-of-things-need-to-monitor","title":"What kind of things need to monitor?","text":"<p>Cluster components</p> <ul> <li>API Server (kube-apiserver): Monitor request rates, errors rates, and latency</li> <li>Scheduler (kube-scheduler): Monitor scheduling latency and errors</li> <li>Controller Manager (kube-controller-manager): Monitor the status of various controllers</li> </ul> <p>Nodes</p> <ul> <li>The number of nodes in the cluster</li> <li>Performance metrics:<ul> <li>CPU and Memory Usage: Monitor resource usage to ensure nodes are not over or underutilized</li> <li>Disk Usage: Monitor disk space and I/O operations</li> <li>Network: Monitor network bandwidth and errors</li> </ul> </li> </ul> <p>Pods</p> <ul> <li>The number of pods in the cluster</li> <li>Performance metrics:<ul> <li>Resource Usage: Monitor CPU, memory, and disk usage of pods and containers.</li> <li>Pod Status: Monitor the status of pods (Running, Pending, Failed, etc.).</li> <li>Container Logs: Monitor logs for errors and warnings.</li> <li>Health Checks: Monitor the results of liveness and readiness probes.</li> <li>Application Metrics: Monitor custom application metrics (e.g., request rates, error rates).</li> </ul> </li> </ul>"},{"location":"docs/kubernetes/monitoring/#metrics-server","title":"Metrics Server","text":"<p>Info</p> <p>You can use other metrics server like Prometheus, Datadog, Dynatrace, etc.</p> <p>Reference</p> <p>Metrics Server is mainly to collect the resource metrics such as CPU and memory usage from the kubelet and aggregate them (resource usage data), and deliver them to the Kubernetes API server (kube-apiserver) for autoscaling purposes such as Horizontal Pod Autoscaler (HPA).</p> <ul> <li>HPA will automatically adjust the number of pods in a deployment based on the metrics collected by the Metrics Server.</li> </ul> <p>How metrics collected from the pods?</p> <ul> <li>The kubelet contains a component called cAdvisor or Container Advisor that is mainly to collect the resource usage data of the pods and will expose them through the kubelet API. So, the Metrics Server will collect the resource usage data in the form of metrics from the kubelet API.</li> </ul> Bash<pre><code># View performance metrics of the cluster/pod\n# This command requires Metrics Server to be correctly configured and working on the server.\nkubectl top node\nkubectl top pod\n</code></pre>"},{"location":"docs/kubernetes/multiple-schedulers/","title":"Multiple Schedulers","text":""},{"location":"docs/kubernetes/multiple-schedulers/#concept-and-usage-of-multiple-schedulers","title":"Concept and Usage of Multiple Schedulers","text":"<p>Reference</p> <p>Multiple schedulers are used to schedule pods on different nodes based on the requirements. By default, Kubernetes uses the default scheduler to schedule pods on nodes using an algorithm to distribute the pods across the nodes evenly. But in some cases, you may want to setup your own scheduling algorithm or any custom conditions to place pods on nodes.</p> <p>Therefore, Kubernetes allows you to write and deploy your own scheduler as default scheduler or as an additional scheduler. In this case, you can use your own custom scheduler to schedule some specific pods (applications) on specific nodes based on your requirements, while other pods can still be scheduled by the default scheduler.</p> <p>This is the default scheduler, the scheduler name is <code>default-scheduler</code> and it must be unique in the cluster. You can find your default scheduler configuration on the master node at <code>/etc/kubernetes/manifests/kube-scheduler.yaml</code>.</p> scheduler-config.yaml<pre><code>apiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n  - schedulerName: default-scheduler # unique name\n</code></pre>"},{"location":"docs/kubernetes/multiple-schedulers/#steps-to-setup-and-use-multiple-schedulers","title":"Steps to setup and use Multiple Schedulers","text":""},{"location":"docs/kubernetes/multiple-schedulers/#step-1-create-a-new-scheduler-configuration-file","title":"Step 1: Create a new scheduler configuration file","text":"/etc/kubernetes.my-new-scheduler.yaml<pre><code>apiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n  - schedulerName: my-new-scheduler\nleaderElection:\n  leaderElect: true\n  resourceNamespace: kube-system\n  resourceName: lock-object-my-scheduler\n</code></pre> <ul> <li><code>leaderElect</code> - ensures that only one instance of the scheduler is active at a time. Assume that there are multiple instances of the scheduler running on different master nodes as a high-availability setup, only one instance will be elected (selected) as a leader to schedule the pods.</li> <li><code>resourceName</code> - assume you have multiple masters, you will need to specify the name of the resource object used for leader election. This is used to ensure that only one instance of the scheduler is active at a time and to avoid conflicts between multiple instances of the scheduler.</li> </ul>"},{"location":"docs/kubernetes/multiple-schedulers/#step-2-deploy-additional-scheduler","title":"Step 2: Deploy Additional Scheduler","text":"<p>You may choose to deploy the scheduler as a Pod or Deployment.</p>"},{"location":"docs/kubernetes/multiple-schedulers/#deploy-as-a-poda","title":"Deploy as a Poda","text":"my-new-scheduler.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-new-scheduler\n  namespace: kube-system\nspec:   \n  containers:\n    - name: my-new-scheduler\n      image: k8s.gcr.io/kube-scheduler:v1.22.0\n      command:\n        - kube-scheduler\n        # make sure these files are exists on the host\n        - --kubeconfig=/etc/kubernetes/scheduler.conf  # this file has the authentication information to access the API server\n        - --config=/etc/kubernetes.my-new-scheduler.yaml\n      volumeMounts:\n        - name: kubeconfig\n          mountPath: /etc/kubernetes\n          readOnly: true\n  volumes:\n    - name: kubeconfig\n      hostPath:\n        path: /etc/kubernetes\n</code></pre> Bash<pre><code>kubectl apply -f my-new-scheduler.yaml\n</code></pre>"},{"location":"docs/kubernetes/multiple-schedulers/#deploy-as-a-deployment","title":"Deploy as a Deployment","text":"<p>Package the Scheduler Bash<pre><code>git clone https://github.com/kubernetes/kubernetes.git\ncd kubernetes\nmake\n</code></pre></p> <p>Create a new container image containing the kube-scheduler binary Dockerfile<pre><code>FROM busybox\nADD ./_output/local/bin/linux/amd64/kube-scheduler /usr/local/bin/kube-scheduler\n</code></pre></p> <p>Build the dockerfile Bash<pre><code>docker build -t gcr.io/my-gcp-project/my-kube-scheduler:1.0 .     # The image name and the repository\ngcloud docker -- push gcr.io/my-gcp-project/my-kube-scheduler:1.0 # used in here is just an example\n</code></pre></p> <p>Define a Kubernetes Deployment for the scheduler my-new-scheduler.yaml<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-scheduler\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: my-scheduler-as-kube-scheduler\nsubjects:\n- kind: ServiceAccount\n  name: my-scheduler\n  namespace: kube-system\nroleRef:\n  kind: ClusterRole\n  name: system:kube-scheduler\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: my-scheduler-as-volume-scheduler\nsubjects:\n- kind: ServiceAccount\n  name: my-scheduler\n  namespace: kube-system\nroleRef:\n  kind: ClusterRole\n  name: system:volume-scheduler\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: my-scheduler-extension-apiserver-authentication-reader\n  namespace: kube-system\nroleRef:\n  kind: Role\n  name: extension-apiserver-authentication-reader\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\n  name: my-scheduler\n  namespace: kube-system\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-scheduler-config\n  namespace: kube-system\ndata:\n  my-scheduler-config.yaml: |\n    apiVersion: kubescheduler.config.k8s.io/v1beta2\n    kind: KubeSchedulerConfiguration\n    profiles:\n      - schedulerName: my-scheduler\n    leaderElection:\n      leaderElect: false    \n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    component: scheduler\n    tier: control-plane\n  name: my-scheduler\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      component: scheduler\n      tier: control-plane\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: scheduler\n        tier: control-plane\n        version: second\n    spec:\n      serviceAccountName: my-scheduler\n      containers:\n      - command:\n        - /usr/local/bin/kube-scheduler\n        - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml\n        image: gcr.io/my-gcp-project/my-kube-scheduler:1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTPS\n          initialDelaySeconds: 15\n        name: kube-second-scheduler\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10259\n            scheme: HTTPS\n        resources:\n          requests:\n            cpu: '0.1'\n        securityContext:\n          privileged: false\n        volumeMounts:\n          - name: config-volume\n            mountPath: /etc/kubernetes/my-scheduler\n      hostNetwork: false\n      hostPID: false\n      volumes:\n        - name: config-volume\n          configMap:\n            name: my-scheduler-config\n</code></pre></p> Bash<pre><code>kubectl apply -f my-new-scheduler.yaml\n</code></pre>"},{"location":"docs/kubernetes/multiple-schedulers/#step-3-verify-the-new-scheduler-is-running","title":"Step 3: Verify the new scheduler is running","text":"Bash<pre><code>kubectl get pods -n kube-system\n\n# output\nNAME                                           READY     STATUS    RESTARTS   AGE\n....\nmy-scheduler-lnf4s-4744f                       1/1       Running   0          2m\n...\n</code></pre>"},{"location":"docs/kubernetes/multiple-schedulers/#step-4-create-a-pod-with-the-new-scheduler","title":"Step 4: Create a Pod with the new scheduler","text":"pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: sample-pod\n      image: ubuntu\n  schedulerName: my-new-scheduler # the name of the scheduler\n</code></pre> <p>When you create a pod with the <code>schedulerName</code> field, the pod will be scheduled by the specified scheduler. You can see the pod assignment events by running the following command:</p> Bash<pre><code># method 1 ---&gt; view the events of the pod\nkubectl get events -o wide\n\n# output\nLAST SEEN   TYPE     REASON      OBJECT       SUBOBJECT                 SOURCE                                                            MESSAGE                                                              FIRST SEEN   COUNT   NAME\n10s         Normal   Scheduled   pod/ubuntu                             custom-scheduler, custom-scheduler-kind-cluster-control-plane     Successfully assigned default/ubuntu to kind-cluster-control-plane   10s          1       ubuntu.18159790928c8e97\n10s         Normal   Pulling     pod/ubuntu   spec.containers{ubuntu}   kubelet, kind-cluster-control-plane                               Pulling image \"ubuntu\"                                               10s          1       ubuntu.18159790b760375c\n\n# method 2 ---&gt; view the logs of the scheduler\nkubectl logs &lt;your-new-scheduler-pod-name&gt; -n kube-system\nkubectl logs my-new-scheduler -n kube-system\n\n# output\nI1229 05:00:45.515663       1 serving.go:380] Generated self-signed cert in-memory\nI1229 05:00:47.196685       1 server.go:154] \"Starting Kubernetes Scheduler\" version=\"v1.30.0\"\nI1229 05:00:47.196790       1 server.go:156] \"Golang settings\" GOGC=\"\" GOMAXPROCS=\"\" GOTRACEBACK=\"\"\nI1229 05:00:47.208976       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259\nI1229 05:00:47.209676       1 tlsconfig.go:240] \"Starting DynamicServingCertificateController\"\nI1229 05:00:47.212183       1 configmap_cafile_content.go:202] \"Starting controller\" name=\"client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\"\nI1229 05:00:47.212327       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\nI1229 05:00:47.212336       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController\nI1229 05:00:47.212350       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController\nI1229 05:00:47.212578       1 configmap_cafile_content.go:202] \"Starting controller\" name=\"client-ca::kube-system::extension-apiserver-authentication::client-ca-file\"\nI1229 05:00:47.212627       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file\nI1229 05:00:47.311593       1 leaderelection.go:250] attempting to acquire leader lease kube-system/my-new-scheduler...\nI1229 05:00:47.312578       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController\nI1229 05:00:47.312641       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\nI1229 05:00:47.312773       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file\nI1229 05:01:05.150116       1 leaderelection.go:260] successfully acquired lease kube-system/my-new-scheduler\n</code></pre>"},{"location":"docs/kubernetes/multiple-schedulers/#scheduler-priority-and-plugins","title":"Scheduler Priority and Plugins","text":"<p>We know that pods are sorted based on the priority defined on the pods, you can read more from this page kube-scheduler.</p> <p>To set a priority for a pod, you need to create a priority class first and apply it to the pod. Reference</p> priority-class.yaml<pre><code>apiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-priority\nvalue: 1000000 # the higher the value, the higher the priority\nglobalDefault: false\ndescription: \"This priority class should be used for XYZ service pods only.\"\n</code></pre> sample-pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  priorityClassName: high-priority\n  containers:\n    - name: sample\n      image: ubuntu\n</code></pre> <ul> <li><code>priorityClassName</code> - It determines the scheduling priority of the Pod. Pods with higher priority are scheduled before Pods with lower priority.</li> </ul> <pre><code>flowchart LR\n  subgraph scheduler [Scheduling Queue]\n    pod1\n    pod2\n    pod3\n  end\n\n  scheduler --&gt;  filtering\n  filtering --&gt; scoring\n  scoring --&gt; binding</code></pre> <p>Steps of how a pod is scheduled:</p> <ol> <li>The pods with higher priority are scheduled first by performing sorting based on values (beginning of the queue).</li> <li>The pod will enter the filtering phase where the scheduler will check the nodes based on the node selector and affinity rules. Also, the scheduler will check the resources (CPU, memory) of the nodes to ensure that the pods can be scheduled on the nodes.</li> <li>Then the node will enter the scoring phase where the scheduler will score the nodes based on the resources. The node with the highest score will be selected to schedule the pod.</li> <li>Binding the pod to the node with the highest score.</li> </ol>"},{"location":"docs/kubernetes/multiple-schedulers/#scheduler-plugins","title":"Scheduler Plugins","text":"<p>Reference</p> <p>Actually, every steps that I mentioned got its own plugins, for example</p> <ul> <li> <p>Scheduling Queue plugins</p> <ul> <li>PrioritySort - Sort the pods based on the priority of the pods.</li> </ul> </li> <li> <p>Filtering</p> <ul> <li>NodeResourcesFit - Identify the nodes that have enough resources to run the pod.</li> <li>NodeName - Check the pod has a specific node name mentioned in the pod spec.</li> <li>NodeUnschedulable - Check the node is unschedulable or not. You can use this command to check the node unschedulable status <code>kubectl describe node &lt;node-name&gt;</code></li> </ul> </li> <li> <p>Scoring</p> <ul> <li>NodeResourcesFit - Score the nodes based on the resources. Remember a single plugin can be used in multiple phases.</li> <li>ImageLocality - Score the nodes based on the container image that the pod runs. Meaning, it will select the node that has the container image cached. What if there is no nodes available? It will still place the pod on a node that doesn't have the container image cached.</li> </ul> </li> <li> <p>Binding</p> <ul> <li>DefaultBinder - Bind the pod to the node.</li> </ul> </li> </ul> <p>Actually, you can write your own plugins to extend the scheduler functionalities. We call it as extension points. For example, you can write a plugin to check the node health on the filtering phase. Reference</p>"},{"location":"docs/kubernetes/multiple-schedulers/#scheduling-profiles","title":"Scheduling Profiles","text":"<p>Reference</p> my-new-scheduler.yaml<pre><code>apiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n  - schedulerName: my-new-scheduler-1\n  - schedulerName: my-new-scheduler-2\n  - schedulerName: my-new-scheduler-3\n</code></pre> test-scheduler.yaml<pre><code>apiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n  - schedulerName: test-scheduler\n</code></pre> <p>Let's said we deploy separate schedulers each with a separate scheduler binary and configuration file. It's a lot of work required to manage these separate processes and due to separate processes, the other scheduler may schedule a pod on a node without considering the other scheduler's decision (race condition).</p> <p>So, we can use scheduling profiles to configure multiple schedulers in a single process.</p> scheduler-config.yaml<pre><code>apiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n  - schedulerName: default-scheduler\n  - schedulerName: my-new-scheduler-1\n    plugins:\n      score:\n        disabled: \n          - name: TaintToleration\n        enabled:\n          - name: CustomPlugin1\n          - name: CustomPlugin2\n          - name: CustomPlugin3\n  - schedulerName: no-scoring-scheduler\n    plugins:\n      preScore:\n        disabled:\n        - name: '*'\n      score:\n        disabled:\n        - name: '*'\n</code></pre>"},{"location":"docs/kubernetes/mutual-tls/","title":"Mutual TLS","text":"<p>Mutual TLS (Transport Layer Security) is a security protocol that ensures both the client and server authenticate each other during a secure connection. This is achieved through the exchange and validation of digital certificates, which include public keys. Each party also uses its private key to prove ownership of the certificate and establish trust.</p> <p>In the traditional TLS model, only the server's identity is verified/authenticated by the client. In contrast, mutual TLS requires both parties to present and validate their digital certificates, establishing a two-way authentication process. The private keys remain securely stored and are used to sign data, while the public keys in the certificates are used for verification.</p> <pre><code>sequenceDiagram\n    participant Client\n    participant Server\n    participant CA as Certificate Authority (CA)\n\n    Client-&gt;&gt;Server: Initiates connection request\n    Server-&gt;&gt;Client: Sends server's certificate\n    Client-&gt;&gt;CA: Validates server's certificate\n    CA--&gt;&gt;Client: Confirms server's certificate is valid\n    Client-&gt;&gt;Server: Proceeds with mTLS handshake\n    Client-&gt;&gt;Server: Sends client's certificate\n    Server-&gt;&gt;CA: Validates client's certificate\n    CA--&gt;&gt;Server: Confirms client's certificate is valid\n    Server-&gt;&gt;Client: Confirms mutual authentication\n    Client-&gt;&gt;Server: Key exchange (e.g., Diffie-Hellman)\n    Server-&gt;&gt;Client: Key exchange (e.g., Diffie-Hellman)\n    Client-&gt;&gt;Server: Secure connection established\n    Server-&gt;&gt;Client: Secure connection established</code></pre> <ol> <li>Client Initiates Connection: The client begins by sending a request to the server to establish a secure connection (e.g., using the TLS protocol).</li> <li>Server's Certificate: The server sends its digital certificate, which includes its public key, to the client. The client will validate this certificate against its trusted CA store.</li> <li>Client Validates Server's Certificate: The client verifies the server's certificate by checking the signature against the trusted Certificate Authority (CA) it has in its trusted store. This process confirms that the server's certificate is valid and trusted.</li> <li>Certificate Presentation: If the server's certificate is valid, the client proceeds with the mTLS handshake process.</li> <li>Client Sends Its Certificate: The client then sends its own digital certificate, which also contains its public key, to the server for validation.</li> <li>Server Validates Client's Certificate: The server checks the client\u2019s certificate against its trusted CA store to ensure it is valid and trusted.</li> <li>Mutual Authentication: Once both the client and server validate each other's certificates, they can confirm mutual authentication.</li> <li>Key Exchange: During the handshake, both the client and server use their private keys and a key exchange algorithm (e.g., Diffie-Hellman) to establish a shared session key for encrypting subsequent traffic.</li> <li>Secure Connection Established: A secure, encrypted connection is established between the client and server, ensuring mutual trust and privacy.</li> </ol>"},{"location":"docs/kubernetes/namespace/","title":"Namespace","text":""},{"location":"docs/kubernetes/namespace/#what-is-namespace","title":"What is Namespace?","text":"<pre><code>---\ntitle: namespace\n---\nflowchart\n    subgraph default1[default]\n        deployment1[deployment]\n        service1[service]\n        other-resources1[other resources]\n    end\n\n    subgraph dev\n        deployment2[deployment]\n        service2[service]\n        other-resources2[other resources]\n    end</code></pre> <p>Namespace used to isolate the resources within a single cluster. Therefore, each namespace can have its own policies, permissions (RBAC), resource control, etc. In other words, it is used to isolate the users' accessibility.</p> <p>By default, Kubernetes will automatically create 4 default namespaces:</p> <ul> <li> <p>default</p> <ul> <li>This is the namespace you can start to deploy the resources without creating a new namespace, when you start using the new cluster.</li> </ul> </li> <li> <p>kube-system</p> <ul> <li>This is the namespace for objects created by the Kubernetes system for itsa internal purpose, for example, kube-dns, kube-proxy, kubernetes-dashboard, ingresses, etc.</li> </ul> </li> <li> <p>kube-public</p> <ul> <li>Basically this namespace contains the resources that is readable and visible publicly by all users without any authentication.</li> <li>Mostly reserved for cluster usage. <code>kubectl cluster-info</code></li> <li>It contains a single ConfigMap object, basically the cluster info is mainly used for aids discovery and security bootstrap. <code>kubectl get configmap -n kube-public</code></li> </ul> </li> <li> <p>kube-node-lease</p> <ul> <li>This namespace holds Lease objects associated with each node. That means node leases will allow the kubelet to send heartbeats so that the control plane (Master Node <code>&lt;-</code> Node controller) can detect node failure.</li> <li>Basically this namespace related to cluster scaling.</li> </ul> </li> </ul>"},{"location":"docs/kubernetes/namespace/#namespace-usage","title":"Namespace usage","text":""},{"location":"docs/kubernetes/namespace/#commands","title":"Commands","text":"Bash<pre><code>kubectl get &lt;resource&gt; -n &lt;namespace-name&gt;\nkubectl get &lt;resource&gt; --namespace=&lt;namespace-name&gt;\nkubectl get &lt;resource&gt; --all-namespaces\n\n# example\nkubectl get pods -n dev\n\n# create namespace\nkubectl create namespace dev\n</code></pre>"},{"location":"docs/kubernetes/namespace/#switch-namespace-in-current-context-permanently","title":"Switch namespace in current context permanently","text":"<p>If we want to switch to other namespace permanently, we can do the following commands, so that we don't have to specify the namespace option.</p> Bash<pre><code>kubectl config set-context $(kubectl config current-context) --namespace=&lt;namespace-name&gt;\n\nkubectl get &lt;resource&gt; # it will by default show your namespace environment resources\n</code></pre> <p>Context is a set of access parameters that define a cluster, namespace, and user in Kubernetes. They actually stored in YAML file <code>kubeconfig</code>, and are used to manage multiple clusters or environments from the same management system.</p>"},{"location":"docs/kubernetes/namespace/#create-a-namespace-in-yaml","title":"Create a namespace in YAML","text":"YAML<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n</code></pre>"},{"location":"docs/kubernetes/namespace/#use-namespace-in-resources-yaml","title":"Use namespace in resources YAML","text":"YAML<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-nginx-pod\n  namespace: dev\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n</code></pre>"},{"location":"docs/kubernetes/namespace/#connect-to-other-namespace-services","title":"Connect to other namespace services","text":"<pre><code>flowchart\n    subgraph prod\n        deployment2[deployment]\n        service2[service]\n    end\n\n  subgraph default1[default]\n        deployment1[deployment]\n        service1[service]\n    end</code></pre> <p>If you want to connect to other namespace services, then you have to reference the DNS of the respective namespace, as a DNS entry will automatically added in this format when the service is created. Here is the format;</p> <ul> <li>Format: <code>&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local</code></li> <li>Example: <code>db-svc.prod.svc.cluster.local</code><ul> <li><code>cluster.local</code> = default domain name of the Kubernetes cluster</li> </ul> </li> </ul>"},{"location":"docs/kubernetes/namespace/#resource-quota-for-namespace","title":"Resource quota for namespace","text":"<p>You can limit the resources to be used within a namespace. You can set each namespace with a guaranteed amount and not use more than the limit.</p> <ul> <li>Scope: Applies to an entire namespace</li> <li>Purpose: Enforces overrall resource usage limits for all pods in a namespace</li> <li>Usage: Ensure that the total resource usage consumption (eg, CPU, memory, number of pods) in a namespace does not exceed the specified limits.</li> </ul> resource-quota.yaml<pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: team-a-quota\n  namespace: dev\nspec:\n  hard:\n    pods: \"15\" # total pods cannot exceed 15\n    requests.cpu: \"2\" # total CPU requests cannot exceed 2 cores\n    requests.memory: 2Gi # total memory requests cannot exceed 2Gi\n    limits.cpu: \"4\" # total CPU limits cannot exceed 4 cores\n    limits.memory: 4Gi # total memory limits cannot exceed 4Gi\n</code></pre> <p>In this case, this <code>dev</code> namespace can only create a maximum of 15 pods. Each pod in the dev namespace will have 2 CPUs and 2G of memory, while the maximum CPU and memory limits are 4 and 4G, respectively.</p> <p>You can check the resource quota by running the following command: Bash<pre><code>kubectl get quota -n dev\nkubectl describe quota team-a-quota -n dev\n</code></pre></p>"},{"location":"docs/kubernetes/namespace/#limit-range-for-namespace","title":"Limit range for namespace","text":"<p>Important</p> <p>The max and min values in a LimitRange apply to both resource requests and limits.</p> <p>You can also set the default minimum and maximum limits for the resources like CPU and memory for pods in the namespace. With this setup, we can ensure that all the pods created in the namespace will have the same limits. Remember, if you just create or change a limit range, it will not affect the existing pods.</p> <ul> <li>Scope: Applies to individual containers or pods within a namespace</li> <li>Purpose: Sets default, minimum, and maximum resource usage limits for containers or pods</li> <li>Usage: Ensures that each container or pod has resource requests and limits within specified bounds, providing a way to control resource allocation at a finer granularity.</li> </ul> <p>cpu-limit-range.yaml<pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpe-resource-constraint\nspec:\n  limits:\n    - default: # specify the default limits for containers\n        cpu: 500m # Default CPU limit is 500m\n        memory: 512Mi # Default memory limit is 512Mi\n      defaultRequest: # specify the default request for containers\n        cpu: 500m # Default CPU request is 500m\n        memory: 512Mi # Default memory request is 512Mi\n      max: # Specify the maximum limit for containers\n        cpu: \"1\" # Maximum CPU limit is 1 core\n        memory: 1Gi # Maximum memory limit is 1Gi\n      min: # Specify the minimum limit for containers\n        cpu: 100m # Minimum CPU limit is 100m\n        memory: 256Mi # Minimum memory limit is 256Mi\n      type: Container\n</code></pre> With this setup, we specify the default CPU and memory limits for the pods in the namespace are <code>500m</code> and <code>512Mi</code>, respectively. The default request for CPU and memory are also set to <code>500m</code> and <code>512Mi</code>, respectively.</p> <ul> <li><code>max</code> - the maximum limit for CPU and memory that can be set on a container<ul> <li>If you explicitly specify the resource requests and limits, they must not be higher than max values</li> </ul> </li> <li><code>min</code> - the minimum limit for CPU and memory that can be set on a container<ul> <li>If you explicitly specify the resource requests and limits, they must not be lower than min values</li> </ul> </li> </ul> <p>Example Scenarios:</p> <ul> <li>No resources specified:<ul> <li>The container will get 500m CPU and 512Mi memory (default values).</li> </ul> </li> <li>Explicitly set 200m CPU and 300Mi memory:<ul> <li>This is valid because it is within the min and max range.</li> </ul> </li> <li>Explicitly set 50m CPU and 200Mi memory:<ul> <li>This will fail because it is below the min values.</li> </ul> </li> <li>Explicitly set 2 CPU and 2Gi memory:<ul> <li>This will fail because it exceeds the max values.</li> </ul> </li> </ul>"},{"location":"docs/kubernetes/node-affinity/","title":"Node Affinity","text":"<p>Note</p> <p>Actually, you can combine taints and tolerations with node affinity to schedule pods on specific nodes. Remember, you need to apply taints and tolerations to the nodes first before you use node affinity. For example, you can apply a taint to your node that has a large GPU to prevent other pods from being scheduled on your node. Then you can use node affinity to prevent your pods from being scheduled on other nodes.</p>"},{"location":"docs/kubernetes/node-affinity/#concept-and-usage-of-node-affinity","title":"Concept and Usage of Node Affinity","text":"<p>The concept of Node Affinity is basically allows the users to specify rules for placing/scheduling pods on specific nodes. For example, if we want to schedule a pod on a node that has a large or medium GPU. We can use Node Affinity to specify the rules for scheduling the pod on the node that has a large or medium GPU.</p> sample-pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: sample-container\n      image: nginx\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n            - key: gpu\n              operator: In # Got other operators like NotIn, Exists\n              values:\n                - large\n                - medium\n</code></pre> <ul> <li>Operator Reference</li> <li><code>Exists</code> operator is used to check if the label exists on the node. So, when you set <code>Exists</code>, then you don't need to specify the <code>values</code> field, as it does not compare the values.</li> </ul>"},{"location":"docs/kubernetes/node-affinity/#node-affinity-types","title":"Node Affinity Types","text":"<p>Reference</p> <p>Let me explain DuringScheduling and DuringExecution.</p> <ul> <li> <p><code>DuringScheduling</code> - The pod does not exist on the node, so the rules must be met for the pod to be scheduled on the node.</p> <ul> <li><code>requiredDuringScheduling</code> will be used when the pod must be scheduled on the node that satisfies the rule.</li> <li><code>preferredDuringScheduling</code> will be used when the pod is less important to be scheduled on the node that satisfies the rule, as if the node does not satisfy the rule, then the pod will be scheduled on the other node.</li> </ul> </li> <li> <p><code>DuringExecution</code> - The pod has been running on the node, so the rules must be met for the pod to continue running on the node. For example, if the admin changes the node label, what happen to the existing pods that being deployed?</p> <ul> <li><code>RequiredDuringExecution</code> - The pod will be evicted if the node label is changed or the pod does not satisfy the rule.</li> <li><code>IgnoredDuringExecution</code> - The pod will continue to run, even if the node label is changed or the pod does not satisfy the rule.</li> </ul> </li> </ul> <p>Node Affinity Types:</p> <ul> <li> <p><code>requiredDuringSchedulingIgnoredDuringExecution</code> - The pod must be scheduled on the node that satisfies the rule, if does not satisfy the rule, then the pod will not be scheduled on the node. Then, if the node labels are changed, the pod will still continue to run (IgnoredDuringExecution).</p> </li> <li> <p><code>preferredDuringSchedulingIgnoredDuringExecution</code> - The pod is less important to be scheduled on the node that satisfies the rule, if does not satisfy the rule, then the pod will be scheduled on the other node. Then, if the node labels are changed, the pod will still continue to run (IgnoredDuringExecution).</p> </li> <li> <p><code>requiredDuringSchedulingRequiredDuringExecution</code> - The pod must be scheduled on the node that satisfies the rule, if does not satisfy the rule, then the pod will not be scheduled on the node. Then, if the node labels are changed, the pod will be evicted.</p> </li> <li> <p><code>preferredDuringSchedulingRequiredDuringExecution</code> - The pod is less important to be scheduled on the node that satisfies the rule, if does not satisfy the rule, then the pod will be scheduled on the other node. Then, if the node labels are changed, the pod will be evicted.</p> </li> </ul>"},{"location":"docs/kubernetes/node-selectors/","title":"Node Selectors","text":"<p>Node selectors are used to specify the nodes where a pod should be scheduled by using labels.</p> <pre><code>flowchart\n  node1[Node 1]\n  node2[Node 2]</code></pre> <p>Assume you have a pod that requires a GPU, but we know that Node 1 has GPU and Node 2 does not have a GPU. So, in this case, we want to schedule this pod on Node 1. There are two ways to achieve this:</p> <ul> <li>Node Selectors</li> <li>Node Affinity</li> </ul> <p>Before we dive into Node Selectors, let's first understand how to label a node.</p> Bash<pre><code>kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt;\nkubectl label nodes node1 gpu=large\n</code></pre> <p>Then we only customize the pod definition file to include the node selector.</p> sample-pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: sample-container\n      image: nginx\n  nodeSelector:\n    gpu: large\n</code></pre> <p>Now of course, this node selector method is not flexible enough to handle more complex scenarios. For example, if we want to schedule a pod on a node that has a large or medium GPU. We can't achieve this using node selectors. In this case, we need to use Node Affinity.</p>"},{"location":"docs/kubernetes/os-upgrades/","title":"OS Upgrades","text":"<p>Summary (Formula);</p> <ul> <li>drain = terminate + schedule + cordoned</li> <li>cordon = cordoned</li> </ul>"},{"location":"docs/kubernetes/os-upgrades/#introduction-of-os-upgrades","title":"Introduction of OS Upgrades","text":"<pre><code>flowchart\n  subgraph \"Cluster\"\n    node1[\"Node 1\"]\n    node2[\"Node 2\"]\n  end</code></pre> <p>Assume you have a cluster with 2 nodes. Now, you want to upgrade one of the nodes to a new version. If the node is down for more than 5 minutes, all the pods running on that node will be dead, else the pods will come back online when the <code>kubectl</code> process starts. If the pods are part of a replicaset, those pods will be provisioned on other nodes.</p> <p>In this case, we need to upgrade the node in a safer way.</p> Bash<pre><code>kubectl drain &lt;node-name&gt;\n# For pods, you have to force it, but the pod will lose forever\nkubectl drain &lt;node-name&gt; --force\nkubectl drain &lt;node-name&gt; --ignore-daemonsets\n</code></pre> <p>What we are doing her is, we are draining the node. When we drain the node, the pods are gracefully terminated from the node and are scheduled on other nodes (it does not move from one node to another). Besides, the node is also marked as unschedulable or cordoned, that means no new pods will be scheduled on that node until you uncordon it.</p> Bash<pre><code>kubectl uncordon &lt;node-name&gt;\n</code></pre> <p>When the node is come back online, you have to uncordon it, so that the node is not marked as unschedulable. Once you uncordon it, the pods can be scheduled on that node again.</p> <p>One thing we have to keep in mind it, those pods that are scheduled on other nodes won't automatically come back to the original node.</p> Bash<pre><code>kubectl cordon &lt;node-name&gt;\n</code></pre> <p>If you want to make the node unschedulable or cordoned, you can use this command. This will make sure that no new pods are scheduled on that node.</p>"},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/","title":"Overview of Kubernetes Compliance and Security Frameworks","text":""},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/#compliance-and-security-frameworks","title":"Compliance and Security Frameworks","text":"<p>Understand what is compliance frameworks?</p> <p>Compliance Frameworks are structured guidelines and best practices that organizations follow to ensure they meet regulatory requirements and industry standards. Meaning that, it defines what to do or what needs to be done to meet legal, regulatory, and industry standards.</p> <p>There are some common compliance frameworks;</p> <ul> <li>General Data Protection Regulation (GDPR)</li> <li>Health Insurance Portability and Accountability Act (HIPAA)</li> <li>Payment Card Industry Data Security Standard (PCI DSS)</li> <li>National Institute of Standards and Technology (NIST)</li> <li>Center for Internet Security (CIS) Benchmarks</li> <li>System and Organization Controls (SOC 2)</li> </ul> Frameworks Purpose When to Apply Key Focus Areas &amp; How to Use Tools GDPR Protect personal data and privacy of EU citizens. When handling data of EU citizens or operating in the EU. Data protection, privacy, breach notification. Implement data encryption, access controls, and ensure compliance with data subject rights. Kubernetes Secrets, Open Policy Agent (OPA), HashiCorp Vault. HIPAA Protect sensitive healthcare information. When handling healthcare data in the U.S. Data encryption, access controls, audit logging. Configure Kubernetes to secure PHI (Protected Health Information). Kubernetes RBAC, Falco, Aqua Security. PCI DSS Secure payment card data. When processing or storing payment card information. Network segmentation, secure storage of sensitive data. Segment networks, secure Kubernetes secrets, and monitor access logs. Sysdig Secure, Trivy, Kubernetes Network Policies. NIST Provide security guidelines for IT systems, including containers. When implementing federal or enterprise-level security standards. Container security, runtime environments, orchestration platforms. Follow NIST SP 800-190 for container security best practices. Anchore, Prisma Cloud, Kubernetes Benchmarks. CIS Provide best practices for securing Kubernetes clusters. When securing Kubernetes configurations and runtime environments. Configuration best practices, runtime security. Apply CIS Benchmarks for Kubernetes to harden cluster security. kube-bench, kube-hunter, CIS-CAT. SOC 2 Ensure trust principles like security, availability, and confidentiality. When providing services that require assurance of data security and privacy. Security, availability, confidentiality. Implement controls to meet SOC 2 trust principles and monitor compliance. Datadog, Splunk, Kubernetes Audit Logs."},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/#threat-modeling-frameworks","title":"Threat Modeling Frameworks","text":"<p>Understand what is threat modeling frameworks?</p> <p>We know that Compliance Frameworks define what to do or what needs to be done to meet legal, regulatory, and industry standards, but it doesn't tell you how to do it. This is where Threat Modeling Frameworks come into play. </p> <p>Threat Modeling Frameworks define how to do it by providing a structured approach to identifying and addressing potential security threats and suggesting mitigation strategies to secure your Kubernetes environment.</p> <p>There are some popular threat modeling frameworks;</p> <ul> <li>STRIDE</li> <li>DREAD</li> <li>MITRE ATT&amp;CK</li> </ul>"},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/#stride","title":"STRIDE","text":"<p>What is Data Integrity?</p> <p>Data integrity refers to the accuracy, consistency, and reliability of data throughout its lifecycle. It ensures that data remains unaltered during storage, transmission, and retrieval, except by authorized processes or users. Maintaining data integrity is critical for preventing unauthorized modifications, corruption, or loss of data.</p> <p>STRIDE (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege) is a widely used threat modeling framework to identify and address security threats in Kubernetes clusters. It helps teams systematically analyze potential threats and design appropriate mitigations.</p> Threat Description Example Mitigation Spoofing Attacker impersonates (pretend) a user, system, or entity as a legitimate user to gain unauthorized access. An attacker uses stolen credentials to access a Kubernetes cluster as an administrator. Use strong authentication mechanisms like multi-factor authentication (MFA) and enforce Role-Based Access Control (RBAC) in Kubernetes. Tampering Attacker modifies data or configurations to compromise integrity. An attacker alters data being sent to a Kubernetes API server, leading to unauthorized changes in the cluster. Use immutable infrastructure, enable audit logging, encryption for data in transit, and digital signatures for data integrity. Repudiation The ability of a user to deny performing an action because there is no evidence to trace the action back to them. This can lead to situations where a user can claim they didn't perform a certain action, such as making unauthorized transactions, accessing restricted data, or any other activity that could have negative consequences. A malicious user deletes Kubernetes pods, but there are no logs to identify who performed the action. Enable Kubernetes audit logs and integrate them with tools like Splunk or Elasticsearch for traceability. Information Disclosure Unauthorized access or exposure of sensitive information to unauthorized entities. Sensitive data in Kubernetes secrets is exposed to unauthorized users. Encrypt sensitive data at REST and in transit, use tools like HashiCorp Vault for secret management, and implement network policies to restrict access. Denial of Service (DoS) Overloading a system to make it unavailable to legitimate users. An attacker floods the Kubernetes API server with requests, causing it to become unresponsive. Implement rate limiting, network policies, and resource quotas to prevent abuse. Elevation of Privilege Gaining higher privileges (eg: admin rights) than intended. A compromised container gains root access to the host node and escalates privileges across the cluster. Use Pod Security Policies (PSPs), enforce least privilege principles, run containers as non-root users, and implement strict RBAC policies."},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/#dread","title":"DREAD","text":"<p>DREAD (Damage, Reproducibility, Exploitability, Affected Users, Discoverability) helps prioritize threats based on their potential impact and likelihood. It assigns a score to each threat based on these factors, allowing teams to focus on the most critical threats first.</p> Factor Description Key Question Example Damage The potential impact or severity of the threat if it is successfully exploited. How much damage will the attack cause? A compromised Kubernetes node allows attackers to access sensitive data across the cluster. Reproducibility How easily the threat can be reproduced or repeated by an attacker. How easy is it to reproduce the attack? An attacker can repeatedly exploit a misconfigured API server to gain unauthorized access. Exploitability The effort or resources required to exploit the threat. How much effort is required to exploit the vulnerability? Exploiting a weak password policy requires minimal effort using automated tools like brute force scripts. Affected Users The number of users or systems impacted by the threat. How many users or systems will be affected? A DoS attack on the Kubernetes API server affects all users and workloads relying on the cluster. Discoverability How easily the threat can be discovered by an attacker. How easy is it to discover the vulnerability? An exposed Kubernetes dashboard with no authentication is easily discoverable through network scanning tools."},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/#mitre-attck","title":"MITRE ATT&amp;CK","text":"<p>More Information</p> <p>Threat Matrix for Kubernetes</p> <p>MITRE ATT&amp;CK is a comprehensive knowledge base of adversary tactics and techniques based on real-world observations. It provides a framework for understanding how attackers operate and helps organizations identify potential threats to their Kubernetes environments.</p> <ul> <li>tactics - what attackers aim to do?</li> <li>techniques - how attackers do it?</li> </ul>"},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/#tactics","title":"Tactics","text":"<p>Tactics represent the high-level goals that an attacker is trying to achieve during an attack. Each tactic defines a specific phase in the attack lifecycle.</p> Tactic Description Example Initial Access Techniques used to gain initial entry into a system. Phishing, exploiting software vulnerabilities. Execution Techniques the attacker uses to run malicious code on a system. Scripting, command-line interfaces. Persistence Methods an attacker uses to maintain access even after a system reboot or interruptions. Setting up startup scripts, modifying registry entries. Privilege Escalation Techniques used to gain higher-level permissions on a system or network. Exploiting vulnerabilities or misconfigurations. Defense Evasion Methods used to avoid detection by security tools. Obfuscation, file modification. Credential Access Techniques for stealing user credentials. Keylogging, credential dumps. Discovery Methods to gather information about a network or system to further the attack. Scanning, enumeration. Lateral Movement Techniques used to move within a network. Exploiting trust relationships, remote services. Collection Gathering data of interest, such as sensitive files or credentials. Data from local files, browser history. Exfiltration Techniques to transfer data out of the target network. Using encrypted channels, covert channels. Impact Techniques that result in disruption or damage to systems. Data encryption, destruction of data."},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/#techniques","title":"Techniques","text":"<p>Techniques are specific methods employed by attackers to accomplish a tactic. Each technique can have multiple variants that describe variations on how the technique may be executed.</p> Tactic Technique Example Execution PowerShell Using PowerShell scripts to execute commands. Execution JavaScript Running malicious JavaScript code in a web browser."},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/#sub-techniques","title":"Sub-techniques","text":"<p>Some techniques have sub-techniques that provide more granularity.</p> Technique Sub-technique Description Credential Dumping LSASS Memory Extracting credentials from the Local Security Authority Subsystem Service memory."},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/#mitigation-and-detection","title":"Mitigation and Detection","text":"<p>For each technique, MITRE ATT&amp;CK typically provides recommendations for mitigations and detection strategies.</p> Aspect Description Mitigation Recommendations to prevent or reduce the impact of a technique. Detection Strategies to identify and respond to potential exploitations by adversaries."},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/#supply-chain-compliance-frameworks","title":"Supply Chain Compliance Frameworks","text":"<p>Understand what is supply chain compliance frameworks?</p> <p>We know that Compliance Frameworks define what to do or what needs to be done to meet legal, regulatory, and industry standards and Threat Modeling Frameworks define how to do it by providing a structured approach to identifying and addressing potential security threats.</p> <p>Both of the frameworks are securing the internal components of the Kubernetes environment or application , what about the external components your application depends on? (For example, libraries, APIs, third-party services, docker images, etc.)</p> <p>All these need to be secured and compliant as well, and this is where Supply Chain Compliance Frameworks come into play. Supply Chain Compliance Frameworks are structured guidelines and best practices that organizations follow to ensure they meet regulatory requirements and industry standards for their supply chain.</p> <ul> <li>verify the integrity and authenticity of external components and services that integrates into the Kubernetes environment or application.</li> </ul> <p>More Information</p> <p>https://www.cncf.io/blog/2022/04/12/a-map-for-kubernetes-supply-chain-security/</p> <p>There are some key components to secure supply chain;</p> <ul> <li>Artifact - The binaries and container images are signed and verified to ensure their integrity and authenticity.</li> <li>Metadata - SBOM will detail all the components, libraries, and dependencies that are included in a software artifact, which can be used to assess security risks and vulnerabilities.</li> <li>Attestation - Signed statements about the software artifacts and their metadata like SBOM. For example, this SBOM is signed by the Kubernetes project and verified by the Kubernetes community.</li> <li>Policy - Integrate policy with Kubernetes Admission Controller to enforce policies on signed artifacts and SBOMs before they are deployed into the cluster.</li> </ul>"},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/#artifact","title":"Artifact","text":"<p>References</p> <ul> <li>https://docs.sigstore.dev/cosign/</li> <li>https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-artifacts/#verifying-image-signatures</li> </ul> <p>Artifacts are the output of a build system. They are the software components that are produced by the build process. Artifacts can be things like container images, binaries, packages, tarballs, etc.</p> <p>These artifacts will be deployed into production environments. So, it's important to verify the integrity and authenticity of these artifacts before deploying them.</p> <p>In Kubernetes, artifacts are signed during the process of building and deploying using tools like Cosign and Notary. These tools can sign and verify container images and other artifacts to ensure their integrity and authenticity.</p> Bash<pre><code>cosign sign --key &lt;path-to-key&gt; &lt;image&gt;\ncosign verify --key &lt;path-to-key&gt; &lt;image&gt;\n</code></pre>"},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/#metadata","title":"Metadata","text":"<p>References</p> <p>https://github.com/anchore/syft</p> <p>Metadata is the data that describes the software artifacts. Meaning that, it provides information about what's inside the artifact. Metadata can include information like the software version, dependencies, build environment, etc. </p>"},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/#sbom","title":"SBOM","text":"<p>One of the most important type of metadata is Software Bill of Materials (SBOM) - Ingredient List. An SBOM is a list of all the components, libraries, and dependencies that are included in a software artifact. It provides a detailed inventory of the software components, their versions, and sources, which can be used to assess security risks and vulnerabilities. It contains two formats;</p> <ul> <li>CycloneDX - A lightweight SBOM standard designed for use in application security contexts and supply chain component analysis.</li> <li>SPDX - A standard format for communicating software bill of materials information, which is widely used in open source and commercial software projects.</li> </ul> <p>Here's an example of an SPDX file tailored for a Kubernetes-related project in the SPDX tag-value format:</p> Text Only<pre><code>SPDXVersion: SPDX-2.2\nDataLicense: CC0-1.0\nSPDXID: SPDXRef-DOCUMENT\nDocumentName: Kubernetes-SPDX-Example\nDocumentNamespace: http://spdx.org/spdxdocs/kubernetes-spdx-example-1.0\nCreator: Tool: SPDX-Generator-Tool-1.0\nCreated: 2025-04-19T12:00:00Z\n\n##### Package Information #####\nPackageName: kubernetes-nginx-deployment\nSPDXID: SPDXRef-Package\nPackageVersion: 1.21.0\nPackageSupplier: Organization: Kubernetes Community\nPackageDownloadLocation: https://github.com/kubernetes/kubernetes\nFilesAnalyzed: true\nPackageVerificationCode: 4edc6a7b8f9c3e2d5a6b8e7f9c3e2d5a6b8e7f9c\nPackageLicenseDeclared: Apache-2.0\nPackageCopyrightText: Copyright 2025 Kubernetes Authors\n\n##### File Information #####\nFileName: manifests/nginx-deployment.yaml\nSPDXID: SPDXRef-File-NginxDeployment\nFileType: TEXT\nFileChecksum: SHA256: 9c56e1c67a2d28fced849ee1bb76e7391b93eb12a6b8e7f9c3e2d5a6b8e7f9c3\nLicenseConcluded: Apache-2.0\nLicenseInfoInFile: Apache-2.0\nFileCopyrightText: Copyright 2025 Kubernetes Authors\n\nFileName: manifests/nginx-service.yaml\nSPDXID: SPDXRef-File-NginxService\nFileType: TEXT\nFileChecksum: SHA256: 7a2d28fced849ee1bb76e7391b93eb12a6b8e7f9c3e2d5a6b8e7f9c3e2d5a6b8\nLicenseConcluded: Apache-2.0\nLicenseInfoInFile: Apache-2.0\nFileCopyrightText: Copyright 2025 Kubernetes Authors\n\n##### Relationships #####\nRelationship: SPDXRef-Package CONTAINS SPDXRef-File-NginxDeployment\nRelationship: SPDXRef-Package CONTAINS SPDXRef-File-NginxService\n</code></pre> <p>Info</p> <p>SBOM will be generated for each release of the Kubernetes project and signed for authenticity.</p> <p>In Kubernetes, SBOMs can be generated using tools like Syft and Grype. These tools can analyze container images and generate SBOMs in various formats, including CycloneDX and SPDX.</p> Bash<pre><code>syft &lt;image&gt; -o &lt;format&gt;\nsyft &lt;image&gt; -o spdx-json &gt; sbom.json\n</code></pre> <p>To verify the integrity of the SBOM, you can use tools like Cosign to sign and verify the SBOM files. You can also use <code>sha512sum</code> to generate a checksum for the SBOM file and verify it against the original file.</p> Bash<pre><code>cosign sign --key &lt;path-to-key&gt; sbom.json\ncosign verify --key &lt;path-to-key&gt; sbom.json\n\nsha512sum sbom.json &gt; sbom.sha512\n# check the downloaded SBOM file matches the checksum\nsha512sum -c sbom.sha512 # verify the integrity of the SBOM file\n</code></pre> <p>This will ensue that the SBOM file has not been tampered with and is authentic.</p>"},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/#attestation","title":"Attestation","text":"<p>sha512sum vs Attestation</p> <p><code>sha512sum</code> is different from Attestation. <code>sha512sum</code> is used to ensure the downloaded file matches what the server published, meaning it will ensure the file's integrity during transit (file content is not changed).</p> <p>But Attestation adds another layer of security (trust) by ensuring that the file is not only intact (complete) but also authentic and verified by a trusted party. This means that the file was created by a trusted source and has not been tampered with since it was signed.</p> <p>Metadata is useful, but how do we ensure that the metadata is trustworthy? This is where Attestation comes into play. Attestation is signed statements about the software artifacts and their metadata like SBOM. For example, this SBOM is signed by the Kubernetes project and verified by the Kubernetes community.</p> <p>The trusted party (eg; Kubernetes release team) will generate the SBOM and sign it using a private key to create an attestation. The attestation will include the SBOM and a signature that can be verified (authenticity) using the public key of the trusted party.</p> <p>Here is the process of creating an attestation using Cosign: Bash<pre><code>cosign attest --key &lt;path-to-private-key&gt; --predicate sbom.json --type spdxjson sbom.json\ncosign verify-attestation --key &lt;path-to-public-key&gt; sbom.json\n</code></pre></p>"},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/#attestation-framework-in-toto","title":"Attestation Framework: in-toto","text":"<p>References</p> <p>https://in-toto.io/</p> <p>One of the widely used attestation frameworks is in-toto. It provides a framework for securing the software supply chain by creating and verifying cryptographically signed attestations. It provides a mechanism to verify that all steps in the supply chain were executed as expected and that the resulting artifacts have not been tampered with.</p>"},{"location":"docs/kubernetes/overview-of-kubernetes-compliance-and-security-frameworks/#policy","title":"Policy","text":"<p>Now, we have the signed artifacts and SBOMs with attestations, but how do we ensure that these artifacts and SBOMs are compliant with our policies at different stages of the software supply chain as well as only deploy verified trustworthy artifacts? This is where Policy comes into play. Policies are the rules and guidelines that define what is acceptable and what is not in your Kubernetes environment to ensure compliance and security.</p> <p>In this case, it can help to prevent insecure or non-compliant artifacts from being deployed into production. There are multiple tools that can enforce policies in Kubernetes;</p> <ul> <li>OPA (Open Policy Agent) - A general-purpose policy engine that can enforce policies in Kubernetes and other systems.</li> <li>Kyverno - A Kubernetes-native policy engine that can validate, mutate, and generate resources based on policies.</li> <li>Sigstore - A tool for signing and verifying container images and other artifacts, which can be used to enforce policies on signed artifacts.</li> </ul> <p>Sigstore's policy controller can be integrated with Kubernetes Admission Controller to enforce policies on signed artifacts and SBOMs before they are deployed into the cluster.</p>"},{"location":"docs/kubernetes/persistent-volume-and-claim/","title":"Persistent Volume and Persistent Volume Claim","text":"<p>Refer here for more references.</p> <p>Persistent volume (PV) is a storage resource in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. PV has its own lifecycle, that means it does not tied to any pods.</p> <p>If you want to use a PV in your pod, you need to create a Persistent Volume Claim (PVC) that requests (claims as volumes) a PV with specific storage requirements. The PVC will be bound to a PV that meets the requirements. Remember, PV and PVC are bound together, meaning one PV can only be bound to one PVC. There is one-to-one relationship between PV and PVC, so no other PVC can use the same PV (remaining capacity in the volume).</p>"},{"location":"docs/kubernetes/persistent-volume-and-claim/#step-1-create-a-pv","title":"Step 1: Create a PV","text":"pv.yaml<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-example\n  labels:\n    type: fast-storage # label for PV (optional)\nspec:\n  capacity:\n    storage: 1Gi # reserve 1Gi storage\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /data\n</code></pre> Access Mode Description ReadWriteOnce The volume can be mounted as read-write by a single node. ReadWriteOnce access mode still can allow multiple pods to access (read from or write to) that volume when the pods are running on the same node. ReadOnlyMany The volume can be mounted as read-only by many nodes. ReadWriteMany The volume can be mounted as read-write by many nodes. ReadWriteOncePod The volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across the whole cluster cna read that PVC or write to it. Reclaim Policy Description Retain Remain the volume unless it is manually deleted. Delete Delete the volume when the PVC is deleted . <p>When you delete a PV, remember to delete the associated PVC first, otherwise, the PV will not be deleted.</p>"},{"location":"docs/kubernetes/persistent-volume-and-claim/#step-2-create-a-pvc","title":"Step 2: Create a PVC","text":"<p>When you create a PVC, Kubernetes will bind the PVC to a PV that meets the requirements. If there is no PV that meets the requirements, the PVC status will remain unbound else it will be bound to a PV.</p> pvc.yaml<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-example\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  selector: # (optional)\n    matchLabels:\n      type: fast-storage # label for PV \n</code></pre> <ul> <li><code>selector</code> is optional, if you want to bind the PVC to a specific PV, you can use the <code>selector</code> field.</li> </ul>"},{"location":"docs/kubernetes/persistent-volume-and-claim/#step-3-use-pvc-in-pod","title":"Step 3: Use PVC in Pod","text":"pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-example\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n      volumeMounts:\n        - name: my-pvc-example # volume name\n          mountPath: /usr/share/nginx/html\n  volumes:\n    - name: my-pvc-example\n      persistentVolumeClaim:\n        claimName: pvc-example\n</code></pre>"},{"location":"docs/kubernetes/pod-security-policy/","title":"Pod Security Policy","text":""},{"location":"docs/kubernetes/pod-security-policy/#overview","title":"Overview","text":"<p>Pod Security Policy (PSP) is a cluster-level resource and it's one of the admission controllers that allows you to control the security of the Kubernetes pods. It allows you to define a set of conditions that a pod must meet in order to run in the cluster.</p> <p>In Kubernetes v1.21, PSP is being deprecated and removed from Kubernets v1.25. There are multiple ways to replace PSPs</p> <ul> <li>Policy-as-code (PAC) solutions within the Kubernetes ecosystem. For example, Kyverno, Open Policy Agent (OPA), Gatekeeper, etc</li> <li>The Kubernetes Pod Security Standards (PSS) with Pod Security Admission (PSA)</li> </ul> sample-pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver-kind-cluster-control-plane\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --advertise-address=10.89.0.2\n    - --allow-privileged=true\n    - --authorization-mode=Node,RBAC\n    - --client-ca-file=/etc/kubernetes/pki/ca.crt\n    - --enable-admission-plugins=NodeRestriction,PodSecurityPolicy\n    ...\n    image: registry.k8s.io/kube-apiserver:v1.30.0\n</code></pre> <p>We have to add the PodSecurityPolicy admission controller to the kube-apiserver. So we add PodSecurityPolicy to the --enable-admission-plugins flag.</p>"},{"location":"docs/kubernetes/pod-security-policy/#usage-and-concept-of-pod-security-policy","title":"Usage and Concept of Pod Security Policy","text":"<p>sample-pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample\nspec:\n  containers:\n    - name: ubuntu\n      image: ubuntu\n      securityContext:\n        privileged: true # privileged user\n        runAsUser: 0 # root user\n        capabilities:\n          add: [\"CAP_SYS_BOOT\"]\n  volumes:\n    - name: data-volume\n      hostPath:\n        path: /data\n</code></pre> Take the above example, the pod is running as a privileged user, which means it allows the processes running in the container to have root privileges on the host. It also run as root user and has the CAP_SYS_BOOT capability, which allows the container to reboot the host. Besides, it mounts the hostPath volume, which allows the container to access the host filesystem. All these configurations are insecure and dangerous.</p>"},{"location":"docs/kubernetes/pod-security-policy/#step-1-create-pod-security-policy","title":"Step 1: Create Pod Security Policy","text":"<p>Therefore, we can create a Pod Security Policy to restrict the pod from running or prevent to create the pod with the above configurations. Pod Security Policy not only restricted the pod from creation but also enforced or mutated (defaultAddCapabilities) the pod's configuration to meet the policy.</p> pod-security-policy.yaml<pre><code>apiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: restricted-psp\nspec:\n  privileged: false\n  seLinux:\n    rule: RunAsAny\n  runAsUser:\n    rule: MustRunAsNonRoot\n  fsGroup:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  volumes:\n    - 'persistentVolumeClaim'\n  requiredDropCapabilities:\n    - 'CAP_SYS_BOOT'\n  defaultAddCapabilities: # default add capabilities to the pod\n    - 'CAP_SYS_TIME'\n</code></pre> <ul> <li>This will reject the pod if the pod privileged flag set to <code>true</code></li> <li>This will reject the pod if the pod runAsUser set to <code>0</code></li> <li>This will reject the pod if the pod is using <code>hostPath</code> volume, as it's not in the volumes list, which only allows <code>persistentVolumeClaim</code> volume</li> <li>This will reject the pod if the pod has the <code>CAP_SYS_BOOT</code> capability</li> <li>This will add the <code>CAP_SYS_TIME</code> capability to the pod default even if the pod doesn't have it</li> </ul>"},{"location":"docs/kubernetes/pod-security-policy/#step-2-authorized-the-pod-security-policy-using-rbac","title":"Step 2: Authorized the Pod Security Policy using RBAC","text":"<p><pre><code>graph LR\n  kubectl --step 1--&gt; authentication --step 2--&gt; authorization\n  authorization --step 3--&gt; ac\n  subgraph ac[Admission Controllers]\n    PodSecurityPolicy\n  end\n\n  ac --step 4--&gt; security[Authorized Security Policies API]\n  ac --step 5--&gt; a[Create Pod]</code></pre> So, the PodSecurityPolicy will check the pod's configuration before creating the pod. If the pod's configuration violates the policy, the pod will be rejected and not created. However, we just enabled the PodSecurityPolicy admission controller, but we do not authorize any security policies yet. In this case, the admission controller will not able to communicate with the pod security policies API and eventually will reject all the pods creation, even if the pod's configuration is meet the policy.</p> <p>Now, we know that every pod has a service account (default) when created even if we don't specify it, as this comes from the namespace. So, we can authorize the default service account from the namespace to access the security policies by creating a Role and RoleBinding, so that the pod can be validated again the PodSecurityPolicy.</p> rolerolebinding role.yaml<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: psp-role\nrules:\n  - apiGroups: ['policy']\n    resources: ['podsecuritypolicies']\n    resourceNames: ['restricted-psp'] # Replace with your PodSecurityPolicy name\n    verbs: ['user']\n</code></pre> rolebinding.yaml<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: psp-rolebinding\nsubjects:\n  - kind: ServiceAccount\n    name: default # replace based on your service account name\n    namespace: default # replace based on your namespace\nroleRef:\n  kind: Role\n  name: psp-role # Replace with your Role name\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"docs/kubernetes/pod-security-policy/#step-3-create-the-pod-with-correct-configuration","title":"Step 3: Create the Pod with correct configuration","text":"<p>Now, we can create the pod with the correct configuration that meets the PodSecurityPolicy.</p> sample-pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample\nspec:\n  containers:\n    - name: ubuntu\n      image: ubuntu\n      securityContext:\n        privileged: false # must be false as per the policy\n        runAsUser: 1000 # must be non-root user as per the policy\n  volumes:\n    - name: data-volume\n      persistentVolumeClaim: # must use persistentVolumeClaim as per the policy\n        claimName: my-pvc # replace with your actual PVC name\n</code></pre>"},{"location":"docs/kubernetes/pod-security-standard-and-admissions/","title":"Pod Security Standard and Admissions","text":""},{"location":"docs/kubernetes/pod-security-standard-and-admissions/#overview","title":"Overview","text":"<p>More Information</p> <p>Pod Security Policy (PSP) and Pod Security Admission (PSA) were introduced by the Kubernetes Auth Special Interest Group (SIG). It is mainly used to govern the security of the pods in the Kubernetes cluster.</p> <p>As Kubernetes evolves, the Pod Security Policy (PSP) is being deprecated and removed from Kubernetes v1.25. The Pod Security Standard (PSS) is a new way to enforce security policies (Pod) in Kubernetes clusters. It provides a set of  baseline and restricted security profiles that can be applied to pods.</p> <p>You can use any profiles in PSS with any modes in PSA.</p>"},{"location":"docs/kubernetes/pod-security-standard-and-admissions/#pod-security-standards-pss","title":"Pod Security Standards (PSS)","text":"<p>Reference</p> <p>https://kubernetes.io/docs/concepts/security/pod-security-standards/</p> <p>The Pod Security Standards (PSS) are a set of security profiles that define the security requirements for pods in Kubernetes. The PSS is divided into three profiles:</p> Profile Restrictiveness Description privileged Unrestricted policy This level allows all capabilities and does not restrict any security context settings. Allows for privilege escalations. Useful for system-wide programs like logging agents, storage drivers, etc that require privileged access. baseline Restricted policy This level restricts the use of certain capabilities (e.g., hostPath, hostNetwork, hostIPC, hostPID) and encourages the use of security context settings like <code>readOnlyRootFilesystem</code> and <code>runAsNonRoot</code>. It is designed to prevent known privilege escalations while allowing common workloads to function. Suitable for most workloads. restricted Highly restricted policy This level enforces strict security measures, such as disallowing privilege escalation, requiring <code>runAsNonRoot</code>, and restricting access to host namespaces and hostPath volumes. It follows best practices for hardening pods and is suitable for workloads requiring the highest level of security and isolation."},{"location":"docs/kubernetes/pod-security-standard-and-admissions/#pod-security-admission-psa","title":"Pod Security Admission (PSA)","text":"<p>Reference</p> <p>https://kubernetes.io/docs/concepts/security/pod-security-admission/</p> <p>Pod Security Admission (PSA) is a built-in admission controller that enforces the Pod Security Standards. It is enabled by default in Kubernetes v1.25 and later. To verify if Pod Security Admission is enabled, you can check the <code>kube-apiserver</code> command line arguments. The <code>--enable-admission-plugins</code> flag should include <code>PodSecurity</code>.</p> kube-apiserver.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver-kind-cluster-control-plane\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --advertise-address=10.89.0.2\n    - --allow-privileged=true\n    - --authorization-mode=Node,RBAC\n    - --client-ca-file=/etc/kubernetes/pki/ca.crt\n    - --enable-admission-plugins=NodeRestriction,PodSecurity\n    ...\n    image: registry.k8s.io/kube-apiserver:v1.30.0\n</code></pre> <p>The Pod Security Admission got three modes to enforce the controls defined by the Pod Security Standards (PSS). A mode defines what happens to the pods that do not meet the Pod Security Standards. The three modes are:</p> Mode Description On violation enforce This mode enforces the Pod Security Standards. Pods that do not meet the policy requirements will be rejected. Reject pod audit This mode audits the pods that do not meet the policy requirements. It will record the violations in the audit logs and the pod is allowed to run if it violates the policy. Record in the audit logs warn This mode warns the user about the pods that do not meet the policy requirements and the pod is allowed to run if it violates the policy. Display warning message"},{"location":"docs/kubernetes/pod-security-standard-and-admissions/#exemptions","title":"Exemptions","text":"<p>References</p> <ul> <li>https://kubernetes.io/docs/concepts/security/pod-security-admission/#exemptions</li> <li>https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-admission-controller/</li> </ul> <p>Here is the scenario, you setup security in your cluster, for some reasons, you want to have some flexibility in your cluster or some exceptions to the rule or security policies. In this case, you can use the exemptions feature in Pod Security Admission. Meaning that all modes (enforce, audit, and warn) will be skipped (ignore PSA) for the exempted pods.</p> <p>There are three key exemption dimensions:</p> Exemption Dimension Description Usernames Requests from authenticated (or impersonated) usernames are ignored by Pod Security Admission. RuntimeClassNames Requests for pods with a specific RuntimeClass are ignored by Pod Security Admission. Namespaces Requests for pods in a specific namespace are ignored by Pod Security Admission. <p>To configure the exemptions, you will need to configure the Admission Controller and pass the configuration to kube-apiserver using the <code>--admission-control-config-file</code> flag.</p>"},{"location":"docs/kubernetes/pod-security-standard-and-admissions/#step-1-configure-the-admission-controller-configuration","title":"Step 1: Configure the Admission Controller configuration","text":"TemplateExample /etc/kubernetes/admission-config.yaml<pre><code>apiVersion: apiserver.config.k8s.io/v1\nkind: AdmissionConfiguration\nplugins:\n- name: PodSecurity\n  configuration:\n    apiVersion: pod-security.admission.config.k8s.io/v1 # see compatibility note\n    kind: PodSecurityConfiguration\n    # Defaults applied when a mode label is not set.\n    #\n    # Level label values must be one of:\n    # - \"privileged\" (default)\n    # - \"baseline\"\n    # - \"restricted\"\n    #\n    # Version label values must be one of:\n    # - \"latest\" (default) \n    # - specific version like \"v1.32\"\n    defaults:\n      enforce: \"privileged\"\n      enforce-version: \"latest\"\n      audit: \"privileged\"\n      audit-version: \"latest\"\n      warn: \"privileged\"\n      warn-version: \"latest\"\n    exemptions:\n      # Array of authenticated usernames to exempt.\n      usernames: []\n      # Array of runtime class names to exempt.\n      runtimeClasses: []\n      # Array of namespaces to exempt.\n      namespaces: []\n</code></pre> /etc/kubernetes/admission-config.yaml<pre><code>apiVersion: apiserver.config.k8s.io/v1\nkind: AdmissionConfiguration\nplugins:\n- name: PodSecurity\n  configuration:\n    apiVersion: pod-security.admission.config.k8s.io/v1 \n    kind: PodSecurityConfiguration\n    defaults:\n      enforce: \"privileged\"\n      enforce-version: \"latest\"\n      audit: \"privileged\"\n      audit-version: \"latest\"\n      warn: \"privileged\"\n      warn-version: \"latest\"\n    exemptions:\n      # You can leave any fields empty\n      usernames: [admin-user, system:serviceaccount:kube-system:default]\n      runtimeClasses: [gvisor, kata-containers]\n      namespaces: [kube-system, monitoring]\n</code></pre> <ul> <li>you can exempt specific users, runtime classes, or namespaces.</li> <li>Here is an example of combined excemptions, but you can leave any fields empty based on your needs.</li> </ul>"},{"location":"docs/kubernetes/pod-security-standard-and-admissions/#step-2-pass-the-configuration-to-kube-apiserver","title":"Step 2: Pass the configuration to kube-apiserver","text":"<p>From the <code>kube-apiserver</code>, we need to pass the <code>--admission-control-config-file</code> flag to the <code>kube-apiserver</code> command line arguments. The <code>--admission-control-config-file</code> flag should point to the admission configuration file.</p> kube-apiserver.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver-kind-cluster-control-plane\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --advertise-address=10.89.0.2\n    - --allow-privileged=true\n    - --authorization-mode=Node,RBAC\n    - --client-ca-file=/etc/kubernetes/pki/ca.crt\n    - --enable-admission-plugins=NodeRestriction,PodSecurity\n    - --admission-control-config-file=/etc/kubernetes/admission-config.yaml\n    ...\n    image: registry.k8s.io/kube-apiserver:v1.30.0\n</code></pre>"},{"location":"docs/kubernetes/pod-security-standard-and-admissions/#steps-to-use-pss-and-psa","title":"Steps to use PSS and PSA","text":""},{"location":"docs/kubernetes/pod-security-standard-and-admissions/#step-1-label-the-namespace-with-respective-mode-and-profiles","title":"Step 1: Label the namespace with respective mode and profiles","text":"<p>What if?</p> <p>If no labels are set on a namespace, then it will use the default policy for the namespace. The default policy is privileged.</p> <p>Important</p> <p>Changing the namespace labels will affect existing pods in the namespace, but it will not delete them. As the changes will trigger the admission plugin to test existing pods against the new policy and return violations as warnings. For example, if the new policy is set to enforce, pods that violate the new policy will be rejected, but existing pods will not be terminated.</p> Bash<pre><code>kubectl label namespace &lt;ns&gt; pod-security.kubernetes.io/&lt;mode&gt;=&lt;profile&gt;\n\n# example\nkubectl label namespace prod pod-security.kubernetes.io/enforce=restricted\nkubectl label namespace dev pod-security.kubernetes.io/warn=baseline\n\n# apply multiple labels to the namespace\nkubectl label namespace staging pod-security.kubernetes.io/audit=baseline pod-security.kubernetes.io/warn=baseline\n</code></pre> <ul> <li>In prod, the restricted profile is enforced. Meaning all pods in the prod namespace must follow the restricted profile. If a pod does not meet the requirements, it will be rejected. Let's say u set the restricted profile with warn mode. The pod will be allowed to run, but a warning message will be displayed.</li> <li>In dev, the baseline profile is set with warn mode. Meaning all pods in the dev namespace must follow the baseline profile. If a pod does not meet the requirements, it will be allowed to run, but a warning message will be displayed.</li> <li>In staging, the baseline profile is set with audit and warn mode. Meaning all pods in the staging namespace must follow the baseline profile. If a pod does not meet the requirements, it will be allowed to run, but a warning message will be displayed and the violation will be recorded in the audit logs.</li> </ul>"},{"location":"docs/kubernetes/pod-security-standard-and-admissions/#step-2-create-a-pod-with-no-violations-in-the-namespace","title":"Step 2: Create a pod with no violations in the namespace","text":"<p>Here are the examples of creating a pod with no violations in the namespace based on the profiles.</p> PrivilegedBaselineRestricted <p>What is privileged?</p> <p>When you set <code>privileged: true</code>, it means that the container has full access to the host system. It can do anything that the host system can do.</p> privileged-pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: privileged-namespace\nspec:\n  containers:\n    - name: privileged-container\n      image: nginx:latest\n      securityContext:\n        privileged: true\n        allowPrivilegeEscalation: true\n        capabilities:\n          add: [\"ALL\"]\n</code></pre> <p>Appropriate for workloads that require privileged access to the host system, such as system daemons or monitoring agents.</p> baseline-pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: baseline-pod\n  namespace: baseline-namespace\nspec:\n  containers:\n    - name: baseline-container\n      image: nginx:latest\n      securityContext:\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        runAsNonRoot: true\n</code></pre> <p>Appropriate for most workloads that do not require privileged access to the host system, such as web servers, application servers, or databases.</p> restricted-pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: restricted-pod\n  namespace: restricted-namespace\nspec:\n  containers:\n    - name: restricted-container\n      image: nginx:latest\n      securityContext:\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        runAsNonRoot: true\n        capabilities:\n          drop: [\"ALL\"]\n</code></pre> <p>Appropriate for workloads that require the highest level of security and isolation, such as sensitive applications or multi-tenant environments. For example, financial applications, healthcare applications, or government applications.</p>"},{"location":"docs/kubernetes/pod/","title":"Pod","text":""},{"location":"docs/kubernetes/pod/#what-is-pod","title":"What is Pod?","text":"<pre><code>flowchart\n  subgraph pod[Pod]\n    container\n  end</code></pre> <p>Pod is a single instance of an application and is the smallest unit of a Kubernetes application. It basically wraps container or provides an abstraction layer over container. Normally, pod is only run one application container inside it (one-to-one relationship).</p> <p>Each pod will get its IP address and when the pod is recreated, then the new IP address will be assigned to that pod. So, the pod can use its IP address to communicate with each other pods using its internal IP address.</p> <p>The containers inside the pod can communicate with each other using <code>localhost</code> as they are sharing the same network space. They can also share the same storage space, which is called volume.</p>"},{"location":"docs/kubernetes/pod/#pods-statuses","title":"Pods statuses","text":"<ul> <li>CrashLoopBackOff<ul> <li>The pod is in a crash loop, meaning the container is crashing repeatedly.</li> <li>The pod is trying to restart the container, but it keeps failing when it starts.</li> </ul> </li> </ul>"},{"location":"docs/kubernetes/pod/#commands","title":"Commands","text":"Bash<pre><code># Deploy nginx image to pod with the pod-name \"nginx\"\nkubectl run &lt;pod-name&gt; --image=&lt;image&gt;\n\n# Example\nkubectl run nginx --image=nginx\nkubectl run nginx --image=nginx --port=8000 # container exposes port\nkubectl run nginx --image=nginx --port=8080 --expose # Create a cluster IP associated with the pod port number\n\nkubectl get pods\nkubectl get pod &lt;pod-name&gt;\nkubectl describe pod &lt;pod-name&gt;\nkubectl delete pod &lt;pod-name&gt;\n\nkubectl exec &lt;pod-name&gt; -- &lt;command&gt;\nkubectl exec mypod -- /bin/bash\n\nkubectl apply -f &lt;filename&gt;\nkubectl create -f &lt;filename&gt;\n\nkubectl edit pod &lt;pod-name&gt;\n\n# extract pod definition to a file\nkubectl get pod &lt;pod-name&gt; -o yaml &gt; pod.yaml\n\n# get more pod information like IP address, etc\nkubectl get pods -o wide\n</code></pre>"},{"location":"docs/kubernetes/pod/#pod-yaml-file","title":"Pod YAML File","text":"<p>Here is the sample pod YAML file, to apply this file, you need to type <code>kubectl apply -f pod.yaml</code>.</p> Kind Version Pod v1 Service v1 ReplicaSet apps/v1 Deployment apps/v1 pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp\n  labels:\n    name: myapp\nspec:\n  containers:\n    - name: myapp\n      image: nginx\n      ports:\n        - containerPort: 8080\n</code></pre>"},{"location":"docs/kubernetes/pod/#restart-policy","title":"Restart Policy","text":"<p>Restart policy is used to define the behavior of the pod when the container crashes. There are three types of restart policy:</p> <ul> <li>Always (default): The container will always restart when it crashes or any termination.</li> <li>OnFailure: The container will restart only when it crashes (non-zero exit status).</li> <li>Never: The container will never restart when it crashes or any termination.</li> </ul> pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp\nspec:\n  containers:\n    - name: ubuntu\n      image: ubuntu\n  restartPolicy: Never\n</code></pre> <ul> <li>In this case, the pod will never restart when the container job is done.</li> </ul>"},{"location":"docs/kubernetes/pod/#init-containers","title":"Init Containers","text":"<p>Init container is a special type of container that run before the main container. It will be run only once and must complete successfully before the main container starts.</p> <p>Use case:</p> <ul> <li>Wait for a database to be ready</li> <li>Prepare a configuration file</li> <li>Load data</li> </ul> init-container.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: main-container\n      image: api:1.0.0\n  initContainers:\n    - name: init-database\n      image: mysql\n      command: ['sh', '-c', 'mysql -h db -u root -p $MYSQL_ROOT_PASSWORD &lt; /scripts/init.sql']\n</code></pre> <ul> <li>If you specify multiple init containers, they will be run in order.</li> </ul>"},{"location":"docs/kubernetes/pod/#multi-container-pods","title":"Multi-container pods","text":"<pre><code>flowchart\n  subgraph pod[Pod]\n    container1\n    container2\n  end</code></pre> <p>Multi-container pods are like helper containers. The two or more containers can communicate directly via localhost as they share the same network space. Of course, they are sharing the same storage space and same lifecycle (created and destroyed together) as well.</p>"},{"location":"docs/kubernetes/pod/#design-patterns","title":"Design Patterns","text":""},{"location":"docs/kubernetes/pod/#sidecar-pattern","title":"Sidecar Pattern","text":"<p>Sidecar pattern is a container attached to the main container to extend or enhance the main container. For example, a sidecar container can collect logs from the main container, then can forward them to a centralized logging system like Elasticsearch.</p> <p>Use case:</p> <ul> <li>Logging</li> <li>Monitoring</li> <li>Security</li> <li>Data synchronization</li> </ul> <pre><code>flowchart LR\n  subgraph pod[Pod]\n    c1[Main container]\n    c2[Sidecar container]\n  end\n\n  c1 --&gt; storage\n  storage[Storage] --&gt; c2\n  c2 --&gt; out[Elasticsearch]</code></pre> sidecar.yml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: app-container\n      image: alpine\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"while true; do date &gt;&gt; /var/log/index.html; sleep 2;done\"]\n      volumeMounts:\n        - name: logs\n          mountPath: /var/log\n    - name: log-agent\n      image: nginx\n      ports:\n        - containerPort: 80\n      volumeMounts:\n        - name: logs\n          mountPath: /usr/share/nginx/html\n  volumes:\n    - name: logs\n      emptyDir: {}\n</code></pre> <ul> <li>In this case, when you port-forward to the port <code>80</code>, you will see the logs from the <code>app-container</code>.</li> </ul>"},{"location":"docs/kubernetes/pod/#adapter-pattern","title":"Adapter Pattern","text":"<p>Adapter pattern is a container that acts as a translator between the main containers and the external systems. Basically, you can use this pattern to manipulate and transform the data into a standardize format before sending it to the external system.</p> <p>For example, you have multiple applications that generated logs in different formats, then you can use the adapter pattern to transform the logs into a standardize format before sending them to the centralized logging system, so that the centralized logging system can easily parse and analyze the logs.</p> <p>Use case:</p> <ul> <li>Data transformation</li> <li>Protocol translation</li> <li>API adaptation (Reverse Proxy)</li> <li>Standardize data format</li> </ul> <pre><code>flowchart LR\n  subgraph pod[Pod]\n    c1[Main container]\n    c2[Adapter container - transform data]\n  end\n\n  c1 --&gt; storage\n  storage[Storage] --extract--&gt; c2\n  c2 --load--&gt; out[Elasticsearch]</code></pre> adapter.yml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: app-container\n      image: alpine\n      command: [\"/bin/sh\"]\n      args: [\"-c\", \"while true; do date &gt;&gt; /var/log/debug.log; sleep 2;done\"]\n      volumeMounts:\n        - name: logs\n          mountPath: /var/log\n    - name: log-adapter\n      args: [\"/bin/sh\"]\n      args: [\"-c\", \"tail -f /var/log/debug.log|sed 's/^/DEBUG: /' &gt; /var/log/transformed.log\"]\n      volumeMounts:\n        - name: logs\n          mountPath: /var/log\n  volumes:\n    - name: logs\n      emptyDir: /var/log\n</code></pre>"},{"location":"docs/kubernetes/pod/#ambassador-pattern","title":"Ambassador Pattern","text":"<p>Ambassador Pattern is a container that acts as a proxy to the external services. It is used to hide the complexity of the external services from the main container. Therefore, this ambassador container will act as a client proxy (manage all incoming and outgoing network traffic) between the main container and the external services. </p> <p>For example, your application need to connect to external database, now the application need to connect to different database based on the environment (development, staging, production) and you also need to make sure that the application connect to the correct database based on the environment, in this case, you can use the ambassador pattern to manage the connection to the database, so that the application only need to connect to the ambassador container as <code>localhost</code> and the ambassador container will manage the connection to the correct database based on the environment.</p> <pre><code>flowchart LR\n  subgraph pod[Pod]\n    c1[Main container]\n    c2[Proxy container]\n  end\n\n  c1 --&gt; c2\n  c2 --&gt; db\n  db[DB]</code></pre> ambassador.yml<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: db-config\ndata:\n  development: \"dev-db.example.com\"\n  staging: \"staging-db.example.com\"\n  production: \"prod-db.example.com\"\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\n  labels:\n    app: ambassador\n    environment: development # Change this based on the environment\nspec:\n  containers:\n    - name: backend\n      image: backend\n      env:\n        - name: DB_HOST\n          value: \"localhost\"\n      ports:\n        - containerPort: 8080\n    - name: ambassador\n      image: ambassador-image\n      env:\n        - name: ENVIRONMENT\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.labels.environment\n        - name: DB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: db-config\n              key: $(ENVIRONMENT)\n      ports:\n        - containerPort: 80\n</code></pre> <ul> <li><code>ConfigMap</code> is used to store the database connection based on the environment.</li> <li><code>ambassador</code> container reads the env variable and configures itself to connect to the correct database based on the environment.</li> <li><code>backend</code> container connects to the <code>ambassador</code> container as <code>localhost</code>.</li> </ul>"},{"location":"docs/kubernetes/port-forward/","title":"Port Forward","text":""},{"location":"docs/kubernetes/port-forward/#concept-and-usage-of-port-forward","title":"Concept and Usage of Port Forward","text":"<pre><code>graph LR\n  subgraph my-laptop[My Laptop]\n    direction LR\n    kubectl --&gt; port-forward[port-forward 8080]\n  end\n\n  port-forward --&gt; resource-type-port\n\n  subgraph cluster\n    resource-type-port\n  end</code></pre> <p>Port forwarding is a technique that allows you to access a service running in a Kubernetes cluster from your local machine. This is useful for debugging purposes or to access a service that is not exposed to the public internet.</p> Bash<pre><code>kubectl port-forward &lt;resource-type&gt;/&lt;name&gt; &lt;local-port&gt;:&lt;resource-type-name-port&gt;\n\n# examples\nkubectl port-forward pod/my-pod 8080:80\nkubectl port-forward deployment/my-deployment 8080:80\nkubectl port-forward service/my-service 8080:80\n</code></pre> <p>In this case, we specify our local port as <code>8080</code> and the port of the resource we want to access as <code>80</code>. This means that we can access the service running in the Kubernetes cluster on port <code>80</code> by visiting <code>http://localhost:8080</code>.</p>"},{"location":"docs/kubernetes/pull-image-with-secret/","title":"Pull image with secret","text":"<p>When you want to pull an image from a private registry, you need to provide the credentials to authenticate with the registry. Therefore, you need to create a secret that contains the credentials and then use that secret to pull the image.</p>"},{"location":"docs/kubernetes/pull-image-with-secret/#step-1-create-registry-secret","title":"Step 1: Create registry secret","text":"<p>You can use either one to create registry secret:</p> <ul> <li>Imperative way to create registry secret</li> <li>Declarative way to create registry secret</li> </ul>"},{"location":"docs/kubernetes/pull-image-with-secret/#step-2-apply-secret-to-pod","title":"Step 2: Apply secret to pod","text":"pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  imagePullSecrets:\n    - name: my-secret # secret name\n  containers:\n    - name: my-container\n      image: my-image\n</code></pre>"},{"location":"docs/kubernetes/readiness-probes/","title":"Readiness Probes","text":""},{"location":"docs/kubernetes/readiness-probes/#pod-setup-process-status-and-conditions","title":"Pod setup process, status, and conditions","text":"<p>Before we straight away understand how to use Readiness Probes. Let's understand the Pod setup process, status, and conditions.</p> <ol> <li>The scheduler will determine which node to place the pod.</li> <li>Once the pod is scheduled on the right node, it will pull the images, convert it into a container, and the container will start after done pulling the image.</li> <li>Then, the pod will go into a running state by making sure all the containers are ready (ready conditions).</li> </ol> <pre><code>flowchart LR\n  status[Pod Scheduled] --&gt; status2\n  status2[Pull, convert, and start the container] --&gt; status3\n  status3[Containers are ready] --&gt; status4\n  status4[Running status or ready condition]</code></pre> <p>The ready condition actually indicates that the application running inside the pod is now ready to accept the user traffic, because by default, Kubernetes assumes when the container is created, it is ready to accept the user traffic, therefore it will set the ready condition to true for all the containers.</p>"},{"location":"docs/kubernetes/readiness-probes/#usage-of-readiness-probes","title":"Usage of Readiness Probes","text":"<p>Reference</p> <p>The concept of Readiness probes basically is to determine whether a container is ready to accept the traffic by performing some health checks. If the readiness probes are successful, then we know that container is going to run and accept the traffic, else, it (kubelet) will remove the pod and try again.</p> <pre><code>flowchart LR\n  A[Container] --&gt; B{Check container is ready to accept the traffic?}\n  B --&gt;|Yes| C[Healthy]\n  B --&gt;|No| D[Unhealthy]\n  D --&gt; E[Remove the pod]\n  E --&gt; B</code></pre> <p>Examples of Readiness Probes usage;</p> <ul> <li>Test the web application is online</li> <li>Check whether the Database TCP connection is online or ready</li> <li>Run the exec command to run a custom script that will determine the application is ready</li> </ul> <p>HTTP readiness-probes.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: web-app\n      image: webapp\n      readinessProbe:\n        httpGet:\n          path: /api/v1/health\n          port: 8000\n        initialDelaySeconds: 5 # Number of seconds after the container has started before startup, that means it should wait for 5 seconds before performing the 1st probe\n        periodSeconds: 3 # how often to perform the probe\n        failureThreshold: 5 # default 3, if the application is not ready after 3 attempts, then the probe will stop\n</code></pre></p> <p>TCP readiness-probes.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: web-app\n      image: webapp\n      readinessProbe:\n        tcpSocket:\n          port: 8080\n        initialDelaySeconds: 5 # Number of seconds after the container has started before startup, that means it should wait for 5 seconds before performing the 1st probe\n        periodSeconds: 3 # how often to perform the probe\n        failureThreshold: 5 # default 3, if the application is not ready after 3 attempts, then the probe will stop\n</code></pre></p> <p>kubelet will try to open a socket to the container on the specified port <code>8080</code>. If it can establish the connection, then the state is healthy.</p> <p>exec readiness-probes.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: web-app\n      image: webapp\n      readinessProbe:\n        exec:\n          command:\n            - cat\n            - /app/healthy\n        initialDelaySeconds: 5 # Number of seconds after the container has started before startup, that means it should wait for 5 seconds before performing the 1st probe\n        periodSeconds: 3 # how often to perform the probe\n        failureThreshold: 5 # default 3, if the application is not ready after 3 attempts, then the probe will stop\n</code></pre></p> <p>It will return 0 status code if the command succeed.</p>"},{"location":"docs/kubernetes/replica-set/","title":"ReplicaSet","text":""},{"location":"docs/kubernetes/replica-set/#what-is-replicaset","title":"What is ReplicaSet ?","text":"<p>Info</p> <p>Replication Controller and ReplicaSet are the same, but ReplicaSet is the next generation of Replication Controller.</p> <pre><code>---\ntitle: ReplicaSet\n---\nflowchart\n  user --&gt; Node\n\n  subgraph \"Node\"\n    subgraph \"ReplicaSet\"\n      pod1[Pod]\n      pod2[Pod]\n    end\n  end</code></pre> <p>Replicaset will ensure a specified number of pod replicas are running. It will maintain the desired state of the application by creating or deleting pods as needed to match the defined number of replicas. With this setup, it will ensure high availability and fault tolerance for applications.</p> <p>Key features:</p> <ul> <li>Self-healing: If a pod fails, it will create a new pod to replace it. Therefore, it will ensure the specified number of pods are running at all times.</li> <li>Scaling: It can scale up or down the number of pods as needed.</li> <li>Load Balancing: It can distribute the traffic (share the load) to the pods.</li> </ul>"},{"location":"docs/kubernetes/replica-set/#replication-controller","title":"Replication Controller","text":"rc.yaml<pre><code>apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: rc\nspec:\n  replicas: 3\n  template:\n    metadata:\n      name: pod-name\n    spec:\n      containers:\n        - name: container-name\n          image: web-server-image\n</code></pre> Bash<pre><code>kubectl get replicationcontroller\n</code></pre>"},{"location":"docs/kubernetes/replica-set/#replicaset_1","title":"ReplicaSet","text":"<p>Info</p> <p>When you perform the editing to the replicaset, you will need to delete the previous deployed pods first, as it won't auto update the pods.</p> <p>The differences between Replication Controller and ReplicaSet is ReplicaSet needs a selector definition. The selector will be used to identify the pods that ReplicaSet will manage. Furthermore, ReplicaSet can also manage pods that were not created as part of the ReplicaSet creation, as it uses label selectors to identify and manage pods. So if you want to create a new pod, you have to specify template section.</p> rc.yaml<pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: replicaset-name\nspec:\n  replicas: 3\n  selector:\n    matchLabels: # matches the pod labels specified in the template section\n      app: web-server\n  template:\n    metadata:\n      name: pod-name\n      labels:\n        app: web-server\n    spec:\n      containers:\n        - name: container-name\n          image: web-server-image\n</code></pre> Bash<pre><code>kubectl get replicaset\nkubectl scale --replicas=5 -f &lt;file-name&gt;\nkubectl scale --replicas=5 replicaset &lt;replicaset-name&gt;\nkubectl delete replicaset &lt;replicaset-name&gt;\n</code></pre>"},{"location":"docs/kubernetes/resource-requirements-and-limits/","title":"Resource Requirements and Limits","text":""},{"location":"docs/kubernetes/resource-requirements-and-limits/#concept-of-resource-requirements-and-limits","title":"Concept of resource requirements and limits","text":"<p>Every node has its own capacity of resources like CPU and memory. When you deploy a pod to a node, the pod is consuming the resources of the node. Now, we know that kube-scheduler is responsible for scheduling the pods to the nodes. Therefore, kube-scheduler will identify the node with the required resources to deploy the pod, if the node does not have the required resources, the pod will not be scheduled (pending state) to that node.</p> Bash<pre><code># See the events of the pod --&gt; it will show insufficient resources\nkubectl describe pods &lt;pod-name&gt;\n</code></pre>"},{"location":"docs/kubernetes/resource-requirements-and-limits/#resource-requests","title":"Resource requests","text":"<p>The resource requests of a container are the minimum amount of CPU or memory requested by the container. That means, the pod will guarantee get the amount of resources that requested. By default, every pod has a default request of 0.5 CPU and 256MB of memory.</p> <p>For the CPU request, you can specify the value in milli (m) or CPU. For example, 0.1 CPU = 100m (milli). Now remember, 1 CPU = 1 vCPU (AWS) = 1 Core (GCP, Azure) = 1 Hyperthread</p> <p>For the memory request, you can specify the value in bytes or binary (Ki, Mi, Gi, Ti).</p> Binary Bytes 1G (Gigabyte) 1,000,000,000 1M (Megabyte) 1,000,000 1K (Kilobyte) 1,000 1Gi (Gibibyte) 1,073,741,824 1Mi (Mebibyte) 1,048,576 1Ki (Kibibyte) 1,024 pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: sample-container\n      image: ubuntu\n      resources:\n        requests:\n          memory: \"2Gi\"\n          cpu: 2\n</code></pre>"},{"location":"docs/kubernetes/resource-requirements-and-limits/#resource-limits","title":"Resource limits","text":"<p>Resource limits of a container are the maximum amount of CPU or memory that the container can use. That means, the pod will not be able to consume more resources than the specified limit. If you do not specify the limit, the pod will be able to consume as much as it wants. By default, every pod has a default limit of 1 CPU and 512MB of memory. </p> <p>If the pod tries to consume more resources than the specified limit, what will happen?</p> <ul> <li>For the CPU, the system will throttle (control) the CPU usage of the container so that it does not go beyond the specified limit.</li> <li>For the memory, the system will kill the container if it consumes more memory (OOM - Out of memory error) than the specified limit.</li> </ul> pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n    - name: sample-container\n      image: ubuntu\n      resources:\n        requests:\n          memory: \"2Gi\"\n          cpu: 2\n        limits:\n          memory: \"4Gi\"\n          cpu: 4\n</code></pre>"},{"location":"docs/kubernetes/resource-requirements-and-limits/#resource-quota","title":"Resource quota","text":"<p>Refer to Resource quota for namespace for more information.</p>"},{"location":"docs/kubernetes/resource-requirements-and-limits/#limit-range","title":"Limit range","text":"<p>Refer to Limit range for namespace for more information.</p>"},{"location":"docs/kubernetes/scan-images-using-admission-controller/","title":"Scan images using admission controller","text":"<p>We can use Trivy as an admission controller to scan images before they are deployed in a Kubernetes cluster. This helps ensure that only images that meet your security policies are allowed to run in your environment.</p> <p>With this approach, it might delay the deployment process because the admission controller will scan the image everytime before it is deployed and if the image is not compliant, it will block the deployment. So, the alternative way is to have your own internal registry with all pre-scanned images and use that registry in your deployment process. This way, the admission controller will only scan the images that are not in your internal registry.</p>"},{"location":"docs/kubernetes/scan-images-using-admission-controller/#steps-to-scan-images-using-trivy-with-an-admission-controller","title":"Steps to scan images using Trivy with an admission controller","text":""},{"location":"docs/kubernetes/scan-images-using-admission-controller/#step-1-create-tls-certificates","title":"Step 1: Create TLS Certificates","text":"openssl.cnf<pre><code>[req]\ndistinguished_name = req_distinguished_name\nreq_extensions = v3_req\nprompt = no\n\n[req_distinguished_name]\nCN = trivy-service.default.svc\n\n[v3_req]\nkeyUsage = keyEncipherment, dataEncipherment\nextendedKeyUsage = serverAuth\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = trivy-service.default.svc\nDNS.2 = trivy-service\nDNS.3 = trivy-service.default\nDNS.4 = localhost\nDNS.5 = trivy-service.default.svc.cluster.local\n</code></pre> Bash<pre><code># Step 1: Generate a Certificate Authority (CA)\n# Generate the CA private key\nopenssl genrsa -out ca.key 2048\n\n# Generate the CA certificate\nopenssl req -x509 -new -nodes -key ca.key -sha256 -days 365 -out ca.crt -subj \"/CN=trivy-ca\"\n\n# Step 2: Generate the Server Key and Certificate Signing Request (CSR)\n# Generate the server private key\nopenssl genrsa -out server-key.pem 2048\n\n# Generate the CSR using the openssl.cnf file\nopenssl req -new -key server-key.pem -out server.csr -config openssl.cnf\n\n# Step 3: Sign the Server Certificate with the CA\n# Sign the server certificate\nopenssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server-cert.pem -days 365 -extensions v3_req -extfile openssl.cnf\n\n# Step 4: Base64 Encode the CA Certificate for the caBundle\n# Base64 encode the CA certificate\ncat ca.crt | base64 -w 0 # copy and replace &lt;base64-encoded-CA-cert&gt; in your validating-webhook.yaml file.\n</code></pre>"},{"location":"docs/kubernetes/scan-images-using-admission-controller/#step-2-create-a-webhook-service","title":"Step 2: Create a Webhook Service","text":"<p>Create a simple Flask application that will act as a webhook server. This server will receive admission review requests from the Kubernetes API server and respond with whether the image is allowed or not.</p> trivy-webhook.py<pre><code>import subprocess\nfrom flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n@app.route('/validate', methods=['POST'])\ndef validate():\n  admission_review = request.get_json()\n  pod_spec = admission_review['request']['object']['spec']['containers']\n  for container in pod_spec:\n    image = container['image']\n    # Scan the image using Trivy\n    result = subprocess.run(\n      [\"trivy\", \"image\", \"--quiet\", \"--severity\", \"CRITICAL\", image],\n      stdout=subprocess.PIPE,\n      stderr=subprocess.PIPE\n    )\n    if result.returncode != 0:\n      return jsonify({\n        \"response\": {\n          \"allowed\": False,\n          \"status\": {\n            \"message\": f\"Image {image} has critical vulnerabilities.\"\n          }\n        }\n      })\n  return jsonify({\"response\": {\"allowed\": True}})\n\nif __name__ == '__main__':\n  context = ('/etc/webhook/certs/server-cert.pem', '/etc/webhook/certs/server-key.pem')\n  app.run(host='0.0.0.0', port=443, ssl_context=context)\n</code></pre> <p>Then, create a <code>Dockerfile</code> for the webhook server.</p> Dockerfile<pre><code># Use Python as the base image\nFROM python:3.9-slim\n\n# Install Trivy\nRUN apt-get update &amp;&amp; apt-get install -y wget \\\n    &amp;&amp; wget -qO- https://github.com/aquasecurity/trivy/releases/latest/download/trivy_0.45.0_Linux-64bit.tar.gz | tar zxv \\\n    &amp;&amp; mv trivy /usr/local/bin/ \\\n    &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the webhook service code\nCOPY webhook.py /app/webhook.py\n\n# Install Python dependencies\nRUN pip install flask\n\n# Expose the webhook service port\nEXPOSE 443\n\n# Run the webhook service\nCMD [\"python\", \"webhook.py\"]\n</code></pre> <p>Build the Docker image.</p> Bash<pre><code>docker build -t trivy-webhook .\n</code></pre>"},{"location":"docs/kubernetes/scan-images-using-admission-controller/#step-3-deploy-the-webhook-service","title":"Step 3: Deploy the Webhook Service","text":"deployment.yamlservice.yaml deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: trivy-webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: trivy-webhook\n  template:\n    metadata:\n      labels:\n        app: trivy-webhook\n    spec:\n      containers:\n        - name: trivy-webhook\n          image: trivy-webhook:latest\n          ports:\n            - containerPort: 443\n          volumeMounts:\n            - name: webhook-certs\n              mountPath: /etc/webhook/certs\n      volumes:\n        - name: webhook-certs\n          secret:\n            secretName: trivy-webhook-certs\n</code></pre> service.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: trivy-service\nspec:\n  ports:\n    - port: 443\n      targetPort: 443\n  selector:\n    app: trivy-webhook\n</code></pre> Bash<pre><code>kubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n</code></pre>"},{"location":"docs/kubernetes/scan-images-using-admission-controller/#step-4-create-validating-webhook-configuration","title":"Step 4: Create validating webhook configuration","text":"validating-webhook.yaml<pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  name: trivy-image-scan-webhook\nwebhooks:\n  - name: trivy-scan.example.com\n    clientConfig:\n      service:\n        name: trivy-service\n        namespace: default\n        path: /validate\n      caBundle: &lt;base64-encoded-CA-cert&gt;\n    rules:\n      - operations: [\"CREATE\"]\n        apiGroups: [\"\"]\n        apiVersions: [\"v1\"]\n        resources: [\"pods\"]\n    admissionReviewVersions: [\"v1\"]\n    sideEffects: None\n</code></pre> Bash<pre><code>kubectl apply -f validating-webhook.yaml\n</code></pre>"},{"location":"docs/kubernetes/scan-images-using-admission-controller/#step-5-test-the-webhook","title":"Step 5: Test the Webhook","text":"sample-pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\nspec:\n  containers:\n    - name: test-container\n      image: alpine:3.10\n</code></pre> Bash<pre><code>kubectl apply -f sample-pod.yaml\n</code></pre> <p>If the webhook is working correctly, the pod creation should be blocked, and you should see an error message similar to:</p> Text Only<pre><code>Error from server (Image alpine:3.10 has critical vulnerabilities.): admission webhook \"trivy-scan.example.com\" denied the request\n</code></pre> <p> </p>"},{"location":"docs/kubernetes/security-context/","title":"Security Context","text":"<p>Security context defines privilege and access control settings for a Pod or Container. So, we can choose to configure the security context at the Pod level or at the Container level.</p> security-context.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  securityContext:\n    runAsUser: 1000 # Run as user with UID 1000\n  containers:\n    - name: ubuntu\n      image: ubuntu\n      command: [\"sleep\", \"3600\"]\n      securityContext:\n        runAsUser: 2000 # This will override the Pod level security context\n        capabilities:\n          add: [\"NET_ADMIN\", \"SYS_TIME\", \"MAC_ADMIN\"]\n</code></pre> <ul> <li>Remember, the security context defined at the Container level will override the Pod level security context.</li> <li><code>capabilities</code> field is used to add or drop capabilities for a container, it is only supported at the container level, not at the pod level.<ul> <li>capabilities are a fine-grained way to control the privileges of processes. By adding specific capabilities, you can grant a container additional privileges without giving it full root access.</li> </ul> </li> <li>For <code>runAsUser</code>, you need to tied it with the user in the Dockerfile when you create a username with uid in the Dockerfile. For example, you have a Dockerfile like this, then in the security context you can use <code>runAsUser: 1000</code> to run the container with the user <code>myuser</code> with, so the container will refer to the user <code>myuser</code> with uid <code>1000</code> in the Dockerfile.</li> </ul> Dockerfile<pre><code>FROM ubuntu:latest\nRUN useradd -u 1000 -m myuser\nUSER myuser\nCMD [\"bash\"]\n</code></pre>"},{"location":"docs/kubernetes/service-account/","title":"Service Account","text":""},{"location":"docs/kubernetes/service-account/#concept-and-usage-of-service-account","title":"Concept and Usage of Service Account","text":"<p>Info</p> <p>We can create a new user using service account mechanism in Kubernetes.</p> <p>Service account is a non-human account that is used by processes/application running inside a pod to interact with the Kubernetes API server. It provides an identity for processes that run in a Pod. When you create a Pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace.</p> Description ServiceAccount User or group Location Kubernetes API (ServiceAccount object) External Access control Kubernetes RBAC or other authorization mechanisms Kubernetes RBAC or other identity and access management mechanisms Intended use Workloads, automation, Third-party application People <pre><code>flowchart LR\n  application --service-account--&gt; kube-apiserver</code></pre> <p>For example, assuming you have an application that needs to pull data like pods, services, etc by making API calls to the Kubernetes API server and display it on the UI. When the application needs to make API calls to the Kubernetes API server, it needs to authenticate itself. This is where service accounts come into the picture.</p> <p>Additional info:</p> <ul> <li>Every namespace will auto create a default service account.</li> <li> <p>By default, when you create a pod, it will use the default service account of the namespace and mount the service account token as a volume in the pod at the path <code>/var/run/secrets/kubernetes.io/serviceaccount</code>.</p> <ul> <li>When you <code>kubectl exec -it &lt;pod-name&gt; -- ls /var/run/secrets/kubernetes.io/serviceaccount</code> you will see the token, ca.crt, and namespace files.</li> <li>If you don't want the Kubernetes auto mount the service account token, you can set <code>automountServiceAccountToken</code> to <code>false</code> in the pod spec.   pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: ubuntu\n      image: ubuntu\n  automountServiceAccountToken: false\n</code></pre></li> </ul> </li> <li> <p>The default service account has limited permissions and it only use to run basic Kubernetes API queries.</p> </li> <li>When you want to change or edit the existing pod service account, you must delete the pod and recreate it, but for deployment case, you can just edit the deployment, as it will auto trigger a new rollout.</li> </ul>"},{"location":"docs/kubernetes/service-account/#step-1-create-service-account","title":"Step 1: Create Service Account","text":"<ol> <li> <p>Imperative way Bash<pre><code>kubectl create serviceaccount &lt;name-of-service-account&gt;\nkubectl create serviceaccount data-sa\n</code></pre></p> </li> <li> <p>Declarative way data-sa.yaml<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: data-sa\n</code></pre></p> </li> <li> <p>Get the service account list   Bash<pre><code>kubectl get serviceaccounts\n</code></pre></p> </li> </ol>"},{"location":"docs/kubernetes/service-account/#step-2-generate-token-for-service-account","title":"Step 2: Generate Token for Service Account","text":"<p>This token is used by the application to authenticate itself with the Kubernetes API server. Now, when you create token, it is important to set the time to live (TTL) for the token. By default, the token will be valid for 1 hour.</p> Bash<pre><code>kubectl create token &lt;name-of-service-account&gt;\nkubectl create token data-sa\nkubectl create token data-sa --duration=30m\n</code></pre>"},{"location":"docs/kubernetes/service-account/#step-3-use-service-account-in-pod-or-make-api-calls","title":"Step 3: Use Service Account in Pod or make API calls","text":"<p>You can use <code>curl</code> and provide the token as a bearer token to make API calls. Besides, you can also mount the service account token as a volume in the pod if you want the make API calls from the pod.</p> Bash<pre><code>curl https://&lt;kube-apiserver&gt;:6443/api --header \"Authorization: Bearer &lt;token&gt;\"\n</code></pre> pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: ubuntu\n      image: ubuntu\n  serviceAccountName: data-sa\n</code></pre> <ul> <li>Let's say you want to make API calls from the pod, you need to provide the token in the header, but when you setup <code>serviceAccountName</code> in the pod spec, you don't need to provide the token in the header anymore as it will auto mount the token as a volume in the pod, which can auto authenticate the pod to the kube-apiserver.</li> </ul>"},{"location":"docs/kubernetes/service-account/#manually-creating-a-long-lived-api-token-for-a-service-account","title":"Manually creating a long-lived API token for a service account","text":"<p>Reference</p> <p>If you want to create a long-lived (non-expiring) token for a service account, you have to create a secret and associate it with the service account.</p>"},{"location":"docs/kubernetes/service-account/#step-1-create-a-service-account","title":"Step 1: Create a service account","text":"data-sa.yaml<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: data-sa\n</code></pre> <ul> <li>This method will not populate the <code>secrets</code> field in the service account object, as the <code>secrets</code> field is only populated with auto-generated secrets.</li> </ul> data-sa.yaml<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: data-sa\nsecrets:\n  - name: data-sa-secret\n</code></pre> <ul> <li>This method will populate the <code>secrets</code> field in the service account object.</li> </ul>"},{"location":"docs/kubernetes/service-account/#step-2-associate-the-secret-with-the-service-account","title":"Step 2: Associate the secret with the service account","text":"data-sa-secret.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: data-sa-secret\n  annotations:\n    kubernetes.io/service-account.name: data-sa # service account name\ntype: kubernetes.io/service-account-token\n</code></pre>"},{"location":"docs/kubernetes/service-account/#step-3-view-the-secret","title":"Step 3: View the secret","text":"Bash<pre><code>kubectl get secret/data-sa-secret -o jsonpath='{.data.token}' | base64 -d\n</code></pre>"},{"location":"docs/kubernetes/services/","title":"Services","text":""},{"location":"docs/kubernetes/services/#services-introduction","title":"Services Introduction","text":"<p>Service is used to connect applications together by exposing a network application (Pod) in the cluster.</p> <p>The internal Pod network is in the range of 10.244.0.0.</p> <ul> <li>You can use <code>kubectl describe pods/&lt;pod-name&gt;</code> to get the pod IP address.</li> </ul> <p>Kubernetes has default services types:</p> <ul> <li>ClusterIP<ul> <li>This service type will create a virtual IP address from a pool of your cluster IP addresses has reserved and assign it to a pod for enabling the communication between different services within a Kubernetes cluster.</li> </ul> </li> <li>NodePort<ul> <li>This service type will expose an application to external clients by opening an internal port on the worker nodes in Kubernetes cluster.</li> </ul> </li> <li>LoadBalancer<ul> <li>This service type can support external load balancers and it will provision a load balancer for your application services, mainly to distribute the network traffic/load across multiple instances of an application in Kubernetes cluster.</li> </ul> </li> </ul>"},{"location":"docs/kubernetes/services/#services-types","title":"Services Types","text":""},{"location":"docs/kubernetes/services/#clusterip-internally","title":"ClusterIP (Internally)","text":"<pre><code>flowchart\n    pod1[Pod1 10.244.0.1]\n    pod2[Pod2 10.244.0.2]\n    pod3[Pod3 10.244.0.3]</code></pre> <p>Here is the diagram, each pod has its own IP address, however, these IP addresses are not static, as pods can go down or restart at any time, then new pods are created, and new IP addresses are assigned. Thus, these IP addresses cannot be used to communicate internally between applications.</p> <p>So with the help of ClusterIP, each service will be assigned with a name and an IP address inside the cluster and the other pods can use either one (Service name/Cluster IP address) to access the service. This is the default type of service.</p> <pre><code>flowchart\n  pod[pod port 8080]\n  service[service port 8000] --&gt; pod</code></pre> service.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: backend-service\nspec:\n  selector:\n    app: backend\n  type: ClusterIP\n  ports:\n    - targetPort: 8080 # The port that Pod exposes\n      port: 8000 # Service Port\n</code></pre>"},{"location":"docs/kubernetes/services/#nodeport","title":"NodePort","text":"<pre><code>flowchart\n  NodePort[NodePort port 30121] --&gt; Service\n  Service[Service port 8000] --&gt; Pod\n  Pod[Pod port 8080]</code></pre> <p>NodePort is actually mapping a internal port on the node to a port that the pod exposes via Service. The port on the node is used to access or expose the application externally.</p> service.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: backend-service\nspec:\n  selector:\n    app: backend\n  type: NodePort\n  ports:\n    - targetPort: 8080 # The port that Pod exposes\n      port: 8000 # Service Port\n      nodePort: 30121 # valid range from 30000-32767\n</code></pre> <ul> <li>You can specify a <code>nodePort</code>, but if you don't specify one, the system will automatically assign one in the range 30000-32767.</li> </ul> Bash<pre><code># Use cluster-info to get your IP address\nkubectl cluster-info\n\n# public node IP = Public IP address of the node\ncurl http://&lt;public-node-ip&gt;:&lt;node-port&gt;\ncurl http://192.168.1.5:30121\n\n# For my case is Podman\ncurl http://kind-cluster-control-plane:30121\n</code></pre> <p></p>"},{"location":"docs/kubernetes/services/#more-explanation","title":"More explanation","text":"<p>What happens if the pods have the same label within the same node?</p> <p>Assuming that the service finds five pods with the same label, then it will pick all five pods as the endpoints for external requests. Another thing is it will use a random algorithm to distribute the network traffic/load across multiple instances of an application.</p> <p>What happen when the pods are distributed across multiple nodes?</p> <p></p> <p>Kubernetes will automatically create a service that spans all the nodes and maps its targetPort to each nodePort on all the nodes in the cluster. Thus, the application can be accessed by using the node IP and the same nodePort number in the cluster.</p> <p>In addition, if you have three nodes and your pods have just been deployed on two of them, they will still be accessed on all the nodes in the cluster and this is how the service is configured in Kubernetes.</p>"},{"location":"docs/kubernetes/services/#load-balancer","title":"Load Balancer","text":"<p>Reference</p> <p>We know that the NodePort url is something like this <code>http://&lt;public-node-ip&gt;:&lt;node-port&gt;</code>, but this is not feasible and user-friendly for the user to access the application. So, we would need to create a DNS name that actually map the portion of this <code>&lt;public-node-ip&gt;:&lt;node-port&gt;</code>. In this case, we have multiple ways to achieve it.</p> <ul> <li> <p>We can create a new VM, install, and configure a suitable load balancer like Nginx, HAProxy, etc. Then, we will have to configure the load balancer to route the traffic to the underlying nodes. But manually setting all these process can be a pain and it is very hard to maintain.</p> </li> <li> <p>Therefore, Kubernetes actually supported some of the cloud platforms like AWS, Google Cloud, Azure, etc. We can leverage and integrate the native load balancer from there. So you no need to configure and maintain the load balancer. What you need to do is just replace <code>type: NodePort</code> to <code>type: LoadBalancer</code>, and it will have the same effect as <code>NodePort</code> to distribute or balance the network load/traffic.</p> </li> </ul> <pre><code>graph TD\n  A[External Client] --&gt;|HTTP/HTTPS Request| B[Ingress]\n  B --Send Request--&gt; Z[\"Load Balancer (GCP, AWS, Azure)\"]\n  Z --&gt;|Route 1: Host/Path A| C[Service A]\n  Z --&gt;|Route 2: Host/Path B| D[Service B]\n  Z --&gt;|Route 3: Host/Path C| E[Service C]\n  C --&gt;|Forwards Traffic| F[Pod A1, Pod A2]\n  D --&gt;|Forwards Traffic| G[Pod B1, Pod B2]\n  E --&gt;|Forwards Traffic| H[Pod C1, Pod C2]</code></pre> <p>Kubernetes will send a request to the cloud provider to create/provision/deploy a load balancer for the service and assign a public IP address to the load balancer. The load balancer will then route the traffic to the service and then to the pod.</p>"},{"location":"docs/kubernetes/static-pod/","title":"Static Pod","text":""},{"location":"docs/kubernetes/static-pod/#concept-of-static-pod","title":"Concept of Static Pod","text":"<p>Before actually understand what is Static Pod, we know that the kubelet will receive the instructions from the kube-apiserver to deploy a container or pod on the node and this decision is made by the kube-scheduler, after that it will store the information in the etcd.</p> <p>Now, there are some questions that we need to ask ourselves:</p> <ul> <li>What if there is no kube-apiserver, kube-scheduler, no controller, no etcd, no  master, etc.</li> <li>What if we want to run a pod on the node without the help of the master.</li> <li>Can it operate as an independent node?</li> </ul> <p>Actually, the kubelet can run the pod without the help of the master, because kubelet knows how to create a pod and run it. So, when we want to create static pods, we need to provide the pod definition file to the kubelet from a specific directory <code>/etc/kubernetes/manifests</code> on the server mainly to store the information of the pods.</p> <p>Here is the explanation of how the static pod created and maintained:</p> <ul> <li>kubelet will regularly check the <code>/etc/kubernetes/manifests</code> directory for the pod definition file, then it will create the pod and will ensure the pod stays running (alive).</li> <li>kubelet will try to restart the pod if it is not running.</li> <li>kubelet will recreate the pod if the file content has been changed.</li> <li>kubelet will delete the pod if the file is removed.</li> </ul> <p>Now, all these pods managed by the kubelet without the intervention of the master (kube-apiserver, cluster components, etc) are called static pods. You can only create pods, not deployments, services, etc, because kubelet only understands the Pods.</p>"},{"location":"docs/kubernetes/static-pod/#why-do-we-need-to-use-static-pod","title":"Why do we need to use static pod?","text":"<p>Well, you can use static pods to deploy the Kubernetes control plane components as Pods on a node, as the kubelet will ensure the pod stays running, as it will automatically restart the pod if it is not running. Actually this is how kubeadm setup the Kubernetes cluster, as when you list the pod in the <code>kube-system</code> namespace, you can see all the control plane components are running as pods.</p> <p></p>"},{"location":"docs/kubernetes/static-pod/#usage-of-static-pod","title":"Usage of Static Pod","text":"<p>Reference for finding and configuring kubelet.service</p> <p>Before you can create the static pods, you have to configure the kubelet to look for a particular directory for the pod definition file.</p> <p>Info</p> <p>When you inspect an existing cluster for the kubelet, then you first check whether the <code>--pod-manifest-path</code> option is configured in the <code>kubelet.service</code> file or not. If it is not configured, then look for <code>--config</code> option in the <code>kubelet.service</code> file. If got configured, then check the config file path <code>staticPodPath</code> option for the pod definition file.</p> <ol> <li>Create a directory <code>/etc/kubernetes/manifests</code> on the server. You can create any directory you want.</li> <li>Configure that option in <code>kubelet.service</code> file. You can find the <code>kubelet.service</code> file in the <code>/etc/systemd/system/kubelet.service.d</code>directory.   kubelet.service<pre><code>ExecStart=/usr/bin/kubelet \\\\\n  ....\n  # Add this line\n  --pod-manifest-path=/etc/kubernetes/manifests \\\\\n  ....\n</code></pre></li> <li>If you don't want to use above method, then you can use <code>config file</code> option to configure that. (kubeadm is using this approach)<ul> <li>Create a config file kubelet-config.yaml<pre><code>staticPodPath: /etc/kubernetes/manifests\n</code></pre></li> <li>Add the config file path in the <code>kubelet.service</code> file.   kubelet.service<pre><code>ExecStart=/usr/bin/kubelet \\\\\n  ....\n  # Add this line\n  --config=kubelet-config.yaml \\\\\n  ....\n</code></pre></li> </ul> </li> </ol> <p>Okay, when the static pods are created, you can see the pods are running by using <code>docker ps</code> command, as <code>kubelet</code> command only works with kube-apiserver and because those Kubernetes components haven't setup or start.</p> <p>If the kubelet receives the instructions from the kube-apiserver, the kubelet still able to create the pod and run it at the same time, just </p> <ul> <li>static pod is through pod defintion files that read from particular directory.</li> <li>for normal pod, it is through the kube-apiserver that send the requests through HTTP API endpoint to kubelet.</li> </ul> <p>Besides, the kube-apiserver will aware of the static pods created by the kubelet, as these static pods are part of the cluster, as when the static pod is created, it will create a read-only mirror pod object in the kube-apiserver. Meaning that you can only view the pod details and cannot edit or delete the static pod from the kube-apiserver, so you have to delete the static pod from that particular directory that you specified.</p> Bash<pre><code>kubectl get pods\n# results\nNAME                 READY   STATUS    RESTARTS   AGE\nstatic-pod-node01    1/1     Running   0          1m\n</code></pre> <ul> <li>Remember, for the static pod deployments, it will auto append the node name to the pod name.</li> </ul> Static Pod DaemonSets Created by the kubelet Create by the kube-apiserver (DaemonSet Controller) Deploy control plane components as static pods Deploy agents like monitoring, logging, etc on nodes Ignored by the kube-scheduler Ignored by the kube-scheduler <p>The kube-scheduler has no effect on these pods that created by static pod and DaemonSets.</p>"},{"location":"docs/kubernetes/taints-and-tolerations/","title":"Taints and Tolerations","text":""},{"location":"docs/kubernetes/taints-and-tolerations/#concept-of-taints-and-tolerations","title":"Concept of Taints and Tolerations","text":"<p>Info</p> <ul> <li>Taints are set on nodes.</li> <li>Tolerations are set on pods.</li> </ul> <p>Remember, Taints and Tolerations do not tell the pod to go to a particular node, instead it just inform the node to only accept pods with certain tolerations. It only restrict nodes from accepting certain pods.</p> <p>Taints and tolerations basically just to ensure that the pods are scheduled onto the right nodes by setting some restrictions on the nodes that only accept the pods with certain tolerations.</p> <pre><code>flowchart\n  style node1 fill:#008000\n  pod1[Pod 1]\n  pod2[Pod 2]\n  pod3[Pod 3]\n\n  subgraph node2[Node 2]\n    pod6[Pod 6]\n    pod7[Pod 7]\n  end\n\n  subgraph node1[Node 1]\n    style node1 fill:#008000\n\n    pod4[Pod 4]\n    pod5[Pod 5]\n  end</code></pre> <p>Let's say we have Node 1 and Node 2 and we have Pod 1, Pod 2, and Pod 3. If we set a taint with a key-value pair <code>app=green</code> on Node 1, then Node 1 will only accept the pods with the toleration of <code>app=green</code>. So, now you want to assign Pod 1 to Node 1, you need to set the tolerations of <code>app=green</code> on Pod 1. </p> <pre><code>flowchart\n  pod2[Pod 2]\n  pod3[Pod 3]\n\n  subgraph node2[Node 2]\n    pod6[Pod 6]\n    pod7[Pod 7]\n  end\n\n  subgraph node1[Node 1]\n    style node1 fill:#008000\n    pod1[Pod 1]\n    pod4[Pod 4]\n    pod5[Pod 5]\n  end</code></pre> <p>Therefore, the other pods like Pod 2 and 3 will not be scheduled/assigned onto Node 1, as Node 1 will only accept the pods with the toleration of <code>app=green</code>. Of course, Pod 1 can also being scheduled on Node 2, because Node 2 does not have any taints applied, therefore taint only tells the node to accept the pods with certain tolerations and does not tell the pod to go to that particular node.</p>"},{"location":"docs/kubernetes/taints-and-tolerations/#steps-of-doing-taints-and-tolerations","title":"Steps of doing taints and tolerations","text":"<p>Before I go through the steps, let me explain why the Kubernetes Scheduler is not scheduling the pods on the master node. It is because the master node has a taint setup automatically when the Kubernetes cluster is created. Therefore, it will prevent any pods from being scheduled on the master node.</p> Bash<pre><code># Check the taints on the node\nkubectl describe node &lt;node-name&gt; | grep Taint\nkubectl describe node node | grep Taint\n</code></pre>"},{"location":"docs/kubernetes/taints-and-tolerations/#step-1-taint-the-node","title":"Step 1: Taint the Node","text":"Bash<pre><code>kubectl taint nodes &lt;node-name&gt; &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;\nkubectl taint nodes node1 app=green:NoSchedule\n</code></pre> <p>The taint effect defines what happens to the pods that do not tolerate the taint. The possible effects are:</p> <ul> <li>NoSchedule: The pod will not be scheduled onto the node.</li> <li>PreferNoSchedule: The scheduler will try to avoid placing a pod that does not tolerate the taint on the node, but this is not guaranteed.</li> <li>NoExecute: The pod will be evicted (kicked out) from the node if it is already running on the node, that means the pod is being killed. Besides, the new pods will not be scheduled onto the node if they do not tolerate the taint.</li> </ul>"},{"location":"docs/kubernetes/taints-and-tolerations/#step-2-tolerations-on-the-pod","title":"Step 2: Tolerations on the Pod","text":"sample-pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containersL:\n    - name: sample-container\n      image: nginx\n  tolerations:\n    - key: \"app\"\n      operator: \"Equal\"\n      value: \"green\"\n      effect: \"NoSchedule\"\n</code></pre> <ul> <li>Remember all tolerations values need to be encoded in <code>\"\"</code> (double quotes).</li> </ul>"},{"location":"docs/kubernetes/taints-and-tolerations/#step-3-optional-remove-the-taint","title":"Step 3: (Optional) Remove the taint","text":"<p>Just add <code>-</code> at behind of the taint effect to remove the taint.</p> Bash<pre><code>kubectl taint nodes &lt;node-name&gt; &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;-\nkubectl taint nodes node1 app=green:NoSchedule-\n</code></pre>"},{"location":"docs/kubernetes/tls/","title":"TLS","text":""},{"location":"docs/kubernetes/tls/#transport-layer-security-tls-basics","title":"Transport Layer Security (TLS) Basics","text":"<p>Info</p> <p>The concept is very similar to public and private keys, where the public key in used to encrypt the data and the private key is used to decrypt the data.</p> <p>Transport Layer Security (TLS) certificate also known as Secure Sockets Layer (SSL) certificate, mainly to secure internet connections by encrypting data sent between browser and server.</p> <pre><code>flowchart LR\n  %% Nodes representing each step in the process\n  A[Browser] ---|Certificate| B[Server]</code></pre> <p>Generate a private key Bash<pre><code># Generate a private key\nopenssl genrsa -out sample.key 1024\ncat sample.key\n\n# sample.key file content\n-----BEGIN PRIVATE KEY-----\nMIICdgIBADANBgkqhkiG9w0BAQEFAASCAmAwggJcAgEAAoGBALZgjDfEXndMv3eg\n+cQqLoL+0jcqb2dWt63cvBH38rqD2XCxgpzsXT5KN0Q6kiT72tnMp2ij5gO1i7DG\nw00t5767AX0kolI0QqvLgW3BMhTMhlZTUtEsNCl3/09P1z7Z6gJhgOfWHBTlkPiv\ntJlmXGqgUnT5yFioemEd0WIMqYERAgMBAAECgYEAqhom7iTDjzYQJz4Hd4V4WEng\nO//iZFone4w0BfZjeL9dYZTPJBn5Zg3GumZ+xOv7VcViJ2EnOjJsFfdtVWEJkfKt\nxDYZPRDah88EiOMSScuAsxLC+gk7N9QbywLPPP4PphrFVMzoAoIKXNtJ/+CRKTTc\npsxCYtEn61hsrKTNJ4ECQQDbsOi5OgdFFM3gvAaRSU7iDfjkYeXmZpfRhrvAqlYs\nunjAdAUkT7NnCglVITdYmxhZvPa2Wby2mWSGixZuHls5AkEA1ITktryAZhjQmdS9\nfPklE/88NRysMw9xXxdTIKJrKbgvbtzkXEIARayw3GK0UmKJtXwSs5mzLH30FnMP\ntyDcmQJAECoGFYrVehm7xCtReGMicLOCeptRZRBrXzlAmz5tNMFLnS1sK23Jz20H\nnsHg5p4SvpeOt4AGyJCkutmE/vkw0QJAHMhS4ZzuFuXNesghUce8idTLprXXbKLu\ny94w1/6nXOMhKXapRwV5W/ZjjbWgjnfiBr5jGgf7CE+Wu2OiAT9mIQJAFe0cO7BV\n8xYNqIty34/iSfRghJ96TtbozgHhXQ5CmbYzt+b5VK1MNx8g5mRr/CkUldBTei8L\neDn7hXBb1eqedg==\n-----END PRIVATE KEY-----\n</code></pre></p> <p>Generate a public key based on private key Bash<pre><code>#------------------------------------------------------------#\n# Generate a public key based on private key\nopenssl rsa -in sample.key -pubout &gt; sample.pem\ncat sample.pem\n\n# sample.pem file content\n-----BEGIN PUBLIC KEY-----\nMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQC2YIw3xF53TL93oPnEKi6C/tI3\nKm9nVret3LwR9/K6g9lwsYKc7F0+SjdEOpIk+9rZzKdoo+YDtYuwxsNNLee+uwF9\nJKJSNEKry4FtwTIUzIZWU1LRLDQpd/9PT9c+2eoCYYDn1hwU5ZD4r7SZZlxqoFJ0\n+chYqHphHdFiDKmBEQIDAQAB\n-----END PUBLIC KEY-----\n</code></pre></p> <p>Now, the above commands will generate a self-signed certificate. This certificate is not trusted by browsers, so you need to get a certificate from a trusted Certificate Authority (CA). CA are companies that are trusted to sign, validate, and issue certificates. For example, GlobalSign, DigiCert, GoDaddy, etc.</p> <p>In this case, we will generate a Certificate Signing Request (CSR) using the private key and with the domain name.</p> Bash<pre><code>openssl req -new \\\n  -key sample.key \\\n  -out sample.csr \\\n  -subj \"/C=US/ST=California/L=San Francisco/O=Example/OU=IT Department/CN=domain.com\"\n</code></pre> <ul> <li>more details on this command can be found here</li> <li>generate a certifate signing request using openssl here</li> </ul> <p>So, this <code>sample.csr</code> file will be sent to the CA to verify, validate, and sign the certificate. The CA will then send back the signed certificate that the browser will trust. Remember, CAs themselves have a set of public and private keys to sign the certificate.</p> <ul> <li>private key = sign the certificate</li> <li>public key = verify and validate the certificate. These public keys from the CA are already installed in the browser.</li> </ul> Public key Private Key <code>*.crt</code>, <code>*.pem</code> <code>*.key</code>, <code>*-key.pem</code> <p>Glossary:</p> <ul> <li>CA: Certificate Authority</li> <li>CSR: Certificate Signing Request</li> <li>CRT: Certificate</li> <li>PEM: Privacy Enhanced Mail, it can contain various types of data, including certificates, private keys, and other related information. PEM files are encoded in Base64 and typically include headers and footers to indicate the type of data they contain. The most common types of data found in PEM files are<ul> <li>Certificates: These can be in the form of X.509 certificates, which are used in SSL/TLS for securing communications.</li> <li>Private Keys: These are the private keys associated with the certificates, used for decryption and signing.</li> <li>Certificate Chains: These include the certificate along with intermediate and root certificates to establish a chain of trust.</li> <li>Public Keys: These are the public keys associated with the private keys, used for encryption and verifying signatures.</li> </ul> </li> <li>KEY: Private key</li> </ul>"},{"location":"docs/kubernetes/tls/#tls-in-kubernetes","title":"TLS in Kubernetes","text":"<p>In Kubernetes, the communication between all the Kubernetes components must be secure and encrypted. This is where TLS comes into play. For example, no matter the admin is using kubectl or Kubernetes API directly, the communication must be secure.</p> <p>So, we have server and client certificates. The server certificate is used secure communication between the Kubernetes components, for example, kube-apiserver, kubelet, etcd, where the client certificate is used to authenticate users and applications that are trying to interact with the Kubernetes cluster and to verify who they are.</p> <p>Remember, when we create these certificates, we need to sign them with a CA. In fact, Kubernetes requires you to have at least one certificate authority to sign all the certificates, but you can have multiple CAs configured in your cluster.</p>"},{"location":"docs/kubernetes/tls/#certificates","title":"Certificates","text":"<ul> <li>Server certificates: Deployed on servers.</li> <li>Client certificates: Used by clients to authenticate themselves to the server.</li> <li>Root certificates: Held by the Certificate Authority (CA) to sign server certificates.</li> </ul>"},{"location":"docs/kubernetes/tls/#server-certificates","title":"Server certificates","text":"<p>We actually got 3 parties here:</p> <ul> <li> <p>kube-apiserver</p> <ul> <li>This is the main component in Kubernetes, so it is a server. Therefore it needs a server certificate to secure the communication betweeh the client and the server (authenticate their clients).</li> <li>For example, you can create <code>kube-apiserver.crt</code> and <code>kube-apiserver.key</code> files. Remember, you can put any file name you want.</li> </ul> </li> <li> <p>etcd</p> <ul> <li>This is the database that will store all the Kubernetes cluster data. So it is a server.</li> <li>For example, you can create <code>etcd.crt</code> and <code>etcd.key</code> files.</li> </ul> </li> <li> <p>kubelet</p> <ul> <li>This is the agent that runs on each node and they will expose an HTTPS API endpoint that the kube-apiserver will talk to in the Kubernetes cluster. So it is a server.</li> <li>For example, you can create <code>kubelet.crt</code> and <code>kubelet.key</code> files.</li> </ul> </li> </ul>"},{"location":"docs/kubernetes/tls/#client-certificates","title":"Client certificates","text":"<p>For the clients parties to access the services, we have:</p> <ul> <li> <p>The clients who access the kube-apiserver. The client needs a client certificate to authenticate themselves to the kube-apiserver.</p> <ul> <li>kube-scheduler<ul> <li>For example, <code>kube-scheduler.crt</code> and <code>kube-scheduler.key</code>.</li> </ul> </li> <li>kube-controller-manager<ul> <li>For example, <code>kube-controller-manager.crt</code> and <code>kube-controller-manager.key</code>.</li> </ul> </li> <li>kube-proxy<ul> <li>For example, <code>kube-proxy.crt</code> and <code>kube-proxy.key</code>.</li> </ul> </li> <li>Admin who uses kubectl or Kubernetes API directly<ul> <li>For example, <code>admin.crt</code> and <code>admin.key</code>.</li> </ul> </li> </ul> </li> <li> <p>The clients who access the etcd or kubelet. The client needs a client certificate to authenticate themselves to the etcd or kubelet.</p> <ul> <li>kube-apiserver<ul> <li>You can use back the <code>kube-apiserver.crt</code> and <code>kube-apiserver.key</code> files, or you want to create a new one also can. The same goes for the kubelet.</li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/kubernetes/tls/#manually-create-certificates","title":"Manually create certificates","text":"<p>Reference</p> <p>You can use any tools like OpenSSL, Easy-RSA, etc to create the certificates.</p>"},{"location":"docs/kubernetes/tls/#step-1-generate-ca-certificate","title":"Step 1: Generate CA certificate","text":"Bash<pre><code># Generate a private key\nopenssl genrsa -out ca.key 2038\n\n# Generate a csr using the private key\n# CN = Common Name\nopenssl req -new -key ca.key -subj \"/CN=Kubernetes-CA\" -out ca.csr\n\n# Sign the certificate\nopenssl x509 -req -in ca.csr -signkey ca.key -out ca.crt\n</code></pre> <ul> <li>With all the above commands, the CA has its own private key and root certificate file.</li> </ul>"},{"location":"docs/kubernetes/tls/#step-2-generate-clients-certificate","title":"Step 2: Generate clients certificate","text":"<p>Let's say we want to create a client certificate for the admin user. You just need to repeat the same steps for other clients that need certificates to access the kube-apiserver.</p> Bash<pre><code># Generate a private key\nopenssl genrsa -out admin.key 2038\n\n# Generate a csr using the private key\n# CN = Common Name\nopenssl req -new -key admin.key -subj \"/CN=Kubernetes-admin\" -out admin.csr\n\n# Sign the certificate using CA cert and key\nopenssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt\n</code></pre> <ul> <li>Remember to sign the certificate using the CA certificate and key, so the cluster will trust the certificate.</li> <li>The <code>admin.crt</code> will be used by the admin to authenticate themselves to the server.</li> </ul> <p>Now assuming you create a new user called john, how do you differentiate betweeh the admin and john users, as admin user is mainly do the administrators job while john user is a normal user. Well, in this case, you have to mention the group details in the processing of generating CSR.</p> Bash<pre><code>openssl req -new -key admin.key \\\n  -subj \"/CN=Kubernetes-admin/O=system:masters\" -out admin.csr\n</code></pre> <ul> <li>The <code>system:masters</code> group name already exists in the Kubernetes cluster with admin privileges.</li> <li>The <code>system:masters</code> is the group name that the admin user belongs to. So, the admin user will have the admin privileges.</li> </ul>"},{"location":"docs/kubernetes/tls/#kubelet","title":"kubelet","text":"<p>For kubelet client certificates, actually got a bit different from the other clients. The certificates naming and formats starts with the <code>system:node:&lt;node-name&gt;</code>. This is because kubelet is running on each node, and nodes are system components like kube-scheduler. With this format, the kube-apiserver knows which node is authenticating and give the right permissions based on this format.</p> <p>You might have a question? How the kube-apiserver give the right permissions based on the format. Well, actually nodes must be added to a group called <code>system:nodes</code> in the Kubernetes cluster. This group is predefined in the Kubernetes cluster and has the right permissions to access the kubelet.</p> <p>After the certificates are generated, remember to configure them in kubeconfig files.</p>"},{"location":"docs/kubernetes/tls/#test-the-certificate","title":"Test the certificate","text":"<p>Actually, there are multiple ways to test the connections.</p> <p>One way is to use the <code>curl</code> command to test the connection.</p> Bash<pre><code>curl https://kube-apiserver:6443/api/v1/pods \\\n  --key admin.key --cert admin.crt --cacert ca.crt\n</code></pre> <p>The other way is to move all these configurations to the <code>kube-config.yaml</code> file and use the <code>kubectl</code> command to test the connection. This method is more common and easier to manage.</p> kube-config.yaml<pre><code>apiVersion: v1\nkind: Config\nclusters:\n  - cluster:\n      certificate-authority: ca.crt\n      server: https://kube-apiserver:6443\n    name: kubernetes\nusers:\n  - name: admin\n    user:\n      client-certificate: admin.crt\n      client-key: admin.key\n</code></pre> <p>Now, remember, all the clients and servers need to have the CA's root certificate to trust the certificates. So, you need to distribute, copy, and specify the CA's root certificate to all the Kubernetes components, clients, and servers when you configure them.</p>"},{"location":"docs/kubernetes/tls/#step-3-generate-servers-certificate","title":"Step 3: Generate servers certificate","text":"<p>The steps of creating a server certificate are similar to creating a client certificate. The only difference is that we want to ensure those servers like kube-apiserver, etcd, and kubelet are high available and secure by deploying them in multiple servers. So, to secure the communication between the servers, we will need to generate additional peer certificates.</p> <pre><code>flowchart\n  A[component] --&gt; B[peer 1]\n  A[component] --&gt; C[peer 2]</code></pre> <ul> <li>each peer will have its own certificate and private key.</li> </ul>"},{"location":"docs/kubernetes/tls/#etcd","title":"etcd","text":"etcd.yaml<pre><code>- etcd\n  - --advertise-client-urls=https://10.89.0.2:2379\n  - --cert-file=/etc/kubernetes/pki/etcd/server.crt # The path to the server's SSL certificate file\n  - --client-cert-auth=true # Enables client certificate authentication for client connections\n  - --data-dir=/var/lib/etcd\n  - --experimental-initial-corrupt-check=true\n  - --experimental-watch-progress-notify-interval=5s\n  - --initial-advertise-peer-urls=https://10.89.0.2:2380\n  - --initial-cluster=kind-cluster-control-plane=https://10.89.0.2:2380\n  - --key-file=/etc/kubernetes/pki/etcd/server.key # The path to the server's SSL key file\n  - --listen-client-urls=https://127.0.0.1:2379,https://10.89.0.2:2379\n  - --listen-metrics-urls=http://127.0.0.1:2381\n  - --listen-peer-urls=https://10.89.0.2:2380\n  - --name=kind-cluster-control-plane\n  - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt # The path to the peer's SSL certificate file\n  - --peer-client-cert-auth=true # Enables client certificate authentication for peer connections\n  - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key # The path to the peer's SSL key file\n  - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt # The path to the trusted CA certificate file for peer connections\n  - --snapshot-count=10000\n  - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt # The path to the trusted CA certificate file for client connections\n</code></pre> <ul> <li>Remember to specify the CA root certificate as well.</li> </ul>"},{"location":"docs/kubernetes/tls/#kube-apiserver","title":"kube-apiserver","text":"<p>We know that every operation in the Kubernetes cluster is done through the kube-apiserver. So this component may have many names and aliases within the cluster. Here is the list of names and aliases that the kube-apiserver may have:</p> <ul> <li>kubernetes</li> <li>kubernetes.default</li> <li>kubernetes.default.svc</li> <li>kubernetes.default.svc.cluster.local</li> <li>ip address of the server like 10.89.0.2</li> </ul> <p>When we have these names available, we have configure these names and aliases in the certificate, so that any clients or users can use these names and aliases to access the kube-apiserver.</p> <p>In this case, we have to create a configuration file to specify the aliases and names.</p> openssl.cnf<pre><code>[req]\nreq_extensions = v3_req\ndistinguished_name = req_distinguished_name\n\n[ v3_req ]\nbasicContrainsts = CA:FALSE\nkeyUsage = nonRepudiation,\nsubjectAltName = @alt_names\n\n[ alt_names ]\nDNS.1 = kubernetes\nDNS.2 = kubernetes.default\nDNS.3 = kubernetes.default.svc\nDNS.4 = kubernetes.default.svc.cluster.local\nIP.1 = 10.89.0.2\n</code></pre> Bash<pre><code># Pass the configuration file while generating a CSR.\nopenssl req -new -key kubeapiserver.key \\\n  -subj \"/CN=kube-apiserver\" -out kubeapiserver.csr \\\n  -config openssl.cnf\n\n# Sign the certificate using CA cert and key\nopenssl x509 -req -in kubeapiserver.csr -CA ca.crt -CAkey ca.key -CAcreateserial \\\n  -out kubeapiserver.crt -extensions v3_req -extfile openssl.cnf -days 1000\n</code></pre> kube-apiserver.yaml<pre><code>- kube-apiserver\n  - --advertise-address=10.89.0.2\n  - --allow-privileged=true\n  - --authorization-mode=Node,RBAC\n  - --client-ca-file=/etc/kubernetes/pki/ca.crt\n  - --enable-admission-plugins=NodeRestriction\n  - --enable-bootstrap-token-auth=true\n  - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt\n  - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt\n  - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key\n  - --etcd-servers=https://127.0.0.1:2379\n  - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt\n  - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key\n  - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n  - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt\n  - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key\n  - --requestheader-allowed-names=front-proxy-client\n  - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt\n  - --requestheader-extra-headers-prefix=X-Remote-Extra-\n  - --requestheader-group-headers=X-Remote-Group\n  - --requestheader-username-headers=X-Remote-User\n  - --runtime-config=\n  - --secure-port=6443\n  - --service-account-issuer=https://kubernetes.default.svc.cluster.local\n  - --service-account-key-file=/etc/kubernetes/pki/sa.pub\n  - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key\n  - --service-cluster-ip-range=10.96.0.0/16\n  - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt\n  - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\n</code></pre>"},{"location":"docs/kubernetes/tls/#kubelet_1","title":"kubelet","text":"<p>Assuming you have multiple worker nodes, you will need to generate a certificate for each worker node, but how do you name the certificates? Well, actually they will be named after the node name. Once the certificates are generated, configure them kubelet YAML configuration file.</p> kubelet-config.yaml<pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nauthentication:\n  x509:\n    clientCAFile: \"/etc/kubernetes/pki/ca.pem\"\nauthorization:\n  mode: Webhook\nclusterDomain: \"cluster.local\"\nclusterDNS:\n  - \"10.32.0.10\"\nresolvConf: \"/etc/resolv.conf\"\nruntimeRequestTimeout: \"15m\"\ntlsCertFile: \"/etc/kubernetes/kubelet/node-1.crt\"\ntlsPrivateKeyFile: \"/etc/kubernetes/kubelet/node-1-key.key\"\n</code></pre>"},{"location":"docs/kubernetes/tls/#certificate-details","title":"Certificate details","text":"<p>Certificate paths and details</p> <p>Before we understand the certificate details, we need to understand how the Kubernetes cluster was setup. There are multiple ways to setup the Kubernetes cluster, each way has its own method to generate and manage the certificates. In this case we are looking at the common practice that people will do: Deploy Kubernetes components as pods.</p> <p>Let's take a look at the kube-apiserver pod.</p> kube-apiserver.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver-kind-cluster-control-plane\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --advertise-address=10.89.0.2\n    - --allow-privileged=true\n    - --authorization-mode=Node,RBAC\n    - --client-ca-file=/etc/kubernetes/pki/ca.crt\n    - --enable-admission-plugins=NodeRestriction\n    - --enable-bootstrap-token-auth=true\n    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt\n    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt\n    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key\n    - --etcd-servers=https://127.0.0.1:2379\n    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt\n    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key\n    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt\n    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key\n    - --requestheader-allowed-names=front-proxy-client\n    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt\n    - --requestheader-extra-headers-prefix=X-Remote-Extra-\n    - --requestheader-group-headers=X-Remote-Group\n    - --requestheader-username-headers=X-Remote-User\n    - --runtime-config=\n    - --secure-port=6443\n    - --service-account-issuer=https://kubernetes.default.svc.cluster.local\n    - --service-account-key-file=/etc/kubernetes/pki/sa.pub\n    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key\n    - --service-cluster-ip-range=10.96.0.0/16\n    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt\n    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\n    image: registry.k8s.io/kube-apiserver:v1.30.0\n    ...\n</code></pre> <p>Take a look at <code>/etc/kubernetes/pki/apiserver.crt</code>.</p> Bash<pre><code>openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout\n\n# output\nCertificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number: 1579479310736570071 (0x15eb70108e8836d7)\n        Signature Algorithm: sha256WithRSAEncryption\n        Issuer: CN = kubernetes\n        Validity\n            Not Before: Dec 28 06:55:09 2024 GMT\n            Not After : Jan  5 06:16:55 2026 GMT\n        Subject: CN = kube-apiserver\n        Subject Public Key Info:\n            Public Key Algorithm: rsaEncryption\n                Public-Key: (2048 bit)\n                Modulus:\n                    00:c4:ac:ac:9c:d8:2c:cb:22:95:cd:a5:16:05:62:\n                    83:2c:3d:34:bb:a3:76:a9:9d:ca:68:a3:cc:89:3d:\n                    90:a8:46:33:5c:2c:eb:5d:d4:66:55:1d:80:16:39:\n                    f2:26:a0:23:74:fd:23:00:cc:49:2e:6f:70:82:c0:\n                    ec:ca:74:8f:c0:9a:d5:c1:ef:f5:80:dd:7e:58:b6:\n                    a9:66:82:5a:10:49:7e:90:f2:6a:e9:18:35:7a:c1:\n                    6c:8a:f9:1d:b9:cc:5f:b2:63:18:7a:d6:3b:48:ec:\n                    95:c4:e9:2f:43:89:cf:4b:5a:5b:08:c6:6d:10:89:\n                    ea:c4:32:1f:48:85:17:ed:da:19:ae:f5:80:76:e6:\n                    7d:93:bb:f2:7a:39:3b:45:a9:c9:c1:99:13:f7:94:\n                    38:e4:97:9b:9e:c5:f2:4a:59:f6:78:0f:bd:47:18:\n                    44:62:6f:15:aa:fe:7b:fc:00:11:ed:c8:f0:24:4b:\n                    16:8f:1b:34:05:34:2a:6c:60:c3:b7:3e:d7:0b:1b:\n                    c3:f5:f3:fe:63:5e:d5:46:47:59:7a:3c:19:de:7b:\n                    57:43:e5:62:6e:a3:fc:98:d5:88:e0:8a:4c:b7:81:\n                    04:5f:b8:3c:9e:ad:d5:f4:80:d9:c1:80:39:18:ff:\n                    60:1e:87:0b:08:0d:be:89:ac:e7:cd:df:f2:f4:3e:\n                    ac:ab\n                Exponent: 65537 (0x10001)\n        X509v3 extensions:\n            X509v3 Key Usage: critical\n                Digital Signature, Key Encipherment\n            X509v3 Extended Key Usage: \n                TLS Web Server Authentication\n            X509v3 Basic Constraints: critical\n                CA:FALSE\n            X509v3 Authority Key Identifier: \n                4A:FB:80:18:F0:10:D6:2B:18:D9:35:6C:28:35:4C:8E:16:C7:84:C2\n            X509v3 Subject Alternative Name:\n                DNS:kind-cluster-control-plane, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, DNS:localhost, IP Address:10.96.0.1, IP Address:10.89.0.2, IP Address:127.0.0.1\n    Signature Algorithm: sha256WithRSAEncryption\n    Signature Value:\n        55:45:84:5c:24:44:d0:e0:be:30:3b:91:01:84:1e:b8:ec:11:\n        b6:cb:d9:d1:9d:ea:70:0c:45:0c:ff:b9:ce:6b:20:68:40:d7:\n        de:86:b5:2d:0b:1e:8c:ec:79:14:c1:fc:ef:99:41:bf:a9:36:\n        34:42:61:d8:9e:2a:4e:82:c2:1c:74:bd:0f:ff:6d:d2:f3:aa:\n        4f:ab:2d:21:b3:e2:3c:47:f5:04:0a:b5:a4:4b:fb:53:b5:1c:\n        fb:bb:e2:ff:dc:82:1c:e6:14:b7:8c:ab:fc:4b:30:b3:28:38:\n        13:9b:d5:79:0d:b6:57:94:4f:a8:92:86:3c:0e:0f:78:dc:70:\n        9a:dc:22:dd:10:c4:9d:56:eb:d8:13:ca:69:81:52:af:ff:9d:\n        4d:65:14:39:42:2e:7f:4a:65:d1:0e:88:4d:eb:0c:e9:18:df:\n        15:18:06:89:cb:49:0c:ee:59:9c:11:51:c4:f7:f6:b7:6e:f9:\n        73:3a:ab:e4:39:ed:ed:6c:31:1c:cb:e6:12:bf:ed:4d:ae:7c:\n        b6:75:1b:50:ef:e0:79:cd:d2:6f:48:cf:8d:33:6a:d0:72:1a:\n        81:2d:c3:ac:71:40:71:9c:2c:22:a2:66:f7:33:82:37:48:6e:\n        39:ce:4b:93:ef:7f:85:d6:df:e7:42:41:18:64:9b:f6:1a:6b:\n        64:db:eb:d1\n</code></pre> <ul> <li><code>Issuer: CN = kubernetes</code> = Issuer of the certificate</li> <li><code>Not After : Jan  5 06:16:55 2026 GMT</code> = Expiry date of the certificate</li> <li><code>Subject: CN = kube-apiserver</code> = Common name of the certificate</li> <li><code>X509v3 Subject Alternative Name</code> = Alias names of the certificate</li> </ul> <p>If you want to debug the certificate, then you can use <code>kubectl logs</code> command to see the logs of the pod, if the <code>kubectl</code> unable to work properly, then you can use the <code>docker logs</code> command to see the logs of the container and troubleshoot.</p>"},{"location":"docs/kubernetes/useful-tools/","title":"Useful tools","text":"Tool Description Link kubectx Faster way to switch between clusters and namespaces in kubectl. https://github.com/ahmetb/kubectx"},{"location":"docs/kubernetes/volume/","title":"Volume","text":"<p>Volumes are used to store and presist data in containers. The data will not be lost when the container is restarted or deleted. Volumes can be used to share data between containers.</p> <p>Refer Kubernetes Volume Types for more information.</p> pod-volume.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n    - name: ubuntu\n      image: ubuntu\n      command: [\"/bin/sh\", \"-c\"]\n      args: [\"for i in {1..10}; do echo $i &gt;&gt; /output/numbers.txt; done\"]\n      volumeMounts:\n        - name: my-volume # same as volume name\n          mountPath: /output\n  volumes:\n    - name: my-volume\n      hostPath:\n        path: /data/output\n        type: Directory\n</code></pre> <ul> <li>The above example will store the container output <code>/output</code> in the host path <code>/data/output</code>.</li> </ul>"},{"location":"docs/kubernetes/what-is-kubernetes/","title":"What is Kubernetes?","text":""},{"location":"docs/kubernetes/what-is-kubernetes/#kubernetes-introduction","title":"Kubernetes Introduction","text":"<p>Reference</p> <p>Kubernetes, also known as K8s, is an open source system for automating deployment, scaling, and management of containerized applications.</p> <p>\u2014 kubernetes.io</p> <p>Kubernetes enables you to deploy multiple instances of your application as required and allows easy communication between different services within your application by hosting your applications as containers in an automated manner.</p>"},{"location":"docs/kubernetes/what-is-kubernetes/#kubectl","title":"kubectl","text":"<p>References</p> <ul> <li>Installation</li> <li>Overview</li> <li>Commands Reference</li> </ul> <p>kubectl is a command line tool used to interact with Kubernetes clusters, using the Kubernetes API. It allows you to run commands (deploy or manage applications) against Kubernetes clusters. It looks for a file in <code>$HOME/.kube</code> for configuration.</p> Bash<pre><code>kubectl cluster-info # get cluster information\nkubectl get nodes # get nodes information in the cluster\n</code></pre>"},{"location":"docs/python-oop/abstraction/","title":"Abstraction","text":""},{"location":"docs/python-oop/abstraction/#concept","title":"Concept","text":"<p>What is the difference between abstraction and encapsulation?</p> <ul> <li>Abstraction is about hiding complexity and showing only essential features.</li> <li>Encapsulation is about hiding data and controlling access to the internal state of an object.</li> </ul> <p>Hiding complex internal implementation details and showing only the essential features of the object. It allows the developer to focus on \"what to do\" (focus on what an object does) rather than \"how to do it\" (how it achieves its functionality).</p> Types of Abstraction Description Data Abstraction Hides the details of data representation and focuses on the essential properties of the data. Process Abstraction Hides the implementation of operations and focuses on the essential features of the process. Control Abstraction Hides the complexity of control flow and focuses on the essential features of control structures."},{"location":"docs/python-oop/abstraction/#implementation","title":"Implementation","text":"<p>Remember Abstract classes cannot be instantiated directly. They are meant to be subclassed, and the subclasses must implement the abstract methods defined in the abstract class. It can contain both abstract methods (without implementation) and concrete methods (with implementation).</p> Python<pre><code>from abc import ABC, abstractmethod\n\nclass Shape(ABC):\n  @abstractmethod\n  def area(self):\n    pass\n\n  @abstractmethod\n  def perimeter(self):\n    pass\n\nclass Circle(Shape):\n  def __init__(self, radius):\n    self.radius = radius\n\n  def area(self):\n    return 3.14159 * self.radius ** 2\n\n  def perimeter(self):\n    return 2 * 3.14159 * self.radius\n\n# This will raise TypeError:\n# shape = Shape() # Can't instantiate abstract class\n\ncircle = Circle(5)\nprint(circle.area())\n</code></pre> <p>Let me give you another example of abstraction.</p> Python<pre><code>class DatabaseConnection(ABC):\n  @abstractmethod\n  def connect(self):\n    pass\n\n  @abstractmethod\n  def execute_query(self, query):\n    pass\n\n  def get_user(self, user_id):\n    query = f\"SELECT * FROM users WHERE id = {user_id}\"\n    return self.execute_query(query)\n\nclass MySQLConnection(DatabaseConnection):\n  def connect(self):\n    # MySQL-specific connection logic\n    pass\n\n  def execute_query(self, query):\n    # MySQL-specific query execution\n    pass\n\nclass PostgreSQLConnection(DatabaseConnection):\n  def connect(self):\n    # PostgreSQL-specific connection logic\n    pass\n\n  def execute_query(self, query):\n    # PostgreSQL-specific query execution\n    pass\n</code></pre>"},{"location":"docs/python-oop/class-and-object/","title":"Class &amp; Object","text":""},{"location":"docs/python-oop/class-and-object/#concept","title":"Concept","text":"Class Object A blueprint or template for creating objects An instance of a class Defines attributes (data) and behaviours (method) that the created objects will have Contains data and methods defined by the class or has its own set of attribute values Uses the <code>class</code> keyword Created using the class name followed by parentheses Contains the <code>__init__</code> method for initialization Created by calling the class like a function"},{"location":"docs/python-oop/class-and-object/#implementation","title":"Implementation","text":"<p>Let's explore how to create classes and objects in Python.</p>"},{"location":"docs/python-oop/class-and-object/#step-1-create-a-class","title":"Step 1: Create a Class","text":"<p>To create a class in Python, use the <code>class</code> keyword followed by the class name. By convention, class names are written in CamelCase.</p> <p><code>Car</code> is your blueprint for creating car objects. It defines the attributes and methods that each car object will have.</p> <pre><code>classDiagram\n    class Car {\n        +int wheels = 4\n        +str brand\n        +str model\n        +int year\n        +bool is_running\n        +start()\n        +stop()\n    }</code></pre> <p>car.py<pre><code>class Car:\n  # Class attributes (shared by all instances)\n  wheels = 4\n\n  def __init__(self, brand, model, year):\n    self.brand = brand # Instance attribute\n    self.model = model # Instance attribute\n    self.year = year # Instance attribute\n    self.is_running = False # Instance attribute\n\n  def start(self): # Instance method\n    self.is_running = True\n    print(f\"{self.brand} {self.model} is now running.\")\n\n  def stop(self): # Instance method\n    self.is_running = False\n    print(f\"{self.brand} {self.model} has stopped.\")\n</code></pre> - <code>__init__()</code> method: This is the constructor that initializes the instance attributes when a new object is created. It takes <code>self</code> (the instance itself) and other parameters to set the initial state of the object.   - Attributes created in <code>__init__()</code> are called instance attributes and are unique to each object.</p>"},{"location":"docs/python-oop/class-and-object/#step-2-create-an-object","title":"Step 2: Create an Object","text":"<pre><code>classDiagram\n    class Car {\n        +int wheels = 4\n        +str brand\n        +str model\n        +int year\n        +bool is_running\n        +start()\n        +stop()\n    }\n\n    Car &lt;|-- my_car : instance\n    Car &lt;|-- another_car : instance</code></pre> <p>car.py<pre><code>class Car:\n  # Class attributes (shared by all instances)\n  wheels = 4\n\n  def __init__(self, brand, model, year):\n    self.brand = brand # Instance attribute\n    self.model = model # Instance attribute\n    self.year = year # Instance attribute\n    self.is_running = False # Instance attribute\n\n  def start(self): # Instance method\n    self.is_running = True\n    print(f\"{self.brand} {self.model} is now running.\")\n\n  def stop(self): # Instance method\n    self.is_running = False\n    print(f\"{self.brand} {self.model} has stopped.\")\n\n# Creating objects (instances)\nmy_car = Car(\"BMW\", \"X5\", 2020)\nanother_car = Car(\"Audi\", \"A4\", 2021)\n\n# Accessing attributes and methods\nprint(my_car.brand) # BMW\nmy_car.start() # BMW X5 is now running!\nprint(my_car.is_running) # True\n</code></pre> </p> <p></p>"},{"location":"docs/python-oop/cohesion-and-coupling/","title":"Cohesion and Coupling","text":"<p>Important</p> <ul> <li>High coupling and low cohesion can lead to difficult-to-maintain code</li> <li>Low coupling and high cohesion can lead to more maintainable, understandable, and flexible code.</li> </ul>"},{"location":"docs/python-oop/cohesion-and-coupling/#coupling","title":"Coupling","text":"<p>Coupling refers to the degree of interdependence between software modules. Meaning that how closely connected modules are, and how much they rely on each other.</p> <ul> <li>High coupling means modules are tightly (closely) connected and changes in one module can significantly affect others.</li> <li>Low coupling means modules are independent and changes in one module has minimal impact on others.</li> </ul> <pre><code>graph\n  A[Module A] --- B[Module B]\n  B --- C[Module C]\n  A --- C</code></pre>"},{"location":"docs/python-oop/cohesion-and-coupling/#cohesion","title":"Cohesion","text":"<p>Cohesion refers to the degree to which elements of a module belong together. Meaning that how closely related and focused the responsibilities of a single module are.</p> <ul> <li>High cohesion means that a module has a single purpose, well-defined responsibility and all its elements are closely related.</li> <li>Low cohesion means that a module has multiple purposes, unrelated responsibilties and its elements are not closely related.</li> </ul>"},{"location":"docs/python-oop/cohesion-and-coupling/#differences-of-cohesion-and-coupling","title":"Differences of Cohesion and Coupling","text":"Aspect Coupling Cohesion Definition Degree of interdependence between modules Degree to which elements within a module belong together Purpose Relationships between modules Relationships within a module Goal Low coupling (loosely couple) High cohesion (highly cohesive) <p>Here's a real-life example using a Library System to illustrate low coupling and high cohesion.</p> <pre><code>graph TD\n  subgraph BookManager\n    A1[Add Book]\n    A2[Remove Book]\n    A3[Search Book]\n    A1 --- A2 --- A3\n  end\n  subgraph UserManager\n    B1[Add User]\n    B2[Remove User]\n    B3[Search User]\n    B1 --- B2 --- B3\n  end\n  subgraph NotificationManager\n    C1[Send Email]\n    C2[Send SMS]\n    C1 --- C2\n  end\n  BookManager -.-&gt; NotificationManager\n  UserManager -.-&gt; NotificationManager</code></pre> <p>High Cohesion - Each module has high cohesion (focused responsibilities):</p> <ul> <li>The <code>BookManager</code> module only manages books (add, remove, search). Low Coupling:</li> <li><code>BookManager</code>, <code>UserManager</code>, and <code>NotificationManager</code> work independently and interact through clear interfaces.</li> </ul> <p>Low Coupling - The modules are loosely coupled:</p> <ul> <li>The dashed arrows show low coupling (modules interact but are not tightly bound).</li> </ul>"},{"location":"docs/python-oop/concepts/","title":"Concepts of Object-Oriented Programming (OOP)","text":""},{"location":"docs/python-oop/concepts/#introduction-to-object-oriented-programming-oop","title":"Introduction to Object-Oriented Programming (OOP)?","text":"<p>Important</p> <p>This section will include all the topics related to Object-Oriented Programming (OOP) in Python. I will cover the following topics in each section.</p> <p>Object-Oriented Programming (OOP) is a programming paradigm that uses \"objects\" to design software. An object will contain attributes (data, properties, variables) and behaviors (methods, functions). You will start by defining classes which will act as blueprints for creating objects.</p> <ul> <li>Attributes: Variables associated with the object that hold data. For example, a <code>Car</code> object might have attributes like <code>color</code>, <code>model</code>, and <code>year</code>.</li> <li>Methods: Functions associated with the object that define its behavior/action. For example, a <code>Car</code> object might have methods like <code>start()</code>, <code>stop()</code>, and <code>accelerate()</code>.</li> </ul>"},{"location":"docs/python-oop/concepts/#classes-and-objects","title":"Classes and Objects","text":"Type Explanation Class A blueprint for creating objects. It defines the attributes and behaviours that the created objects will have. Object An instance of a class. It contains data and methods defined by the class."},{"location":"docs/python-oop/concepts/#four-pillars-of-oop","title":"Four Pillars of OOP","text":"Pillar Explanation Encapsulation Bundle data (attributes) and methods (behaviours) into a single unit (class). It restricts direct access to some of the object's components, which can prevent the accidental modification of data. Mainly hiding internal state and implementation details. Inheritance A mechanism to create a new class (subclasses) that inherits attributes and methods from an existing class (parent classes). It allows for code reuse and establishes a relationship between classes. Polymorphism The ability to use a single interface to represent different underlying data types. It allows methods to do different things based on the object it is acting upon. Also, it allows methods to have the same name but behave differently based on the object type by using method overriding or method overloading. Abstraction Hiding complex internal implementation details and showing only the essential features of the object. It allows the developer to focus on \"what to do\" (focus on what an object does) rather than \"how to do it\" (how it achieves its functionality)."},{"location":"docs/python-oop/concepts/#benefits-of-oop","title":"Benefits of OOP","text":"<p>I just highlight some of the key benefits of Object-Oriented Programming (OOP) below. There are many more benefits, but these are the most important ones that you should know.</p> Benefit Explanation Code Reusability OOP allows for reusing existing code through inheritance, which reduces redundancy and improves maintainability. Modularity Breaks down complex systems into smaller manageable pieces (classes and objects), making it easier to understand, develop, modify, maintain, and debug code. Flexibility and Scalability OOP allows for easier modifications and extensions to existing code. New features can be added with minimal changes to existing code, making it easier to scale applications. Clear structure and enhanced collaboration OOP promotes a clear structure for organizing xode, which makes it easier for teams to work together."},{"location":"docs/python-oop/encapsulation/","title":"Encapsulation","text":""},{"location":"docs/python-oop/encapsulation/#concept","title":"Concept","text":"<p>Bundle data (attributes) and methods (behaviours) into a single unit (class). It restricts direct access to some of the object's components, which can prevent the accidental modification of data. Mainly hiding internal state and implementation details.</p> Type of Encapsulation Description Public Accessible from anywhere, no restrictions. Protected Accessible within the class and its subclasses, but not from outside. Private Accessible only within the class, not from outside or subclasses. <ul> <li> <p>Data Protection</p> <ul> <li>Prevent direct access to sensitive data.</li> <li>Validate data before modification.</li> <li>Maintains data integrity.</li> <li>Controls how data is accessed and modified.</li> </ul> </li> <li> <p>Code Maintainability</p> <ul> <li>Internal implementation can change without affecting external code.</li> <li>External interface remains stable.</li> <li>Reduces dependencies on internal details (Reduces coupling between classes).</li> <li>Makes code easier to maintain and refactor.  </li> </ul> </li> </ul>"},{"location":"docs/python-oop/encapsulation/#implementation","title":"Implementation","text":"<p>Encapsulation can be implemented through private, public, and protected attributes and methods.</p>"},{"location":"docs/python-oop/encapsulation/#get-and-set-methods","title":"Get and Set Methods","text":"Python<pre><code>class BankAccount:\n  def __init__(self, owner, balance, pin):\n    self.owner = owner # Public attribute\n    self._balance = balance  # Protected attribute\n    self.__pin = pin  # Private attribute\n\n  def get_pin(self):\n    return self.__pin\n\n  def set_pin(self, pin):\n    self.__pin = pin\n\n  def deposit(self, amount):\n    if amount &gt; 0:\n      self._balance += amount\n      return True\n    return False\n\n  def withdraw(self, amount):\n    if 0 &lt; amount &lt;= self._balance:\n      self._balance -= amount\n      return True\n    return False\n\n  def get_balance(self):\n    return self._balance\n</code></pre>"},{"location":"docs/python-oop/encapsulation/#property-decorators","title":"Property Decorators","text":"<p>You can use property decorators to create getter and setter methods for encapsulated attributes.</p> Python<pre><code>class BankAccount:\n  def __init__(self, username, password, balance, pin):\n    self.username = username\n    self._balance = balance\n    self.__password = password\n    self.__pin = pin\n\n  @property\n  def balance(self):\n    return self._balance\n\n  @balance.setter\n  def balance(self, amount):\n    if amount &gt;= 0:\n      self._balance = amount\n    else:\n      raise ValueError(\"Balance cannot be negative\")\n</code></pre>"},{"location":"docs/python-oop/inheritance/","title":"Inheritance","text":""},{"location":"docs/python-oop/inheritance/#concept","title":"Concept","text":"<p>A mechanism to create a new class (subclasses) that inherits attributes and methods from an existing class (parent classes). It allows for code reuse and establishes a relationship between classes.</p> Type of Inheritance Description Example Single Inheritance A subclass inherits from one parent class. <code>class Dog(Animal): pass</code> Multiple Inheritance A subclass inherits from multiple parent classes. <code>class FlyingDog(Dog, Bird): pass</code> Multilevel Inheritance A subclass inherits from a parent class, which in turn inherits from another parent class. <code>Animal -&gt; Mammal -&gt; Dog</code> Hierarchical Inheritance Multiple subclasses inherit from a single parent class. <code>Animal -&gt; Dog, Cat, Bird</code> Hybrid Inheritance A combination of two or more types of inheritance. <code>class FlyingDog(Dog, Bird): pass</code> where <code>Dog</code> and <code>Bird</code> are subclasses of <code>Animal</code>."},{"location":"docs/python-oop/inheritance/#implementation","title":"Implementation","text":""},{"location":"docs/python-oop/inheritance/#single-inheritance","title":"Single Inheritance","text":"<pre><code>classDiagram\n  Animal &lt;|-- Dog</code></pre> <p>Python<pre><code>class Animal:\n  def __init__(self, name, age):\n    self.name = name\n    self.age = age\n\n  def speak(self):\n    print(\"Animal speaks\")\n\nclass Dog(Animal):\n  def __init__(self, name, age):\n    super().__init__(name, age)  # Call the parent class constructor\n\n  def speak(self): # method overriding\n    print(\"Woof!\")\n\ndog = Dog(\"Buddy\", 3)\ndog.speak()  # Output: Woof!\nprint(dog.name)  # Output: Buddy\nprint(dog.age)   # Output: 3\nprint(isinstance(dog, Animal))  # Output: True\n</code></pre> - <code>super().__init__(name, age)</code> is used to call the constructor of the parent class (<code>Animal</code>) to initialize the inherited attributes. - <code>isinstance(dog, Animal)</code> checks if <code>dog</code> is an instance of the <code>Animal</code> class or its subclasses.</p>"},{"location":"docs/python-oop/inheritance/#multiple-inheritance","title":"Multiple Inheritance","text":"<pre><code>classDiagram\n  Dog &lt;|-- FlyingDog\n  Bird &lt;|-- FlyingDog</code></pre> <p>When a class inherits from multiple parent classes, the order is determined by the Method Resolution Order (MRO), which follows the C3 linearization algorithm. This ensures a consistent order of method resolution. The MRO can be checked using the <code>__mro__</code> attribute or the <code>mro()</code> method. The MRO is a depth-first, left-to-right traversal of the class hierarchy.</p> Python<pre><code>class Dog:\n  def bark(self):\n    print(\"Woof!\")\n\nclass Bird:\n  def fly(self):\n    print(\"Bird flies\")\n\nclass FlyingDog(Dog, Bird):\n  pass\n\nfd = FlyingDog()\nfd.bark()  # Output: Woof!\nfd.fly()   # Output: Bird flies\n</code></pre> <p>The MRO for this is <code>FlyingDog -&gt; Dog -&gt; Bird</code></p>"},{"location":"docs/python-oop/inheritance/#multilevel-inheritance","title":"Multilevel Inheritance","text":"<pre><code>classDiagram\n  Animal &lt;|-- Mammal\n  Mammal &lt;|-- Dog</code></pre> Python<pre><code>class Animal:\n  def eat(self):\n    print(\"Animal eats\")\n\nclass Mammal(Animal):\n  pass\n\nclass Dog(Mammal):\n  pass\n\ndog = Dog()\ndog.eat()  # Output: Animal eats\n</code></pre>"},{"location":"docs/python-oop/inheritance/#hierarchical-inheritance","title":"Hierarchical Inheritance","text":"<pre><code>classDiagram\n  Animal &lt;|-- Dog\n  Animal &lt;|-- Cat</code></pre> Python<pre><code>class Animal:\n  def sleep(self):\n    print(\"Animal sleeps\")\n\nclass Dog(Animal):\n  pass\n\nclass Cat(Animal):\n  pass\n\ndog = Dog()\ncat = Cat()\ndog.sleep()  # Output: Animal sleeps\ncat.sleep()  # Output: Animal sleeps\n</code></pre>"},{"location":"docs/python-oop/inheritance/#hybrid-inheritance","title":"Hybrid Inheritance","text":"<pre><code>classDiagram\n  Animal &lt;|-- Dog\n  Animal &lt;|-- Bird\n  Dog &lt;|-- FlyingDog\n  Bird &lt;|-- FlyingDog</code></pre> Python<pre><code>class Animal:\n  def move(self):\n    print(\"Animal moves\")\n\nclass Dog(Animal):\n  def bark(self):\n    print(\"Woof!\")\n\nclass Bird(Animal):\n  def fly(self):\n    print(\"Bird flies\")\n\nclass FlyingDog(Dog, Bird):\n  pass\n\nfd = FlyingDog()\nfd.move()  # Output: Animal moves\nfd.bark()  # Output: Woof!\nfd.fly()   # Output: Bird flies\n</code></pre>"},{"location":"docs/python-oop/polymorphism/","title":"Polymorphism","text":""},{"location":"docs/python-oop/polymorphism/#concept","title":"Concept","text":"<p>The ability to use a single interface to represent different underlying data types. It allows methods to do different things based on the object it is acting upon. Also, it allows methods to have the same name but behave differently based on the object type by using method overriding or method overloading.</p> Types of Polymorphism Description Compile-time Polymorphism (Method or operator overloading) The method to be executed is determined at compile time. It allows the same method name to be used with different parameters or types. Runtime Polymorphism (Method Overriding) The method to be executed is determined at runtime. It allows a subclass to provide a specific implementation of a method that is already defined in its superclass."},{"location":"docs/python-oop/polymorphism/#implementation","title":"Implementation","text":""},{"location":"docs/python-oop/polymorphism/#method-overriding","title":"Method Overriding","text":"<p>Based on the below example, the <code>area</code> method is overridden in the <code>Circle</code> and <code>Rectangle</code> classes to provide specific implementations for calculating the area of each shape. The <code>Shape</code> class serves as a base class with a generic <code>area</code> method.</p> Python<pre><code>class Shape:\n  def area(self):\n    pass\n\nclass Circle(Shape):\n  def __init__(self, radius):\n    self.radius = radius\n\n  def area(self):\n    return 3.14 * self.radius ** 2\n\nclass Rectangle(Shape):\n  def __init__(self, length, width):\n    self.length = length\n    self.width = width\n\n  def area(self):\n    return self.length * self.width\n\n# Runtime Polymorphism\nshapes = [Circle(5), Rectangle(4, 6)]\nfor shape in shapes:\n  print(f\"Area: {shape.area()}\") # Calls the overridden method based on the object type\n</code></pre>"},{"location":"docs/python-oop/polymorphism/#method-overloading","title":"Method Overloading","text":"<p>Based on the below example, the <code>add</code> method is overloaded to handle different numbers of parameters. The method can accept either two or three arguments, and it will return the sum accordingly.</p> Python<pre><code>class Calculator:\n    def add(self, a, b):\n      return a + b\n    def add(self, a, b, c):\n      return a + b + c\n\ncalculator = Calculator()\nprint(calculator.add(2, 3))        # Output: 5\nprint(calculator.add(2, 3, 4))      # Output: 9\n</code></pre>"},{"location":"docs/python-oop/polymorphism/#operator-overloading","title":"Operator Overloading","text":"<p>More Information</p> <p>Methods like <code>__add__</code>, <code>__str__</code>, <code>__eq__</code>, etc are called dunder methods (double underscore methods) or magic methods. They allow you to define how operators and built-in functions behave for user-defined classes.</p> <p>Operator overloading allows you to define how operators behave for user-defined classes. For example, you can define how the <code>+</code> operator works for a custom class by implementing the <code>__add__</code> method.</p> Python<pre><code>class Vector:\n  def __init__(self, x, y):\n    self.x = x\n    self.y = y\n\n  def __add__(self, other):\n    return Vector(self.x + other.x, self.y + other.y)\n\n  def __str__(self):\n    return f\"Vector({self.x}, {self.y})\"\n\n  def __eq__(self, other):\n    return self.x == other.x and self.y == other.y\n\nvector1 = Vector(1, 2)\nvector2 = Vector(3, 4)\nresult = vector1 + vector2  # Uses the __add__ method\n\nprint(result)  # Output: Vector(4, 6)\nprint(vector1 == vector2)  # Uses the __eq__ method, Output: False\n</code></pre>"},{"location":"docs/python-oop/uml-class-diagram/","title":"UML Class Diagram","text":""},{"location":"docs/python-oop/uml-class-diagram/#what-is-a-uml-class-diagram","title":"What is a UML Class Diagram?","text":"<p>A UML (Unified Modeling Language) class diagram is a visual representation of classes and their relationships in an object-oriented system. It helps in understanding the structure of the system, showing how classes interact with each other.</p> <ul> <li>Class name</li> <li>Attributes (properties)</li> <li>Methods (functions)</li> <li>Relationships (associations, inheritance, etc.)</li> </ul>"},{"location":"docs/python-oop/uml-class-diagram/#uml-class-notation","title":"UML Class Notation","text":"<pre><code>classDiagram\n  class Car {\n    -brand: String\n    -model: String\n    -year: int\n    +startEngine(): void\n    +stopEngine(): void\n    +drive(speed: int): void\n  }</code></pre> <p>The class notation includes:</p> <ul> <li>Class name</li> <li>Attributes</li> <li>Methods</li> <li>Visibility:<ul> <li><code>+</code> Public</li> <li><code>-</code> Private</li> <li><code>#</code> Protected</li> </ul> </li> </ul>"},{"location":"docs/python-oop/uml-class-diagram/#relationships-between-classes","title":"Relationships between Classes","text":""},{"location":"docs/python-oop/uml-class-diagram/#association","title":"Association","text":"<p>Association represents a bi-directional (two-ways relationship) relationship between two classes where one class uses or interacts with another. It is a one-to-one or one-to-many relationship. It typically means uses, has-a, works with, or belongs to.</p> <ul> <li>Permanent reference between classes. This means one class holds a reference to another class as part of its state or structure, not just temporarily within a method</li> </ul> <p>Example: Suppose a <code>Car</code> has a <code>Engine</code> and <code>Wheel</code>. This is an association because a car object is associated with an engine object and multiple wheel objects.</p> <ul> <li>Each <code>Car</code> has one <code>Engine</code> and each <code>Engine</code> belongs to one <code>Car</code></li> <li>Each <code>Car</code> contains multiple <code>Wheels</code> and each <code>Wheel</code> belongs to one <code>Car</code></li> </ul> <pre><code>classDiagram\n  class Car {\n    -brand: String\n    -model: String\n    -year: int\n    +startEngine(): void\n    +stopEngine(): void\n    +drive(speed: int): void\n  }\n  class Engine {\n    -type: String\n    -horsepower: int\n    +start(): void\n    +stop(): void\n  }\n  class Wheel {\n    -size: int\n    -type: String\n  }\n  Car \"1\" -- \"1\" Engine : has a\n  Car \"1\" -- \"1..*\" Wheel : has</code></pre> <p>Another example is that <code>Library</code> has many <code>Books</code>. The association is bi-directional, meaning the <code>Library</code> contains multiple <code>Books</code>, and each <code>Book</code> belongs to one <code>Library</code>.</p>"},{"location":"docs/python-oop/uml-class-diagram/#directed-association","title":"Directed Association","text":"<p>A directed association is a one-way (unidirectional) relationship where one class points to another, indicating that one class uses or interacts with another.</p> <p>Example: Suppose a <code>Driver</code> drives a <code>Car</code>. The association is directed from <code>Driver</code> to <code>Car</code>, meaning the <code>Driver</code> uses the <code>Car</code>, but this car does not necessarily belong to the <code>Driver</code>.</p> <pre><code>classDiagram\n  class Driver {\n    -name: String\n    +drive(car: Car): void\n  }\n  class Car {\n    -brand: String\n    -model: String\n    -year: int\n    +startEngine(): void\n    +stopEngine(): void\n    +drive(speed: int): void\n  }\n  Driver --&gt; Car : drives</code></pre> <p>Another example is that <code>Teacher</code> teaches this <code>Course</code>. The association is directed from <code>Teacher</code> to <code>Course</code>, meaning the <code>Teacher</code> teaches the <code>Course</code>, but this course does not necessarily belong to the <code>Teacher</code>.</p>"},{"location":"docs/python-oop/uml-class-diagram/#aggregation","title":"Aggregation","text":"<p>Aggregation represents a part-of relationship where one class is a whole and another class is a part. It is a one-to-many relationship where the part can exist independently of the whole.</p> <p>Example: A <code>Team</code> consists of multiple <code>Players</code>. The <code>Team</code> is the whole, and the <code>Players</code> are the parts. If the <code>Team</code> is deleted, the <code>Players</code> can still exist independently.</p> <pre><code>classDiagram\n  class Team {\n    -name: String\n    +addPlayer(player: Player): void\n  }\n  class Player {\n    -name: String\n    -position: String\n  }\n  Team o-- \"1..*\" Player : has</code></pre> <p>Another example is that <code>Department</code> has many <code>Employees</code>. The <code>Department</code> is the whole, and the <code>Employees</code> are the parts. If the <code>Department</code> is deleted, the <code>Employees</code> can still exist independently.</p>"},{"location":"docs/python-oop/uml-class-diagram/#composition","title":"Composition","text":"<p>Composition is a special type of aggregation that represents a strong part-of relationship where the part cannot exist independently of the whole. It is a one-to-many relationship where the part is tightly coupled with the whole. If the whole is deleted, the parts are also deleted.</p> <p>Example: A <code>House</code> consists of multiple <code>Rooms</code>. The <code>House</code> is the whole, and the <code>Rooms</code> are the parts. If the <code>House</code> is destroyed, the <code>Rooms</code> are also destroyed.</p> <pre><code>classDiagram\n  class House {\n    -address: String\n    +addRoom(room: Room): void\n  }\n  class Room {\n    -name: String\n    -area: float\n  }\n  House *-- \"1..*\" Room : composed of</code></pre> <p>Another example is that <code>Book</code> has many <code>Chapters</code>. The <code>Book</code> is the whole, and the <code>Chapters</code> are the parts. If the <code>Book</code> is deleted, the <code>Chapters</code> are also deleted.</p>"},{"location":"docs/python-oop/uml-class-diagram/#generalization-inheritance","title":"Generalization (Inheritance)","text":"<p>Generalization (also known as inheritance) represents a is-a relationship where one class (subclass or child) &amp;&amp; from another class (superclass or parent). The subclass inherits the attributes and methods of the superclass, allowing for code reuse and polymorphism. </p> <p>Example: A <code>Bird</code> is a type of <code>Animal</code>, and a <code>Fish</code> is also a type of <code>Animal</code>. Both <code>Bird</code> and <code>Fish</code> inherit from the <code>Animal</code> class.</p> <pre><code>classDiagram\n  class Animal {\n    -name: String\n    +eat(): void\n    +sleep(): void\n  }\n  class Bird {\n    +fly(): void\n  }\n  class Fish {\n    +swim(): void\n  }\n  Animal &lt;|-- Bird : is a\n  Animal &lt;|-- Fish : is a</code></pre> <p>Another example is that <code>Vehicle</code> is a type of <code>Transport</code>. The <code>Vehicle</code> class inherits from the <code>Transport</code> class, allowing it to reuse its attributes and methods.</p>"},{"location":"docs/python-oop/uml-class-diagram/#realization-interface","title":"Realization (Interface)","text":"<p>Realization represents a contract between a class and an interface. A class that implements an interface must provide concrete implementations for the methods defined in the interface. This allows for polymorphism and code flexibility.</p> <p>Example: A <code>Shape</code> interface defines methods for drawing and calculating the area, and classes like <code>Circle</code> and <code>Rectangle</code> implement this interface.</p> <pre><code>classDiagram\n  class Shape {\n    &lt;&lt;interface&gt;&gt;\n    +draw(): void\n    +area(): float\n  }\n  class Circle {\n    -radius: float\n    +draw(): void\n    +area(): float\n  }\n  class Rectangle {\n    -width: float\n    -height: float\n    +draw(): void\n    +area(): float\n  }\n  Shape &lt;|.. Circle : realizes\n  Shape &lt;|.. Rectangle : realizes</code></pre> <p>Another example is that <code>Vehicle</code> interface defines methods for starting and stopping, and classes like <code>Car</code> and <code>Bike</code> implement this interface.</p>"},{"location":"docs/python-oop/uml-class-diagram/#dependency","title":"Dependency","text":"<p>Dependency represents a depends on relationship where one class depends/relies on another class to function correctly. Meaning that an object of one class might use an object of another class in the code of a method or temporarily within its code. It is considered a weak relationship because the dependent (Client) class is only affected if the supplier class changes. They are not tightly coupled, but changes in the supplier can still impact the client.</p> <ul> <li>If the supplier class changes (for example, its interface, method signatures, or behavior), the client class may need to be updated to work with those changes</li> </ul> <p>More Information</p> <p>You can consider dependency relationship is loosely coupled, as the client class uses the supplier class temporarily and does not maintain a permanent reference to it. This means the classes are not tightly bound together, making the relationship weak and flexible. However, changes in the supplier class (like method signatures) can still require updates in the client class.</p> <p>Example: A <code>ReportGenerator</code> class generates reports and uses a <code>Printer</code> class to print the reports. The <code>ReportGenerator</code> depends on the <code>Printer</code> to perform its function.</p> <ul> <li><code>ReportGenerator</code> - Client</li> <li><code>Printer</code> - Supplier</li> </ul> <pre><code>classDiagram\n  class ReportGenerator {\n    +generate(): void\n    +printReport(printer: Printer): void\n  }\n  class Printer {\n    +print(): void\n  }\n  ReportGenerator ..&gt; Printer</code></pre> <p>Another example is that <code>Person</code> class depends on the <code>Book</code> class to read books. However, the <code>Book</code> class does not depend on the <code>Person</code> class, meaning the <code>Book</code> can exist independently without the <code>Person</code>.</p>"},{"location":"docs/python-oop/uml-class-diagram/#usage-dependency","title":"Usage (Dependency)","text":"<p>More Information</p> <p>Same concept as Dependency, but with a focus on temporary usage.</p> <p>Usage Dependency is a type of dependency where one class (the client) uses another class (the supplier) for a specific purpose, usually within a method or temporarily. The client does not maintain a permanent reference to the supplier.</p> <p>Example: A <code>Mechanic</code> class repairs a <code>Car</code>. The <code>Mechanic</code> uses the <code>Car</code> to perform repairs, but the <code>Car</code> does not depend on the <code>Mechanic</code>.</p> <ul> <li><code>Mechanic</code> - Client</li> <li><code>Car</code> - Supplier</li> </ul> <pre><code>classDiagram\n  class Mechanic {\n    +repair(car: Car): void\n  }\n  class Car {\n    +startEngine(): void\n    +stopEngine(): void\n  }\n  Mechanic ..&gt; Car : uses</code></pre> <p>Another example is that <code>Chef</code> class cooks a <code>Dish</code>. The <code>Chef</code> uses the <code>Dish</code> to prepare a meal, but the <code>Dish</code> does not depend on the <code>Chef</code>.</p>"},{"location":"docs/python-oop/uml-class-diagram/#full-example","title":"Full Example","text":""},{"location":"docs/python-oop/uml-class-diagram/#order-management-system","title":"Order Management System","text":"<pre><code>classDiagram\n    %% Generalization (inheritance)\n    Customer &lt;|-- RegisteredCustomer\n    Customer &lt;|-- GuestCustomer\n\n    %% Realization (interface implementation)\n    PaymentMethod &lt;|.. CreditCard\n    PaymentMethod &lt;|.. PayPal\n\n    %% Association (bidirectional)\n    Customer \"1\" -- \"0..*\" Order : places\n\n    Order \"1\" *-- \"1\" ShippingInfo : ships to\n\n    %% Aggregation (hollow diamond)\n    Order \"1\" o-- \"1..*\" OrderItem : contains\n    OrderItem \"1\" o-- \"1\" Product : refers to\n\n    %% Dependency (dashed arrow)\n    Invoice ..&gt; Order\n\n    %% Usage Dependency (dashed arrow)\n    Order ..&gt; PaymentMethod : uses\n\n    %% Classes and Interfaces\n    class Customer {\n        +id: int\n        +name: string\n        +email: string\n    }\n    class RegisteredCustomer {\n        +username: string\n        +password: string\n    }\n    class GuestCustomer {\n        +guestId: string\n    }\n    class Order {\n        +orderId: int\n        +date: Date\n        +status: string\n    }\n    class OrderItem {\n        +quantity: int\n        +price: float\n    }\n    class Product {\n        +productId: int\n        +name: string\n        +price: float\n    }\n    class ShippingInfo {\n        +address: string\n        +deliveryDate: Date\n    }\n    class Invoice {\n        +invoiceId: int\n        +amount: float\n    }\n    class PaymentMethod {\n        &lt;&lt;interface&gt;&gt;\n        +pay(amount: float): bool\n    }\n    class CreditCard {\n        +cardNumber: string\n        +expiry: Date\n    }\n    class PayPal {\n        +email: string\n    }</code></pre> <ol> <li> <p>Generalization (Inheritance)</p> <ul> <li><code>RegisteredCustomer</code> and <code>GuestCustomer</code> both inherit from <code>Customer</code>.</li> <li>Meaning: Both types of customers share common attributes (like id, name, email) but also have their own specific attributes.</li> </ul> </li> <li> <p>Realization (Interface Implementation)</p> <ul> <li><code>PaymentMethod</code> is an interface.</li> <li><code>CreditCard</code> and <code>PayPal</code> implement the PaymentMethod interface.<ul> <li>Meaning: Both payment types must provide a <code>pay(amount: float): bool</code> method.</li> </ul> </li> </ul> </li> <li> <p>Association</p> <ul> <li><code>Customer \"1\" -- \"0..*\" Order : places</code><ul> <li>Meaning: One customer can place multiple orders; each order is placed by one customer.</li> </ul> </li> </ul> </li> <li> <p>Composition</p> <ul> <li><code>Order \"1\" *-- \"1\" ShippingInfo : ships to</code><ul> <li>Meaning: Each order has one shipping info, and if the order is deleted, its shipping info is also deleted (strong ownership).</li> </ul> </li> </ul> </li> <li> <p>Aggregation</p> <ul> <li><code>Order \"1\" o-- \"1..*\" OrderItem : contains</code><ul> <li>Meaning: An order contains multiple order items, but order items can exist independently of the order.</li> </ul> </li> <li><code>OrderItem \"1\" o-- \"1\" Product : refers to</code><ul> <li>Meaning: Each order item refers to a product, but products exist independently of order items.</li> </ul> </li> </ul> </li> <li> <p>Dependency</p> <ul> <li><code>Invoice ..&gt; Order</code><ul> <li>Meaning: An invoice is generated from an order, but the invoice does not own the order.</li> </ul> </li> </ul> </li> <li> <p>Usage Dependency</p> <ul> <li><code>Order ..&gt; PaymentMethod : uses</code><ul> <li>Meaning: An order uses a payment method to process payment (The <code>Order</code> just needs a <code>PaymentMethod</code> to complete the payment operation), but does not own the payment method.</li> </ul> </li> </ul> </li> </ol>"},{"location":"docs/ssh/host-configuration/","title":"Host Configuration","text":""},{"location":"docs/ssh/host-configuration/#host-specific-configuration-file","title":"Host Specific Configuration file","text":"<p>Create a config filename <code>config</code> in your local <code>~/.ssh</code> directory.</p> Bash<pre><code>nano ~/.ssh/config\n</code></pre> <p>You can define each individual SSH configuration options into this file. You can find all the SSH configuration options from this ssh_config.</p> ~/.ssh/config<pre><code>Host * # all hosts\n  ServerAliveInterval 180\n  StrictHostKeyChecking no\n  UserKnownHostsFile /dev/null\n\nHost &lt;remove-alias&gt;\n  HostName &lt;remote-host/ipaddress&gt;\n  Port &lt;port-number&gt;\n  User &lt;username&gt;\n\nHost &lt;remote-alias&gt;\n  HostName &lt;remote-host/ipaddress&gt;\n  Port &lt;port-number&gt;\n\n# sample\nHost server1\n  HostName 192.168.0.1\n  Port 22\n  User karchunt\n</code></pre> Declaration Description Host * All hosts Host <code>remote-alias</code> You can name <code>remote-alias</code> whatever you want ServerAliveInterval If set 180, then every 3 minutes, send a packet to the server to let it know not to close the connection StrictHostKeyChecking If set \"no\", it will disable host checking and it will auto-add new hosts to the <code>known_hosts</code> file (fingerprint) directly UserKnownHostsFile Not warn on new or changed hosts HostName remove host name or IP Address Port Port number to access User Username used to access"},{"location":"docs/ssh/local-tunneling/","title":"Local Tunneling","text":"More Information <p>A Visual Guide to SSH Tunnels: Local and Remote Port Forwarding</p>"},{"location":"docs/ssh/local-tunneling/#local-tunneling-to-a-server","title":"Local tunneling to a server","text":"<p>Traffic between the localhost and remote host can be tunneled via SSH connections. To establish a local tunnel on your remote server, use <code>-L</code> parameter when connecting and must provide</p> <ul> <li>Local port for accessing the tunneled connection</li> <li>Remote host IP/name</li> <li>Remote host port</li> </ul> <p>For general usage, connect to <code>10.0.0.12</code> on port 80 on your remote host, then your local machine is able to ping or make the connection to the remote host through port 8080.</p> Bash<pre><code>ssh -f -N -L &lt;local-port&gt;:&lt;remote-host-ip-address/name&gt;:&lt;remote-port&gt; &lt;username&gt;@&lt;host&gt;\n# example\nssh -L 8080:10.0.0.12:80 username@host\n</code></pre> <p>Now if you go to your browser/curl to <code>localhost:8080</code>, you are able to see the content that hosted at <code>10.0.0.12:80</code>.</p> Parameters Description -f let SSH go into the background before executing -N does not open a shell or execute a program on the remote side -L establish a local tunnel to your remote server <p>If you want to terminate the background connection, you have to find the PID and kill it.</p> Bash<pre><code>ps aux | grep &lt;local-port&gt;\nkill &lt;process-id&gt;\n</code></pre> Bash<pre><code>Output\n1001      5965  0.0  0.0  48168  1136 ?        Ss   12:28   0:00 ssh -f -N -L 8888:your_domain:80 username@remote_host\n1001      6113  0.0  0.0  13648   952 pts/2    S+   12:37   0:00 grep --colour=auto 8888\n</code></pre> Bash<pre><code>kill 5965\n</code></pre>"},{"location":"docs/ssh/local-tunneling/#local-tunneling-local-network","title":"Local tunneling local network","text":"<p>You can tunnel remote host local network to your local. Here is the animation diagram of the local network web server and common SSH server.</p> <p></p> Bash<pre><code>ssh -L 8080:localhost:8080 user@server\n</code></pre>"},{"location":"docs/ssh/local-tunneling/#local-tunneling-private-network","title":"Local tunneling private network","text":"<p>You can tunnnel remote host private network to your local. Here is the animation diagram of the private network web server and common SSH server.</p> <p></p> Bash<pre><code>ssh -L &lt;local-port&gt;:&lt;server-ip-address&gt;:&lt;server-port&gt; &lt;username&gt;@&lt;bastion-server-ip-address&gt;\nssh -L 8080:10.0.0.12:8080 user@bastion\n</code></pre>"},{"location":"docs/ssh/play-with-ssh-keys/","title":"Play with SSH Keys","text":""},{"location":"docs/ssh/play-with-ssh-keys/#generating-an-ssh-key-pair","title":"Generating an SSH Key Pair","text":"<p>Your first step should be creating a new SSH key pair on your computer, then you can connect without a password to a remote server.</p> Bash<pre><code># You can leave those settings as default by pressing ENTER\nssh-keygen\nssh-keygen -t dsa -C \"Comment\" -b 4096\n\n# make sure your private key file exists\nssh-keygen -p # remove or change passphrase on private key\nssh-keygen -l # display the SSH key fingerprint\n</code></pre> <p>For your information, private key's passphrase is just to secure the private key, so that no one will gain access to the remote server even they have your private key, but you have to enter your private key's passphrase everytime if you want to initiate a SSH connection, but this can avoid by using SSH agent.</p> <p>It will generate <code>id_rsa</code> and <code>id_rsa.pub</code> key file to <code>/home/&lt;username&gt;/.ssh</code> hidden directory.</p> <ul> <li><code>id_rsa</code> = private key</li> <li><code>id_rsa.pub</code> = public key</li> </ul>"},{"location":"docs/ssh/play-with-ssh-keys/#optional-parameters","title":"Optional Parameters","text":"Parameters Description Example -t Type of cryptographic algorithms. Default is RSA. rsa, dsa, ecdsa, ed25519 -C Comment simple comment -b The number of bits, default is 2048 bits 4096 -p Removing or changing passphrase on private key (make sure private key file exists) Your password or leave it empty -l Displaying the SSH key fingerprint (make sure private key file exists) -"},{"location":"docs/ssh/play-with-ssh-keys/#copy-the-public-ssh-key-to-the-server","title":"Copy the public SSH key to the server","text":"<p>You can authenticate yourself to the server without a password (passwordless), but you have to copy your public key to the server. There are multiple ways to do it.</p>"},{"location":"docs/ssh/play-with-ssh-keys/#using-ssh-copy-id-command","title":"Using <code>ssh-copy-id</code> command","text":"Bash<pre><code>ssh-copy-id &lt;username&gt;@&lt;remote-server-ip-address/name&gt;\n\n# You can specify the public key through through \"-i\" option\nssh-copy-id -i &lt;public-key-path&gt; &lt;username&gt;@&lt;remote-server-ip-address/name&gt;\n</code></pre> <p>After you type the remote server password, it will copy your public key from your local file <code>~/.ssh/id_rsa.pub</code> to remote server <code>~/.ssh/authorized_keys</code> file.</p>"},{"location":"docs/ssh/play-with-ssh-keys/#manually-copy-ssh-public-key-from-local-to-a-server","title":"Manually copy SSH public key from local to a server","text":"Bash<pre><code># You can copy your local public key through any methods you like\ncat ~/.ssh/id_rsa.pub\nmkdir -p ~/.ssh\necho \"&lt;public_key&gt;\" &gt;&gt; ~/.ssh/authorized_keys\n\n# combination\ncat ~/.ssh/id_rsa.pub | ssh &lt;username&gt;@&lt;remote-server-ip-address/name&gt; \"mkdir -p ~/.ssh &amp;&amp; cat &gt;&gt; ~/.ssh/authorized_keys\"\n</code></pre>"},{"location":"docs/ssh/play-with-ssh-keys/#using-an-ssh-agent-to-avoid-typing-your-private-key-passphrase","title":"Using an SSH agent to avoid typing your private key passphrase","text":"<p>Assume you had set your private SSH key with a passphrase, but you want to eliminate the typing of the private key passphrase. SSH Agent comes into the play to solve this kind of problem.</p> <p>Once the passphrase is entered for the first time, the SSH agent will store your private key to the agent so you don't have to reenter it again.</p> Bash<pre><code># start SSH agent\neval $(ssh-agent)\n\nssh-add # add your private key to agent, default to id_rsa\nssh-add &lt;private-key-path&gt; # You can specify the private key path\n</code></pre>"},{"location":"docs/ssh/play-with-ssh-keys/#forward-ssh-credentials-to-use-on-a-server","title":"Forward SSH credentials to use on a server","text":"<p>In order to connect to one server without a password from within another, you must forward the SSH key information.</p> <p>Before you proceed, you need to make sure your SSH agent starts and your SSH key is added to the agent (ssh-add).</p> Bash<pre><code># forwards your credentials to the server for this session\nssh -A &lt;username&gt;@&lt;remote-server-ip-address/name&gt;\n</code></pre> <p>This will allow you to SSH into any other host that your SSH key has permission to access because the server that you are connected to right now will \"know\" your private SSH key on this server.</p>"},{"location":"docs/ssh/remote-server-configuration/","title":"Remote Server Connection","text":""},{"location":"docs/ssh/remote-server-configuration/#connecting-to-a-remote-server","title":"Connecting to a remote server","text":"Bash<pre><code>ssh &lt;remote-host-ip-address/name&gt; # Your local machine username is same as on the remote server\nssh &lt;username&gt;@&lt;remote-host-ip-address/name&gt; # specify your username if it is different\nssh -p &lt;port-number&gt; &lt;username&gt;@&lt;remote-host-ip-address/name&gt; # specify different port number\nssh -v &lt;username&gt;@&lt;remote-host-ip-address/name&gt; # get the verbose information\n\n# running a single command on a remote host instead of spawning a shell session\n# After the command is completed running, the connection will straightaway close\nssh &lt;username&gt;@&lt;remote-host-ip-address/name&gt; &lt;command-to-run&gt;\nssh &lt;username&gt;@&lt;remote-host-ip-address/name&gt; ls -la\n</code></pre> Parameters Description Example -p Port Number 2222 -v More information (Verbose) - <p>When you SSH to a server, it will prompt you whether you want to connect by showing the fingerprint. It will save the fingerprint to known_hosts file.</p> Info <p>To simplify the connection process, you can create a Host specific configuration file.</p>"},{"location":"docs/ssh/remote-server-configuration/#known_hosts","title":"known_hosts","text":"Info <p>It will generate a <code>known_hosts</code> file in your local <code>~/.ssh</code> directory.</p> <p>A fingerprint will be displayed when you SSH to a server. If you put \"yes\", the system will save the fingerprint to your local <code>~/.ssh/known_hosts</code> file, so you won't have to enter the same thing again next time.</p> <p>As a result, it can help to prevent the man-in-the-middle-attack.</p>"},{"location":"docs/ssh/remote-tunneling/","title":"Remote Tunneling","text":""},{"location":"docs/ssh/remote-tunneling/#remote-tunneling-to-a-server","title":"Remote tunneling to a server","text":"Info <p>The concept is same as local tunneling.</p> <p>A remote tunnel makes a connection to a remote server. The remote port must be specified when creating the remote tunnel. As a result, the remote computer able to connect or access your localhost. To establish a remote tunnel to your remote server, use <code>-R</code> parameter when connecting and must provide</p> <ul> <li>Remote port for accessing the tunneled connection</li> <li>Local host IP/name</li> <li>Local host port</li> </ul> <p>For general usage, connect to <code>localhost</code> on port 8080 on our local computer, then your remote machine is able to ping or make the connetion to the localhost through port 8080.</p> Bash<pre><code>ssh -f -N -R &lt;remote-port&gt;:&lt;local-host-ip-address/name&gt;:&lt;local-port&gt; &lt;username&gt;@&lt;remote-host-ip-address/name/gateway&gt;\n# example\nssh -R 8080:localhost:8080 user@remote-host-ip-address/gateway\n</code></pre> <p>Now if you go to your remote browser/curl to <code>localhost:8080</code>, you are able to see the content that hosted at <code>localhost:80</code>.</p> Parameters Description -R establish a remote tunnel to your remote server"},{"location":"docs/ssh/remote-tunneling/#remote-tunneling-local-network","title":"Remote tunneling local network","text":"<p>You can tunnel your local network to remote host. Here is the animation diagram of the local network web server and common SSH server.</p> <p>Here is the animation diagram of the local network web server and common SSH server.</p> Bash<pre><code>ssh -R 8080:localhost:8080 user@remote-host-ip-address/gateway\nssh -R 0.0.0.0:8080:localhost:8080 user@remote-host-ip-address/gateway\n</code></pre>"},{"location":"docs/ssh/remote-tunneling/#remote-tunneling-private-network","title":"Remote tunneling private network","text":"<p>You can tunnel your local private network to remote host. Here is the animation diagram of the private network web server and common SSH server.</p> Bash<pre><code>ssh -R 8080:&lt;local-server-ip-address&gt;:8080 user@remote-host-ip-address/gateway\nssh -R 0.0.0.0:8080:&lt;local-server-ip-address&gt;:8080 user@remote-host-ip-address/gateway\n</code></pre>"},{"location":"docs/ssh/server-configuration/","title":"Server Configuration","text":""},{"location":"docs/ssh/server-configuration/#disable-password-authentication","title":"Disable password authentication","text":"<p>If you have set up your SSH keys, then my advice is to disable password authentication, as passwordless is more secure than password authentication.</p> <ol> <li> <p>Go to your remote server, find, and edit <code>/etc/ssh/sshd_config</code>.</p> Bash<pre><code>sudo nano /etc/ssh/sshd_config\n</code></pre> </li> <li> <p>Search for <code>PasswordAuthentication</code> text and set it to \"no\".</p> sshd_config<pre><code>PasswordAuthentication no\n</code></pre> </li> <li> <p>Restart the SSH service</p> Bash<pre><code>sudo service ssh restart\n</code></pre> </li> </ol>"},{"location":"docs/ssh/server-configuration/#change-ssh-daemon-runslistens-on-port","title":"Change SSH Daemon runs/listens on port","text":"<p>By default, SSH Daemon runs/listens on port 22. You can change it as well.</p> <ol> <li> <p>Go to your remote server, find, and open <code>/etc/ssh/sshd_config</code>.</p> Bash<pre><code>sudo nano /etc/ssh/sshd_config\n</code></pre> </li> <li> <p>Search for <code>Port</code> text and edit it based on your needs</p> sshd_config<pre><code>#Port 22\nPort 1234\n</code></pre> </li> <li> <p>Restart the SSH service     Bash<pre><code>sudo service ssh restart\n</code></pre></p> </li> </ol>"},{"location":"docs/ssh/server-configuration/#limit-authenticate-users-to-login","title":"Limit authenticate users to login","text":"<ol> <li> <p>Go to your remote server, find, and open <code>/etc/ssh/sshd_config</code>.</p> Bash<pre><code>sudo nano /etc/ssh/sshd_config\n</code></pre> </li> <li> <p>Search for <code>AllowUsers</code> or <code>AllowGroups</code>, if not found, then create it anywhere. Either one should be fine, or you want to implement both too.</p> sshd_config<pre><code>AllowUsers user1 user2 user3\nAllowGroups groupname\n</code></pre> </li> <li> <p>Restart the SSH service</p> Bash<pre><code>sudo service ssh restart\n</code></pre> </li> </ol>"},{"location":"docs/ssh/server-configuration/#disable-root-login","title":"Disable root login","text":"<p>It is a good practice to disable root login</p> <ol> <li> <p>Go to your remote server, find, and open <code>/etc/ssh/sshd_config</code>.</p> Bash<pre><code>sudo nano /etc/ssh/sshd_config\n</code></pre> </li> <li> <p>Search for <code>PermitRootLogin</code> text and set it to \"no\".</p> sshd_config<pre><code>PermitRootLogin no\n</code></pre> </li> <li> <p>Restart the SSH service</p> Bash<pre><code>sudo service ssh restart\n</code></pre> </li> </ol>"},{"location":"docs/ssh/ssh-installation/","title":"SSH Installation","text":"More Information <p>For Windows Users: Enable WSL and SSH into Windows with Bash. Get started with OpenSSH for Windows</p>"},{"location":"docs/ssh/ssh-installation/#install-ssh-on-linux","title":"Install SSH on Linux","text":"Bash<pre><code>sudo apt-get update\nsudo apt-get install -y openssh-client openssh-server\nsudo systemctl enable ssh\nsudo systemctl start ssh\n\n# you can trigger either one will do\nsudo systemctl status ssh\nsudo systemctl status sshd\n</code></pre> <p>You can find all the files under <code>/home/&lt;username&gt;/.ssh</code></p>"},{"location":"docs/ssh/ssh-overview/","title":"SSH Overview","text":""},{"location":"docs/ssh/ssh-overview/#what-is-ssh","title":"What is SSH?","text":"<p>SSH stands for Secure Shell (SSH) Protocol that is mainly used to connect to a Linux server remotely. Basically, it gives you the opportunity to access a server/computer over an unsecured network.</p>"},{"location":"docs/ssh/ssh-overview/#how-ssh-works","title":"How SSH Works?","text":"<p>Client-server architecture is used to implement SSH connections. The remote machine (Server) must be running SSH daemon (The heart of SSH). In SSH, a specific network port (22) is used for connection requests, authentication, and login into shell sessions when the user provides the correct credentials.</p>"},{"location":"docs/ssh/ssh-overview/#how-ssh-authenticate-users","title":"How SSH Authenticate Users?","text":"<p>We can use passwords or SSH keys to authenticate the right users to login.</p>"},{"location":"docs/ssh/ssh-overview/#password","title":"Password","text":"<p>It is not recommended to use passwords to log in, as when the malicious users or bots will keep repeatedly trying to authenticate their accounts, it might potentially lead to security compromises although password logins are encrypted. Therefore, it is less secure compared to SSH Keys.</p>"},{"location":"docs/ssh/ssh-overview/#ssh-keys","title":"SSH Keys","text":"<p>The SSH keys consist of both public and private cryptographic keys. For the public key, the user can share with anyone freely without any concerns, while the private key must be stored in a secure way and cannot be exposed to anyone.</p> <p></p> <p>Steps to authenticate;</p> <ol> <li>The clients must have an SSH key pair (public and private) on their local computers.</li> <li>The local client's public key must be copied to the user's home directory at <code>~/.ssh/authorized_keys</code> on the remote server.</li> <li>When the client connects to the host/server, it will inform the server which public key to use to authenticate.</li> <li>The server will validate the public key from <code>~/.ssh/authorized_keys</code> file, if it is valid, then it will generate a random string and encrypt it using the public key.</li> <li>The server will send the encrypted message to the client to test whether the client has the associated private key.</li> <li>After receiving an encrypted message from the server, the client will use the client's private key to decrypt and send the decrypted information back to the server.</li> <li>Lastly,the client is able to log into shell sessions when the server determines that the client has the associated private key by validating the decrypted information is correct.</li> </ol>"},{"location":"docs/taskfile/basic/","title":"Basic","text":""},{"location":"docs/taskfile/basic/#how-to-get-started-with-taskfile","title":"How to get started with Taskfile","text":""},{"location":"docs/taskfile/basic/#step-1-create-your-taskfile","title":"Step 1: Create your Taskfile","text":"<p>There are multiple ways to create a Taskfile.</p> <ul> <li>Create a Taskfile in the current directory</li> <li>Create a Taskfile in a specific directory</li> <li>Create a Taskfile with a custom filename</li> <li>Manually create a Taskfile</li> </ul> Bash<pre><code># This will create a Taskfile.yml in the current directory\ntask --init\n\n# This will create a Taskfile.yml in the specified directory\n# Make sure the directory exists\ntask --init ./directory-path\n\n# This will create a custom Taskfile\ntask --init CustomTaskfile.yml\n\n# Manually create a Taskfile\ntouch Taskfile.yml\n</code></pre>"},{"location":"docs/taskfile/basic/#step-2-open-the-taskfile-in-your-favorite-text-editor","title":"Step 2: Open the Taskfile in your favorite text editor.","text":"<p>The Taskfile will look like this:</p> Taskfile.yml<pre><code>version: '3'\n\nvars:\n  GREETING: Hello, World!\n\ntasks:\n  default:\n    cmds:\n      - echo \"{{.GREETING}}\"\n    silent: true\n</code></pre> <ul> <li>The task name is <code>default</code>.</li> <li>You can define more tasks by adding them to the <code>task</code> section.</li> </ul> <p>Here is a breakdown of the Taskfile:</p> Field Description <code>version</code> The version of the Taskfile format to run <code>vars</code> Variables that can be used in the Taskfile <code>tasks</code> The tasks to run <code>cmds</code> The commands to run <code>silent</code> If true, The tasks metadata will not be printed, it will only print the output of the command."},{"location":"docs/taskfile/basic/#step-3-add-a-new-task-to-the-taskfile","title":"Step 3: Add a new task to the Taskfile","text":"Taskfile.yml<pre><code>version: '3'\n\nvars:\n  GREETING: Hello, World!\n\ntasks:\n  default:\n    cmds:\n      - echo \"{{.GREETING}}\"\n    silent: true\n  hello:\n    cmds: # commands to run\n      - echo 'Hello World from Task!' &gt; output.txt\n</code></pre>"},{"location":"docs/taskfile/basic/#step-4-call-a-task","title":"Step 4: Call a task","text":"<p>More Info</p> <p>You can use <code>task --help</code> to see all available options and flags.</p> <p>When you run the <code>task</code> command, it will look for a Taskfile in the current directory or in the specified directory.</p> Bash<pre><code>task &lt;task-name&gt;\n\n# run the task in the current directory\ntask default\n\n# run the task in the specified directory\ntask --dir ./directory-path default\n\n# run the task with a custom Taskfile\ntask --taskfile CustomTaskfile.yml default\n</code></pre> <p>In this case, we will run the <code>hello</code> task that we just added to the Taskfile.</p> Bash<pre><code>task hello\n</code></pre>"},{"location":"docs/taskfile/basic/#step-5-verify-the-output-by-checking-the-contents-of-outputtxt","title":"Step 5: Verify the output by checking the contents of <code>output.txt</code>","text":"Bash<pre><code>cat output.txt\n</code></pre> <p>You should see the following output:</p> output.txt<pre><code>Hello World from Task!\n</code></pre>"},{"location":"docs/taskfile/display-taskfile-summary/","title":"Display Taskfile Summary","text":"<p>You can display a summary of your Taskfile using the <code>task</code> command with the <code>--summary</code> flag. This will provide you with an overview of the tasks defined in your Taskfile, including their names, descriptions, and any dependencies.</p> <p>If a summary is missing, the description will be displayed instead. If neither is available, a warning is printed.</p> Taskfile.yml<pre><code>version: '3'\ntasks:\n  deploy:\n    desc: Deploy the application\n    deps: [build]\n    summary: |\n      This task deploys the application after building it.\n\n      It ensures that all necessary components are in place before deployment.\n    cmds:\n      - echo \"Deploying application...\"\n  build:\n    desc: Build the application\n    cmds:\n      - echo \"Building application...\"\n  test:\n    cmds:\n      - echo \"Running tests...\"\n</code></pre> Demo and Output<pre><code># deploy task summary\nubuntu@touted-mite:~$ task --summary deploy\ntask: deploy\n\nThis task deploys the application after building it.\n\nIt ensures that all necessary components are in place before deployment.\n\ndependencies:\n - build\n\ncommands:\n - echo \"Deploying application...\"\n\n# build task summary\nubuntu@touted-mite:~$ task --summary build\ntask: build\n\nBuild the application\n\ncommands:\n - echo \"Building application...\"\n\n# test task summary\nubuntu@touted-mite:~$ task --summary test\ntask: test\n\n(task does not have description or summary)\n\ncommands:\n - echo \"Running tests...\"\n</code></pre>"},{"location":"docs/taskfile/dry-run-mode/","title":"Dry Run Mode","text":"<p>You can use the <code>--dry</code> flag to preview the execution of tasks without actually running them. This is useful for debugging/verifying what actions will be taken before executing a task.</p> Bash<pre><code>task --dry &lt;task&gt;\n</code></pre>"},{"location":"docs/taskfile/environment-variable/","title":"Environment Variables","text":""},{"location":"docs/taskfile/environment-variable/#environment-variable-in-taskfile","title":"Environment variable in Taskfile","text":"<p>There are two ways to declare environment variables in Taskfile:</p> <ol> <li>Using a <code>.env</code> file via the <code>dotnet</code> option in the Taskfile.</li> <li>Using <code>env</code> option in the Taskfile.</li> </ol> <p> If you want to override the value of an environment variable, you have to reference it using the <code>$ENV_NAME</code> format in the Taskfile, not <code>{{.ENV_NAME}}</code>. You can refer below for more details.</p> <ul> <li><code>export STAGE=dev</code> will override the value of <code>STAGE</code> defined in <code>.env</code> file or <code>env</code> option in the Taskfile.</li> </ul>"},{"location":"docs/taskfile/environment-variable/#using-env-file","title":"Using .env file","text":"<p>Info</p> <p>You can specify <code>dotnet</code> option in global or task level.</p> Text Only<pre><code>/home/&lt;user&gt;/\n\u251c\u2500\u2500 Taskfile.yml\n\u251c\u2500\u2500 .env\n\u2514\u2500\u2500 test/\n    \u2514\u2500\u2500 .env\n</code></pre> <p>Assume you have the following content:</p> ~/.envtest/.env Text Only<pre><code>STAGE=test\n</code></pre> Text Only<pre><code>ENDPOINT=testing.com\n</code></pre> Taskfile.yml<pre><code>version: '3'\n\ndotenv: # global/system environment variables for all tasks\n  - '.env'\n\nenv:\n  MYENV: test\n\ntasks:\n  hello:\n    dotenv: ['{{ .MYENV }}/.env', '{{ .HOME }}/.env']\n    cmds:\n      - 'echo \"Endpoint is: $ENDPOINT, Stage is: $STAGE\"'\n</code></pre> <ul> <li><code>.HOME</code> is a magic variable that refers to the home directory of the user.</li> <li><code>export STAGE=dev</code>, if set explicitly in the environment, it will override the value of <code>STAGE</code> defined in <code>.env</code> file.</li> <li>but if you set <code>export MYENV=prod</code>, it will not override the value of <code>MYENV</code> defined in Taskfile.</li> </ul> demo and output<pre><code>ubuntu@touted-mite:~$ task hello \ntask: [hello] echo \"Endpoint is: $ENDPOINT, Stage is: $STAGE\"\nEndpoint is: testing.com, Stage is: test\n\n# Override the value of STAGE\nubuntu@touted-mite:~$ export STAGE=dev\nubuntu@touted-mite:~$ task hello \ntask: [hello] echo \"Endpoint is: $ENDPOINT, Stage is: $STAGE\"\nEndpoint is: testing.com, Stage is: dev\n\n# Try to override MYENV but it will not override\nubuntu@touted-mite:~$ export MYENV=dev\nubuntu@touted-mite:~$ task hello \ntask: [hello] echo \"Endpoint is: $ENDPOINT, Stage is: $STAGE\"\nEndpoint is: testing.com, Stage is: dev\n</code></pre>"},{"location":"docs/taskfile/environment-variable/#using-env-option","title":"Using env option","text":"Taskfile.yml<pre><code>version: '3'\n\nenv: # global/system environment variables for all tasks\n  HOME: /root\n\ntasks:\n  msg:\n    env:\n      MESSAGE: Hello\n    cmds:\n      - 'echo \"Message is: $MESSAGE\"'\n      - 'echo \"Home directory is $HOME\"'\n</code></pre>"},{"location":"docs/taskfile/environment-variable/#override-dotenv-variable-using-env-option","title":"Override dotenv variable using env option","text":".env<pre><code>ENDPOINT=testing.com\n</code></pre> Taskfile.yml<pre><code>version: '3'\n\ntasks:\n  hello:\n    dotenv:\n      - '.env'\n    env:\n      ENDPOINT: dev.com # Override the value of ENDPOINT\n    cmds:\n      - 'echo \"Endpoint is: $ENDPOINT\"'\n</code></pre> demo and output<pre><code>ubuntu@touted-mite:~$ task hello \ntask: [hello] echo \"Endpoint is: $ENDPOINT\"\nEndpoint is: dev.com\n</code></pre>"},{"location":"docs/taskfile/forward-cli-arguments-to-cmd/","title":"Forward CLI Arguments to a Command","text":"<p>You can forward CLI arguments to a command in Taskfile by using the <code>--</code> syntax. This allows you to pass additional arguments directly to the command being executed.</p> Taskfile.yaml<pre><code>version: '3'\ntasks:\n  example:\n    cmds:\n      - npm {{.CLI_ARGS}}\n</code></pre> Bash<pre><code>task example -- install\n</code></pre>"},{"location":"docs/taskfile/if-else-condition/","title":"If Else Condition","text":"<p>You can use if-else conditions in Taskfile to control the execution of tasks based on certain conditions. This allows you to create more dynamic and flexible task definitions.</p> Taskfile.yaml<pre><code>version: '3'\ntasks:\n  example:\n    summary: |\n      An example task that uses if-else conditions\n    vars:\n      CONDITION: true\n    cmds:\n      - echo \"Starting task...\"\n      - |\n        {{ if .CONDITION }}\n          echo \"Condition is true, executing this block\"\n        {{ else }}\n          echo \"Condition is false, executing this block\"\n        {{ end }}\n      - echo \"Task completed\"\n</code></pre> Demo and Output<pre><code>ubuntu@touted-mite:~$ task example \ntask: [example] echo \"Starting task...\"\nStarting task...\ntask: [example] \n  echo \"Condition is true, executing this block\"\n\n\nCondition is true, executing this block\ntask: [example] echo \"Task completed\"\nTask completed\n</code></pre>"},{"location":"docs/taskfile/ignore-errors/","title":"Ignore Errors","text":"<p>You can use the <code>ignore_error: true</code> attribute in your Taskfile to ignore errors during task execution. This allows subsequent tasks to continue running even if a previous task fails.</p> Taskfile.yaml<pre><code>version: '3'\ntasks:\n  lint:\n    internal: true\n    ignore_error: true\n    cmds:\n      - exit 1\n  deploy:\n    cmds:\n      - task: lint\n      - echo \"This will run even if lint fails\"\n</code></pre> Demo and Output<pre><code>ubuntu@touted-mite:~$ task deploy \ntask: [lint] exit 1\ntask: [deploy] echo \"This will run even if lint fails\"\nThis will run even if lint fails\n</code></pre>"},{"location":"docs/taskfile/include-other-taskfile/","title":"Include Other Taskfiles","text":""},{"location":"docs/taskfile/include-other-taskfile/#including-other-taskfiles","title":"Including Other Taskfiles","text":"<p>This feature allows you to include other Taskfiles in your main Taskfile, which is useful for modularizing and reusing tasks across different projects or directories. What you need to do is just using the <code>includes</code> keyword in your Taskfile.</p> <p>Here is the scenario, in your root directory, you have a <code>Taskfile.yml</code> that need to include other Taskfiles from <code>app</code> directory and <code>docker-task.yml</code> file.</p> Text Only<pre><code>app/\n\u2502   Taskfile.yml\nTaskfile.yml\ndocker-task.yml\n</code></pre> app/Taskfile.ymldocker-task.yml YAML<pre><code>version: '3'\ntasks:\n  get-app-version:\n    desc: \"Get the version of the app\"\n    cmds:\n      - 'echo \"App version is 1.0.0\"'\n</code></pre> YAML<pre><code>version: '3'\ntasks:\n  get-docker-version:\n    desc: \"Get the version of the docker\"\n    cmds:\n      - docker --version\n</code></pre> <p>~/Taskfile.yml<pre><code>version: '3'\n\nincludes:\n  # You can provide any namespace you want\n  app: ./app # this will look for ./app/Taskfile.yml\n  docker: ./docker-task.yml\n</code></pre> In this case, you can run the <code>&lt;namespace&gt;:&lt;task&gt;</code> to execute the tasks from the included Taskfiles.</p> Demo and output<pre><code>ubuntu@touted-mite:~$ task app:get-app-version \ntask: [app:get-app-version] echo \"App version is 1.0.0\"\nApp version is 1.0.0\n\nubuntu@touted-mite:~$ task docker:get-docker-version \ntask: [docker:get-docker-version] docker --version\nDocker version 28.1.1, build 4eba377\n</code></pre>"},{"location":"docs/taskfile/include-other-taskfile/#include-other-taskfile-tasks-in-tasks-and-variables-of-included-task","title":"Include other Taskfile tasks in tasks and variables of included Task","text":"<p>You can also include other Taskfile tasks in your Taskfile tasks. This is useful when you want to reuse tasks from other Taskfiles without duplicating the code.</p> Text Only<pre><code>app/\n\u2502   Taskfile.yml\nTaskfile.yml\ndocker-task.yml\n</code></pre> app/Taskfile.ymldocker-task.yml YAML<pre><code>version: '3'\ntasks:\n  get-app-version:\n    desc: \"Get the version of the app\"\n    cmds:\n      - 'echo \"App version is 1.0.0\"'\n</code></pre> YAML<pre><code>version: '3'\ntasks:\n  get-docker-version:\n    desc: \"Get the version of the docker\"\n    cmds:\n      - docker --version\n  build-docker-image:\n    cmds:\n      - docker build -t {{.NAME}} {{.SRC_DIR}}\n</code></pre> ~/Taskfile.yml<pre><code>version: '3'\nincludes:\n  app: ./app\n  docker: ./docker-task.yml\ntasks:\n  app:build:\n    cmds:\n      - task: docker:build-docker-image\n        vars:\n          SRC_DIR: \"app\"\n          NAME: \"my-app\"\n</code></pre> <ul> <li>You can use <code>default</code> keyword to set the default task for the included Taskfile, then you can override the variable values in the <code>vars</code> keyword. Refer here</li> </ul>"},{"location":"docs/taskfile/include-other-taskfile/#os-specific-taskfiles","title":"OS-Specific Taskfiles","text":"<p>You can also include Taskfiles based on the operating system by using special variable.</p> Taskfile.yml<pre><code>version: '3'\nincludes:\n  build: ./Taskfile_{{OS}}.yml\n</code></pre>"},{"location":"docs/taskfile/include-other-taskfile/#include-other-taskfile-to-run-on-different-directory","title":"Include other taskfile to run on different directory","text":"<p>Now, we know that included Taskfiles are run in the current directory, even the Taskfile is included from another directory. But you can actually run the Taskfile tasks in another directory by using the <code>dir</code> keyword.</p> Text Only<pre><code>app/\n\u2502   Taskfile.yml\napp1/\nTaskfile.yml\n</code></pre> app/Taskfile.yml<pre><code>version: '3'\ntasks:\n  get-app-version:\n    desc: \"Get the version of the app\"\n    cmds:\n      - 'echo \"App version is 1.0.0\"'\n      - pwd\n</code></pre> ~/Taskfile.yml<pre><code>version: '3'\nincludes:\n  app:\n    taskfile: ./app/Taskfile.yml\n    dir: ./app1 # this will run the included Taskfile in the ./app1 directory\n</code></pre> <ul> <li>If the directory does not exist, Task will create it.</li> </ul> Demo and output<pre><code>ubuntu@touted-mite:~$ task app:get-app-version \ntask: [app:get-app-version] echo \"App version is 1.0.0\"\nApp version is 1.0.0\ntask: [app:get-app-version] pwd\n/home/ubuntu/app1\n</code></pre>"},{"location":"docs/taskfile/include-other-taskfile/#optional-includes","title":"Optional includes","text":"<p>Optional includes allow you to include Taskfiles that may not exist, without causing an error if they are missing.</p> Taskfile.yml<pre><code>version: '3'\nincludes:\n  missing:\n    taskfile: ./missing/Taskfile.yml\n    optional: true\ntasks:\n  test:\n    cmds:\n      - echo \"This task runs even if the included Taskfile is missing\"\n</code></pre>"},{"location":"docs/taskfile/include-other-taskfile/#internal-includes","title":"Internal includes","text":"<p>Internal includes allow you to include utility tasks that are not intended to be run directly by users.</p> Text Only<pre><code>app/\n\u2502   Taskfile.yml\nTaskfile.yml\n</code></pre> app/Taskfile.yml<pre><code>version: '3'\ntasks:\n  get-app-version:\n    desc: \"Get the version of the app\"\n    cmds:\n      - 'echo \"App version is 1.0.0\"'\n      - pwd\n</code></pre> ~/Taskfile.yml<pre><code>version: '3'\nincludes:\n  app:\n    taskfile: ./app/Taskfile.yml\n    internal: true # this will not be listed in the task list\ntasks:\n  test:\n    cmds:\n      - echo \"Hello World\"\n</code></pre> Demo and output<pre><code># app:get-app-version will not be listed in the task list\nubuntu@touted-mite:~$ task --list-all\ntask: Available tasks for this project:\n* test:       \n\n# if you try to run the internal task, it will not work\nubuntu@touted-mite:~$ task app:get-app-version\ntask: Task \"app:get-app-version\" is internal\n</code></pre>"},{"location":"docs/taskfile/include-other-taskfile/#flatten-includes","title":"Flatten includes","text":"<p>Info</p> <p>Make sure the tasks do not have same name across the included Taskfiles, otherwise it will provide an error.</p> <p>Flatten includes allow you to include tasks from other Taskfiles without the need to specify the namespace.</p> Text Only<pre><code>app/\n\u2502   Taskfile.yml\nTaskfile.yml\n</code></pre> app/Taskfile.yml<pre><code>version: '3'\ntasks:\n  get-app-version:\n    desc: \"Get the version of the app\"\n    cmds:\n      - 'echo \"App version is 1.0.0\"'\n      - pwd\n</code></pre> ~/Taskfile.yml<pre><code>version: '3'\nincludes:\n  app:\n    taskfile: ./app/Taskfile.yml\n    flatten: true # You can call the tasks without the namespace\ntasks:\n  test:\n    cmds:\n      - echo \"Hello World\"\n      - task: get-app-version\n</code></pre> Demo and output<pre><code>ubuntu@touted-mite:~$ task -a\ntask: Available tasks for this project:\n* get-app-version:       Get the version of the app\n* test:                  \n\nubuntu@touted-mite:~$ task get-app-version \ntask: [get-app-version] echo \"App version is 1.0.0\"\nApp version is 1.0.0\ntask: [get-app-version] pwd\n/home/ubuntu\n</code></pre>"},{"location":"docs/taskfile/include-other-taskfile/#exclude-tasks-from-includes","title":"Exclude Tasks from Includes","text":"<p>You can exclude specific tasks from being included by using the <code>exclude</code> keyword.</p> Text Only<pre><code>app/\n\u2502   Taskfile.yml\nTaskfile.yml\n</code></pre> app/Taskfile.yml<pre><code>version: '3'\ntasks:\n  get-app-version:\n    desc: \"Get the version of the app\"\n    cmds:\n      - 'echo \"App version is 1.0.0\"'\n      - pwd\n  get-secret:\n    cmds:\n      - 'echo \"This is a secret task\"'\n</code></pre> ~/Taskfile.yml<pre><code>version: '3'\nincludes:\n  app:\n    taskfile: ./app/Taskfile.yml\n    excludes: [get-secret]\n</code></pre> Demo and output<pre><code>ubuntu@touted-mite:~$ task -a\ntask: Available tasks for this project:\n* app:get-app-version:       Get the version of the app\n</code></pre>"},{"location":"docs/taskfile/include-other-taskfile/#vars-of-included-taskfiles","title":"Vars of included Taskfiles","text":"<p>You can also use the <code>vars</code> keyword to pass variables to the included Taskfiles. This is useful when you want to customize the behavior of the included tasks.</p> app/Taskfile.yml<pre><code>version: '3'\ntasks:\n  build-docker-image:\n    cmds:\n      - docker build -t {{.NAME}} {{.SRC_DIR}}\n</code></pre> ~/Taskfile.yml<pre><code>version: '3'\nincludes:\n  app:\n    taskfile: ./app/Taskfile.yml\n    vars:\n      NAME: \"my-app\"\n      SRC_DIR: \"app\"\n</code></pre> <ul> <li>You can use <code>default</code> keyword to set the default task for the included Taskfile, then you can override the variable values in the <code>vars</code> keyword. Refer here</li> </ul>"},{"location":"docs/taskfile/include-other-taskfile/#namespace-aliases-shortform","title":"Namespace aliases (Shortform)","text":"<p>You can also use the <code>alias</code> keyword to create short aliases for namespaces. This is useful when you want to reduce the typing effort.</p> ~/Taskfile.yml<pre><code>version: '3'\nincludes:\n  app:\n    taskfile: ./app/Taskfile.yml\n    aliases: [a]\n</code></pre> Demo and output<pre><code>ubuntu@touted-mite:~$ task a\na:get-app-version    a:get-secret         app:get-app-version  app:get-secret\n</code></pre>"},{"location":"docs/taskfile/installation/","title":"Installation","text":""},{"location":"docs/taskfile/installation/#snap","title":"Snap","text":"Bash<pre><code>sudo snap install task --classic\n</code></pre>"},{"location":"docs/taskfile/installation/#binary","title":"Binary","text":"<p>Download the binary from release page and add it to your <code>PATH</code>.</p> Bash<pre><code>wget https://github.com/go-task/task/releases/download/v3.43.3/task_linux_amd64.deb\nsudo dpkg -i task_linux_amd64.deb\n</code></pre>"},{"location":"docs/taskfile/installation/#script","title":"Script","text":"<p>Override the installation directory with <code>-b</code> flag.</p> Bash<pre><code>sh -c \"$(curl --location https://taskfile.dev/install.sh)\" -- -d -b /usr/local/bin\n</code></pre>"},{"location":"docs/taskfile/installation/#github-action","title":"GitHub Action","text":"<p>You can understand more about this action on GitHub.</p> YAML<pre><code>- name: Install Task\n  uses: arduino/setup-task@v2\n  with:\n    version: 3.x\n    repo-token: ${{ secrets.GITHUB_TOKEN }}\n</code></pre>"},{"location":"docs/taskfile/integrate-taskfile-with-cicd/","title":"Integrate Taskfile with CI/CD","text":"<p>When you want to integrate Taskfile with your CI/CD pipeline, there are multiple ways that you can reference the remote Taskfile.</p> <ul> <li><code>https://raw.githubusercontent.com/&lt;username&gt;/&lt;repo&gt;/&lt;branch&gt;/&lt;path&gt;/Taskfile.yml</code></li> <li><code>https://github.com/&lt;username&gt;/&lt;repo&gt;.git//&lt;path&gt;/Taskfile.yml?ref=main</code><ul> <li><code>//&lt;path&gt;</code> make sure to include the <code>//</code> before the path.</li> </ul> </li> <li><code>git@github.com/&lt;username&gt;/&lt;repo&gt;.git//&lt;path&gt;/Taskfile.yml?ref=main</code></li> </ul>"},{"location":"docs/taskfile/integrate-taskfile-with-cicd/#step-1-enable-this-remote-experiment-feature","title":"Step 1: Enable this remote experiment feature","text":"Bash<pre><code># Set the environment variable to download the remote taskfile\nexport TASK_X_REMOTE_TASKFILES=1 \n</code></pre>"},{"location":"docs/taskfile/integrate-taskfile-with-cicd/#step-2-include-remote-taskfile-or-execute-from-cli-directly","title":"Step 2: Include remote Taskfile or execute from CLI directly","text":"Bash<pre><code>task --taskfile https://raw.githubusercontent.com/go-task/task/main/website/static/Taskfile.yml\n</code></pre> <p>OR</p> Taskfile.yml<pre><code>version: '3'\n\nincludes:\n  # without authentication\n  app-remote: https://raw.githubusercontent.com/go-task/task/main/website/static/Taskfile.yml\n  # with authentication\n  app-remote-auth: https://{{.TOKEN}}@raw.githubusercontent.com/go-task/task/main/website/static/Taskfile.yml\n</code></pre> Bash<pre><code>task app-remote\n\nexport TOKEN=&lt;your-token&gt;\ntask app-remote-auth\n</code></pre>"},{"location":"docs/taskfile/integrate-taskfile-with-cicd/#step-3-optional-force-download-the-remote-taskfile","title":"Step 3 (Optional): Force download the remote Taskfile","text":"<p>By default, the cache will be stored in the <code>.task</code> directory. But if you want to make sure the remote Taskfile always up-to-date, you can force download it by using the <code>--download</code> flag or you can use <code>--clear-cache</code> to clear all remote cache.</p> Bash<pre><code># Choose one of the following commands that suits your needs\ntask --download -y # force download the remote Taskfile\ntask --clear-cache # clear all remote cache\n</code></pre>"},{"location":"docs/taskfile/interactive-cli-application/","title":"Interactive CLI Application","text":"<p>In some cases, your application may need to interact with the user. So, we can use <code>interactive: true</code> to enable an interactive CLI application. This allows you to prompt the user for input and handle it accordingly.</p> Taskfile.yaml<pre><code>version: '3'\n\ntasks:\n  default:\n    cmds:\n      - vim my-file.txt\n    interactive: true\n</code></pre>"},{"location":"docs/taskfile/internal-tasks/","title":"Internal Tasks","text":"<p>Info</p> <p>Refer here for Internal includes</p> <p>Internal tasks in Taskfile are used to define tasks that are not intended to be run directly by users. They are useful for organizing utility tasks that support the main tasks of your project without cluttering the task list.</p> Taskfile.yml<pre><code>version: '3'\ntasks:\n  secret-task:\n    internal: true\n    cmds:\n      - echo \"This is a secret task\"\n  public-task:\n    cmds:\n      - echo \"This is a public task\"\n      - task: secret-task\n</code></pre> Demo and output<pre><code>ubuntu@touted-mite:~$ task -a\ntask: Available tasks for this project:\n* public-task:       \n</code></pre> <p>In this case, it will only show the <code>public-task</code> in the task list, while <code>secret-task</code> is hidden from the user.</p>"},{"location":"docs/taskfile/looping/","title":"Looping","text":""},{"location":"docs/taskfile/looping/#loop-a-static-list-loop-the-tasks","title":"Loop a static list / Loop the tasks","text":"<p>Assuming you have a Taskfile with a task to build the Docker images. You can use a loop to iterate over a static list of applications and build each one.</p> Taskfile.yml<pre><code>version: '3'\ntasks:\n  image:build:\n    vars:\n      REPOSITORY: '{{.REPOSITORY}}'\n      SRC: '{{.SRC}}'\n    cmds:\n      - docker build -t {{.REPOSITORY}}:latest {{.SRC}}\n  build-all:\n    cmds:\n      - for:\n          - app1\n          - app2\n        task: image:build\n        vars:\n          REPOSITORY: \"{{.ITEM}}\" # refer to app1, app2\n          SRC: \"{{.ITEM}}\" # refer to app1, app2\n  # run different atsks based on the loop value\n  task-test:\n    cmds:\n      - echo \"Running task test\"\n  task-build:\n    cmds:\n      - echo \"Running task build\"\n  test-start:\n    cmds:\n      - for: [test, build]\n        task: task-{{ .ITEM }} # refer to task-test, task-build\n</code></pre> Demo and output<pre><code>ubuntu@touted-mite:~$ task build-all \ntask: [image:build] docker build -t app1:latest app1\n[+] Building 5.9s (5/5) FINISHED                                docker:default\n =&gt; [internal] load build definition from Dockerfile                      0.0s\n =&gt; =&gt; transferring dockerfile: 56B                                       0.0s\n =&gt; [internal] load metadata for docker.io/library/ubuntu:latest          3.2s\n =&gt; [internal] load .dockerignore                                         0.0s\n =&gt; =&gt; transferring context: 2B                                           0.0s\n =&gt; [1/1] FROM docker.io/library/ubuntu:latest@sha256:6015f66923d7afbc53  2.5s\n =&gt; =&gt; resolve docker.io/library/ubuntu:latest@sha256:6015f66923d7afbc53  0.0s\n =&gt; =&gt; sha256:6015f66923d7afbc53558d7ccffd325d43b4e249f4 6.69kB / 6.69kB  0.0s\n =&gt; =&gt; sha256:dc17125eaac86538c57da886e494a34489122fb6a3ebb6 424B / 424B  0.0s\n =&gt; =&gt; sha256:a0e45e2ce6e6e22e73185397d162a64fcf2f80a41c 2.30kB / 2.30kB  0.0s\n =&gt; =&gt; sha256:0622fac788edde5d30e7bbd2688893e5452a19ff 29.72MB / 29.72MB  1.7s\n =&gt; =&gt; extracting sha256:0622fac788edde5d30e7bbd2688893e5452a19ff237a2e4  0.6s\n =&gt; exporting to image                                                    0.0s\n =&gt; =&gt; exporting layers                                                   0.0s\n =&gt; =&gt; writing image sha256:a3a57681676475ef2e0d451fcfd3daaca8d23d3ad42b  0.0s\n =&gt; =&gt; naming to docker.io/library/app1:latest                            0.0s\n\ntask: [image:build] docker build -t app2:latest app2\n[+] Building 0.5s (5/5) FINISHED                                docker:default\n =&gt; [internal] load build definition from Dockerfile                      0.0s\n =&gt; =&gt; transferring dockerfile: 56B                                       0.0s\n =&gt; [internal] load metadata for docker.io/library/ubuntu:latest          0.3s\n =&gt; [internal] load .dockerignore                                         0.0s\n =&gt; =&gt; transferring context: 2B                                           0.0s\n =&gt; CACHED [1/1] FROM docker.io/library/ubuntu:latest@sha256:6015f66923d  0.0s\n =&gt; exporting to image                                                    0.0s\n =&gt; =&gt; exporting layers                                                   0.0s\n =&gt; =&gt; writing image sha256:a3a57681676475ef2e0d451fcfd3daaca8d23d3ad42b  0.0s\n =&gt; =&gt; naming to docker.io/library/app2:latest                            0.0s\nubuntu@touted-mite:~$ docker images\nREPOSITORY    TAG       IMAGE ID       CREATED        SIZE\napp1          latest    a3a576816764   4 weeks ago    78.1MB\napp2          latest    a3a576816764   4 weeks ago    78.1MB\n</code></pre>"},{"location":"docs/taskfile/looping/#loop-a-matrix","title":"Loop a matrix","text":"<p>You can also loop over a matrix of values. This is useful when you have multiple variables that need to be combined in each iteration.</p> <p>You can also reference the matrix to other variables as long as they are lists.</p> Taskfile.yml<pre><code>version: '3'\ntasks:\n  image:build:\n    vars:\n      REPOSITORY: '{{.REPOSITORY}}'\n      SRC: '{{.SRC}}'\n      TAG: '{{.TAG}}'\n    cmds:\n      - docker build -t {{.REPOSITORY}}:{{.TAG}} {{.SRC}}\n  build-all:\n    cmds:\n      - for:\n          matrix:\n            REPOSITORY: [app1, app2]\n            TAG: [latest, v1.0]\n        task: image:build\n        vars:\n          REPOSITORY: \"{{.ITEM.REPOSITORY}}\" # refer to app1, app2\n          SRC: \"{{.ITEM.REPOSITORY}}\" # refer to app1, app2\n          TAG: \"{{.ITEM.TAG}}\" # refer to latest, v1.0\n</code></pre> <p>This will generate the following images:</p> <ul> <li><code>app1:latest</code></li> <li><code>app1:v1.0</code></li> <li><code>app2:latest</code></li> <li><code>app2:v1.0</code></li> </ul> Demo and output<pre><code>ubuntu@touted-mite:~$ docker images\nREPOSITORY    TAG       IMAGE ID       CREATED        SIZE\napp1          latest    a3a576816764   4 weeks ago    78.1MB\napp1          v1.0      a3a576816764   4 weeks ago    78.1MB\napp2          latest    a3a576816764   4 weeks ago    78.1MB\napp2          v1.0      a3a576816764   4 weeks ago    78.1MB\n</code></pre>"},{"location":"docs/taskfile/looping/#loop-the-tasks-sources-or-generated-files","title":"Loop the task's <code>sources</code> or <code>generated</code> files","text":"<p>It is the same concept as above, but you can use the <code>sources</code> or <code>generated</code> files to loop over a list of files.</p> SourcesGenerates Taskfile.yml<pre><code>version: '3'\ntasks:\n  default:\n    sources:\n      - foo.txt\n      - bar.txt\n    cmds:\n      - for: sources\n        cmd: cat {{ .ITEM }}\n</code></pre> Taskfile.yml<pre><code>version: '3'\ntasks:\n  default:\n    generates:\n      - foo.txt\n      - bar.txt\n    cmds:\n      - for: generates\n        cmd: cat {{ .ITEM }}\n</code></pre>"},{"location":"docs/taskfile/looping/#loop-the-variables","title":"Loop the variables","text":"<p>If you want to loop over the variables, you can use the <code>vars</code> keyword. This is useful when you have a list of variables that need to be iterated over.</p> Taskfile.yml<pre><code>version: '3'\ntasks:\n  loop1: # loop over a single variable\n    vars:\n      MY_VAR: hello world\n    cmds:\n      - for: { var: MY_VAR }\n        cmd: echo {{ .ITEM }}\n  loop2: # split and loop\n    vars:\n      MY_VAR: hello,world\n    cmds:\n      - for: { var: MY_VAR, split: ','}\n        cmd: echo {{ .ITEM }}\n  loop3: # loop over a list of variables\n    vars:\n      LIST: [hello, world]\n    cmds:\n      - for:\n          var: LIST\n        cmd: echo {{ .ITEM }}\n  loop4: # loop over a map\n    vars:\n      MY_MAP:\n        map:\n          key1: hello\n          key2: world\n    cmds:\n      - for:\n          var: MY_MAP\n        cmd: echo {{.KEY}} {{ .ITEM }} # KEY is the key, ITEM is the value\n  loop5: # loop with dynamic variablses\n    vars:\n      MY_VAR:\n        sh: find -type f -name '*.txt'\n    cmds:\n      - for: { var: MY_VAR }\n        cmd: echo {{ .ITEM }}\n</code></pre> Demo and output<pre><code>ubuntu@touted-mite:~$ task loop1 loop2 loop3 loop4\ntask: [loop1] echo hello\nhello\ntask: [loop1] echo world\nworld\ntask: [loop2] echo hello\nhello\ntask: [loop2] echo world\nworld\ntask: [loop3] echo hello\nhello\ntask: [loop3] echo world\nworld\ntask: [loop4] echo hello key1\nhello key1\ntask: [loop4] echo world key2\nworld key2\n</code></pre>"},{"location":"docs/taskfile/looping/#loop-and-rename-variables","title":"Loop and rename variables","text":"<p>When you loop over a list of variables, you can also rename the variables using the <code>as</code> keyword.</p> Taskfile.yml<pre><code>version: '3'\ntasks:\n  loop-rename:\n    vars:\n      LIST: [hello, world]\n    cmds:\n      - for:\n          var: LIST\n          as: MESSAGE\n        cmd: echo {{ .MESSAGE }} # MESSAGE is the renamed variable\n</code></pre>"},{"location":"docs/taskfile/looping/#loop-the-dependencies","title":"Loop the dependencies","text":"<p>Remember <code>deps</code> are run in parallel, so the iterations are not guaranteed to run in order.</p> Taskfile.yml<pre><code>version: '3'\n\ntasks:\n  default:\n    deps:\n      - for: [foo, bar]\n        task: my-task\n        vars:\n          FILE: '{{.ITEM}}'\n\n  my-task:\n    cmds:\n      - echo '{{.FILE}}'\n</code></pre>"},{"location":"docs/taskfile/overriding-task-name/","title":"Overriding Task Name","text":"<p>You can override the task name printed on the summary by using the <code>label:</code> field in your Taskfile. This is useful when you want to customize how tasks are displayed in the output, especially when using variables.</p> Taskfile.yaml<pre><code>version: '3'\ntasks:\n  display:\n    internal: true\n    label: 'display-{{.MESSAGE}}'\n    cmds:\n      - echo \"{{.MESSAGE}}\"\n  test:\n    cmds:\n      - task: display\n        vars:\n          MESSAGE: hello\n      - task: display\n        vars:\n          MESSAGE: world\n</code></pre> Demo and Output<pre><code>ubuntu@touted-mite:~$ task test \ntask: [display-hello] echo \"hello\"\nhello\ntask: [display-world] echo \"world\"\nworld\n</code></pre>"},{"location":"docs/taskfile/platform-specific-tasks/","title":"Platform-Specific Tasks","text":"<p>Supported Platforms &amp; Architectures</p> <p>Refer here for the list of supported platforms and architectures</p> <p>You can create platform-specific tasks in Taskfile to run commands based on the operating system. This is useful when you need to perform different actions on different platforms, such as installing software or running scripts.</p> <p>If there is no OS match, the task will not run (no error will be thrown).</p>"},{"location":"docs/taskfile/platform-specific-tasks/#restrict-to-specific-platforms","title":"Restrict to Specific Platforms","text":"<p>Taskfile.yml<pre><code>version: '3'\n\ntasks:\n  hello:windows:\n    desc: Say hello from Windows\n    platforms: [windows]\n    cmds:\n      - echo \"Hello from Windows!\"\n\n  hello:linux:\n    desc: Say hello from Linux and Darwin\n    platforms: [linux, darwin]\n    cmds:\n      - echo \"Hello from Linux and Darwin!\"\n</code></pre> Remember multiple platforms are supported.</p>"},{"location":"docs/taskfile/platform-specific-tasks/#restrict-to-specific-architectures","title":"Restrict to Specific Architectures","text":"Taskfile.yml<pre><code>version: '3'\n\ntasks:\n  hello:amd64:\n    desc: Say hello from amd64\n    platforms: [amd64]\n    cmds:\n      - echo \"Hello from amd64!\"\n</code></pre>"},{"location":"docs/taskfile/platform-specific-tasks/#restrict-to-specific-platforms-andor-architectures","title":"Restrict to Specific Platforms and/or Architectures","text":"Taskfile.yml<pre><code>version: '3'\ntasks:\n  hello:windows:amd64:\n    desc: Say hello from Windows amd64\n    platforms: [windows/amd64]\n    cmds:\n      - echo \"Hello from Windows amd64!\"\n</code></pre>"},{"location":"docs/taskfile/platform-specific-tasks/#restrict-to-specific-platforms-andor-architectures-within-an-individual-commands","title":"Restrict to Specific Platforms and/or Architectures within an individual commands","text":"Taskfile.yml<pre><code>version: '3'\ntasks:\n  hello:\n    cmds:\n      - cmd: echo \"Hello from Windows!\"\n        platforms: [windows]\n      - cmd: echo \"Hello from Windows!\"\n        platforms: [linux, amd64]\n      - cmd: |-\n          echo \"Running on all platforms!\"\n          echo \"This will run regardless of the platform.\"\n</code></pre>"},{"location":"docs/taskfile/prevent-unnecessary-task-execution/","title":"Prevent Unnecessary Task Execution","text":""},{"location":"docs/taskfile/prevent-unnecessary-task-execution/#by-fingerprinting-locally-generated-files-and-their-sources","title":"By fingerprinting locally generated files and their sources","text":"<p>We can avoid unnecessary workload of tasks to reduce the time taken. Meaning that the tasks will only run when there are changes in the required files.</p> <p>For example, when you work on a Node.JS application, you may have a task to install dependencies and build the application.</p> <ul> <li><code>npm install</code> - run this command when there are changes in the <code>package.json</code> file.</li> <li><code>npm run build</code> - run this command where there are changes in any <code>**/*.js</code> files.</li> </ul> Taskfile.yaml<pre><code>version: '3'\n\ntasks:\n  # if the package-lock.json is deleted, the task will not run again\n  npm:install:without-generates:\n    cmds:\n      - npm install\n    sources: # it will compare the checksum to see whether the files got any changes\n      - package.json\n\n  # if the package-lock.json is deleted, the task will run again\n  npm:install:\n    cmds:\n      - npm install\n    sources: # it will compare the checksum to see whether the files got any changes\n      - package.json\n    generates: # this will be the files generated by the command, if this file is deleted, the task will run again\n      - package-lock.json\n\n  npm:build:\n    cmds:\n      - npm run build\n    sources:\n      - '**/*.js'\n      - exclude: ignore.js\n\n  python_build:\n    cmds:\n      - python3 -m build\n    sources:\n      - setup.py\n      - '**/*.py'\n      - exclude: tests/** # exclude all files in the tests directory\n    generates:\n      - dist/*.whl # generated wheel file\n    method: timestamp\n</code></pre> <ul> <li><code>sources</code> and <code>generates</code> can be files or glob patterns<ul> <li><code>sources</code> - it will compare the checksum to see whether the files got any changes</li> <li><code>generates</code> - this will be the files generated by the command, if this file is deleted, the task will run again</li> </ul> </li> <li><code>exclude</code> - exclude files from fingerprinting, but you have to make sure the <code>sources</code> are in glob format and must come after the positive glob it is negating.</li> <li><code>method: timestamp</code> - If you prefer to check the timestamp of the files instead of their checksum, you can use <code>method: timestamp</code>. This is useful for large files where checksum calculation might be expensive.<ul> <li><code>method: none</code> - Skips the validation, meaning the task will always run regardless of changes in the source files.</li> <li><code>method: checksum</code> - This is the default method, which checks the checksum of the files to determine if they have changed.</li> </ul> </li> <li><code>status</code> - You can also use the <code>status</code> command to check the status of the task. It will show whether the task is up to date or not. Refer Using programmatic checks to indicate a task is up to date</li> </ul> Demo and Output<pre><code>ubuntu@touted-mite:~/nodejsfun$ task npm:install\ntask: [npm:install] npm install\nnpm WARN EBADENGINE Unsupported engine {\nnpm WARN EBADENGINE   package: 'ansi-escapes@6.2.1',\nnpm WARN EBADENGINE   required: { node: '&gt;=14.16' },\nnpm WARN EBADENGINE   current: { node: 'v12.22.9', npm: '8.5.1' }\nnpm WARN EBADENGINE }\nnpm WARN EBADENGINE Unsupported engine {\nnpm WARN EBADENGINE   package: 'marked-terminal@5.2.0',\nnpm WARN EBADENGINE   required: { node: '&gt;=14.13.1 || &gt;=16.0.0' },\nnpm WARN EBADENGINE   current: { node: 'v12.22.9', npm: '8.5.1' }\nnpm WARN EBADENGINE }\n\nup to date, audited 62 packages in 723ms\n\n3 packages are looking for funding\n  run `npm fund` for details\n\n4 moderate severity vulnerabilities\n\nSome issues need review, and may require choosing\na different dependency.\n\nRun `npm audit` for details.\n\n# second time - no changes in package.json\nubuntu@touted-mite:~/nodejsfun$ task npm:install\ntask: Task \"npm:install\" is up to date\n</code></pre> <p>Note</p> <p>A <code>.task</code> directory will be created in the source directory to store the fingerprint of the files. This directory is used to track changes in the files specified in the <code>sources</code> section of the task. If you want to avoid committing this directory to version control, consider adding it to your <code>.gitignore</code> file.</p> <p>If you want to change the location of the <code>.task</code> directory, you can set the <code>TASK_TEMP_DIR</code> environment variable to the desired path.</p> <ul> <li><code>export TASK_TEMP_DIR=/path/to/custom/.task</code></li> </ul> <p>When you rerun the task, it will not run the <code>npm install</code> command as there are no changes in the <code>package.json</code> file. You will see a message indicating that the task is up to date.</p>"},{"location":"docs/taskfile/prevent-unnecessary-task-execution/#using-programmatic-checks-to-indicate-a-task-is-up-to-date","title":"Using programmatic checks to indicate a task is up to date","text":"<p>Info</p> <p>Use <code>task --status &lt;tasks&gt;</code> to check the status of a task is up to date or not.</p> <p>You can also use programmatic checks to indicate whether a task is up to date. In this case, you can use the <code>status</code> command to check the status of the task before running the commands. If the task is up to date, it will skip running the commands.</p> <p>This method is quite useful when you want to create artifacts or files that are not directly related to the source files, or when you want to ensure that certain files exist before running a task.</p> <p>Here is how it works: - Task always checks the status first.</p> <ol> <li>On the first run, the status checks will fail (because the directory and files don't exist), so the commands will execute to create them.</li> <li>On subsequent runs, the status checks will pass (because the directory and files now exist), so the commands will not run again. The task is skipped.</li> </ol> Taskfile.yaml<pre><code>version: '3'\n\ntasks:\n  generate-files:\n    cmds:\n      - mkdir directory\n      - touch directory/file1.txt\n      - touch directory/file2.txt\n    # test existence of files\n    status:\n      - test -d directory\n      - test -f directory/file1.txt\n      - test -f directory/file2.txt\n</code></pre> Demo and Output<pre><code>ubuntu@touted-mite:~$ task generate-files \ntask: [generate-files] mkdir directory\ntask: [generate-files] touch directory/file1.txt\ntask: [generate-files] touch directory/file2.txt\n\nubuntu@touted-mite:~$ task generate-files \ntask: Task \"generate-files\" is up to date\n</code></pre>"},{"location":"docs/taskfile/prevent-unnecessary-task-execution/#combination-of-sources-and-status-checks","title":"Combination of <code>sources</code> and <code>status</code> checks:","text":"<p>With the combination of <code>sources</code> and <code>status</code> checks, if either the sources change or programmatic checks fail, then the task will run again.</p> Taskfile.yaml<pre><code>version: '3'\n\ntasks:\n  npm:install:\n    cmds:\n      - npm install\n    sources:\n      - package.json\n    status:\n      - test -f package-lock.json\n</code></pre>"},{"location":"docs/taskfile/prevent-unnecessary-task-execution/#using-programmatic-checks-to-cancel-the-execution-of-a-task-and-its-dependencies-preconditions-checks","title":"Using programmatic checks to cancel the execution of a task and its dependencies (Preconditions Checks)","text":"<p>You can setup preconditions checks to run before the task commands. If any of the checks fail, the task will not run. It is very similar to the <code>status</code> command just that it support <code>sh</code> expansion. If any preconditions fail, the task and its dependencies are canceled and do not run.</p> Taskfile.yaml<pre><code>version: '3'\ntasks:\n  generate-files:\n    cmds:\n      - mkdir directory\n    # test existence of files\n    preconditions:\n      - test -f .env\n      - sh: '[ 1 = 0 ]'\n        msg: \"This will not run because the condition is false\"\n</code></pre> <p>Here is an example of the dependencies are canceled when the preconditions fail:</p> Taskfile.yaml<pre><code>version: '3'\n\ntasks:\n  task-will-fail:\n    preconditions:\n      - sh: 'exit 1'\n\n  task-will-also-fail:\n    deps:\n      - task-will-fail\n\n  task-will-still-fail:\n    cmds:\n      - task: task-will-fail\n      - echo \"I will not run\"\n</code></pre>"},{"location":"docs/taskfile/prevent-unnecessary-task-execution/#difference-between-status-and-preconditions","title":"Difference between <code>status</code> and <code>preconditions</code>","text":"Feature <code>status</code> <code>preconditions</code> Purpose Check if task is up to date Ensure requirements are met before running Effect on Task Execution Skips task if up to date and continue executing tasks that depend on it Fails task and any dependent task if conditions are not met"},{"location":"docs/taskfile/prevent-unnecessary-task-execution/#limiting-when-tasks-run","title":"Limiting when tasks run","text":"<p>More Information</p> <p><code>run</code> can also be set at the root level of the Taskfile to change the behavior of all tasks in the Taskfil unless overridden in individual tasks.</p> <p>Sometimes you may want to limit when tasks run based on certain conditions especially there are multiple <code>cmds</code> or <code>deps</code> in the task. In this case, you can use the <code>run</code> to change the behavior of the task execution.</p> <p>Supported <code>run</code> options:</p> <ul> <li><code>always</code> (default) - The task will always run regardless of the number of times it has been run.</li> <li><code>once</code> - The task will only run once, even if it has been run multiple times.</li> <li><code>when_changed</code> - The task will only run once for each unique set of variables passed into the task.</li> </ul> Taskfile.yaml<pre><code>version: '3'\n\ntasks:\n  default:\n    cmds:\n      - task: generate-file\n        vars: { CONTENT: '1' }\n      - task: generate-file\n        vars: { CONTENT: '2' }\n      - task: generate-file\n        vars: { CONTENT: '2' }\n\n  generate-file:\n    run: when_changed\n    deps:\n      - install-deps\n    cmds:\n      - echo {{.CONTENT}}\n\n  install-deps:\n    run: once\n    cmds:\n      - sleep 5 # long operation like installing packages\n</code></pre> Demo and Output<pre><code>ubuntu@touted-mite:~$ task default\ntask: [install-deps] sleep 5\ntask: [generate-file] echo 1\n1\ntask: [generate-file] echo 2\n2\n</code></pre> <p>As you can see, the <code>install-deps</code> task only runs once, even though it is called multiple times in the <code>default</code> task. The <code>generate-file</code> task runs only once for each unique set of variables passed into it.</p>"},{"location":"docs/taskfile/run-task-in-specific-order/","title":"Run task in specific order","text":"<p>We know that dependencies actually run in parallel, so dependencies are not guaranteed to run in order. If you want to run tasks in a specific order or run serially, then you can do the following:</p> Taskfile.yml<pre><code>version: '3'\ntasks:\n  build:\n    desc: Build the project\n    cmds:\n      - echo \"Building the project...\"\n\n  test:\n    desc: Run tests\n    cmds:\n      - echo \"Running tests...\"\n\n  lint:\n    desc: Lint the code\n    cmds:\n      - echo \"Linting code...\"\n\n  all:\n    desc: Run lint, build, and test in order\n    cmds:\n      - task: lint\n      - task: build\n      - task: test\n      - echo \"All tasks completed!\"\n</code></pre>"},{"location":"docs/taskfile/running-taskfile/","title":"Running Taskfile","text":"<p><code>task</code> command is used to run a Taskfile. By default, it looks for one of the supported Taskfile names in the current directory. You can also specify a Taskfile explicitly using the <code>-t</code> or <code>--taskfile</code> option.</p> Bash<pre><code>task -t &lt;file-path&gt; &lt;task-name&gt;\ntask -t directory/ default\ntask -t directory/CustomTask.yml custom\n</code></pre>"},{"location":"docs/taskfile/running-taskfile/#supported-taskfile-names","title":"Supported Taskfile names","text":"<ul> <li><code>Taskfile.yml</code></li> <li><code>taskfile.yml</code></li> <li><code>Taskfile.yaml</code></li> <li><code>taskfile.yaml</code></li> <li><code>Taskfile.dist.yml</code></li> <li><code>taskfile.dist.yml</code></li> <li><code>Taskfile.dist.yaml</code></li> <li><code>taskfile.dist.yaml</code></li> </ul>"},{"location":"docs/taskfile/running-taskfile/#run-a-taskfile-from-a-subdirectory","title":"Run a Taskfile from a subdirectory","text":"Text Only<pre><code>main-directory/\n\u251c\u2500\u2500 Taskfile.yml\n\u2514\u2500\u2500 sub-directory/\n    \u2514\u2500\u2500 sample.txt\n</code></pre> <p>Let's said we're in the <code>sub-directory</code>, but the <code>Taskfile.yml</code> cannot be found in the current directory, in this case, when you run the <code>task</code> command, it will go up one directory level to look for the Taskfile until it finds one or reaches the root directory.</p> <p>In this case, it will still work if you run the <code>task</code> command from the <code>sub-directory</code>.</p> demo and output<pre><code>ubuntu@touted-mite:~/main-directory$ ls -R ./\n./:\nTaskfile.yml  sub-directory\n\n./sub-directory:\nsample.txt\n\nubuntu@touted-mite:~/main-directory$ cd sub-directory/\nubuntu@touted-mite:~/main-directory/sub-directory$ task default\nHello, World!\n</code></pre>"},{"location":"docs/taskfile/running-taskfile/#special-user_working_dir-variable","title":"Special <code>{{ .USER_WORKING_DIR }}</code> variable","text":"Text Only<pre><code>main-directory/\n\u251c\u2500\u2500 Taskfile.yml\n\u2514\u2500\u2500 sub-directory/\n    \u251c\u2500\u2500 sample.txt\n    \u2514\u2500\u2500 Dockerfile\n</code></pre> <p><code>{{ .USER_WORKING_DIR }}</code> is a special variable that contains the path to the directory where the <code>task</code> commadn was run. For example, we can <code>cd</code> into the <code>sub-directory</code> and run a <code>task</code> command to build a Docker image or display <code>Dockerfile</code> content without having to specify the full path and can remove duplicate Taskfiles with the same content.</p> <p>Info</p> <p>We just need to make sure the <code>sub-directory</code> contains the <code>Dockerfile</code>, then it will be working as expected.</p> Taskfile.yml<pre><code>version: '3'\n\ntasks:\n  build-docker:\n    dir: '{{ .USER_WORKING_DIR }}'\n    cmds:\n      - docker build -t new-image .\n  display-file:\n    dir: '{{ .USER_WORKING_DIR }}'\n    cmds:\n      - cat Dockerfile\n</code></pre> demo and output<pre><code>ubuntu@touted-mite:~/main-directory/sub-directory$ task display-file\ntask: [display-file] cat Dockerfile\nFROM ubuntu:20\n</code></pre>"},{"location":"docs/taskfile/running-taskfile/#run-a-global-taskfile","title":"Run a global Taskfile","text":"<p>You can run a global Taskfile by specifing the <code>-g</code> or <code>--global</code> option. This will look for a Taskfile in the user's home directory.</p> <ul> <li><code>/home/&lt;username&gt;/Taskfile.yml</code></li> <li><code>C:/Users/&lt;username&gt;/Taskfile.yml</code></li> </ul> ~/Taskfile.yml<pre><code>version: '3'\n\ntasks:\n  from-home:\n    cmds:\n      - pwd\n\n  from-working-directory:\n    dir: '{{.USER_WORKING_DIR}}'\n    cmds:\n      - pwd\n</code></pre> demo and output<pre><code>ubuntu@touted-mite:~$ task -g from-home\ntask: [from-home] pwd\n/home/ubuntu\n\nubuntu@touted-mite:~$ cd main-directory/sub-directory/\nubuntu@touted-mite:~/main-directory/sub-directory$ task -g from-working-directory\ntask: [from-working-directory] pwd\n/home/ubuntu/main-directory/sub-directory\n</code></pre>"},{"location":"docs/taskfile/running-taskfile/#read-a-taskfile-from-stdin","title":"Read a Taskfile from stdin","text":"<p>This method is very useful when you are generating Taskfile content dynamically or when you want to run a Taskfile without saving it to a file. To read a Taskfile from stdin, you have to specify two things:</p> <ol> <li><code>-t</code> or <code>--taskfile</code> option</li> <li><code>-</code> pipe</li> </ol> <p>Bash<pre><code>task -t - &lt;(cat Taskfile.yml)\n\n# OR I prefer this way\ncat ./Taskfile.yml | task -t -\ncat ./Taskfile.yml | task -t - &lt;task-name&gt;\n</code></pre> - This <code>Taskfile</code> path must correctly point to the Taskfile content.</p> Text Only<pre><code>main-directory/\n\u251c\u2500\u2500 Taskfile.yml\n\u2514\u2500\u2500 sub-directory/\n    \u251c\u2500\u2500 sample.txt\n    \u2514\u2500\u2500 Dockerfile\n</code></pre> demo and output<pre><code># wrong path\nubuntu@touted-mite:~/main-directory/sub-directory$ cat ./Taskfile.yml | task -t - display-file\ncat: ./Taskfile.yml: No such file or directory\ntask: Missing schema version in Taskfile \"__stdin__\"\n\n# correct path\nubuntu@touted-mite:~/main-directory/sub-directory$ cat ../Taskfile.yml | task -t - display-file\ntask: [display-file] cat Dockerfile\nFROM ubuntu:20\n</code></pre>"},{"location":"docs/taskfile/silent-mode/","title":"Silent Mode","text":"<p>Silent Mode in Taskfile allows you to run tasks without displaying their output, which can be useful for background tasks or when you want to keep the console clean. To enable silent mode, you can use the <code>--silent</code> flag when running a task or use <code>silent: true</code> in the Taskfile.</p> <p>There are four ways to enable silent mode:</p> <ul> <li>at command level</li> <li>at task level</li> <li>globally at taskfile level or using <code>--slient</code> flag</li> <li>redirect the output to <code>/dev/null</code></li> </ul> Taskfile.yaml<pre><code>version: '3'\n\nsilent: true # global silent mode\n\ntasks:\n  silent-command-task:\n    cmds:\n      - cmd: echo \"This will not be displayed in silent mode\"\n        silent: true\n  silent-task:\n    silent: true\n    cmds:\n      - echo \"This will not be displayed in silent mode\"\n  silent-task-with-redirect:\n    cmds:\n      - echo \"This will not be displayed in silent mode\" &gt; /dev/null\n</code></pre>"},{"location":"docs/taskfile/task-aliases/","title":"Task Aliases","text":"<p>Info</p> <p>Refer here for namespace aliases.</p> <p>You can create aliases for tasks in Taskfile to simplify command execution. This allows you to define shorter or more intuitive names for tasks, making it easier to run them.</p> Taskfile.yaml<pre><code>version: '3'\ntasks:\n  complex-task-name:\n    aliases: [complex]\n    cmds:\n      - echo \"This is a complex task name\"\n</code></pre>"},{"location":"docs/taskfile/task-cleanup-with-defer/","title":"Task cleanup with Defer","text":"<p>Note</p> <p>If you have multiple deferred commands, they will run in the reverse order. This means the last deferred command will run first, followed by the second last, and so on. This is useful for tasks that require a specific order of cleanup actions.</p> <p>You can use the <code>defer</code> feature in Taskfile to ensure that a cleanup command runs after the main task, regardless of whether the task succeeded or failed. This is particularly useful for tasks that require cleanup actions, such as removing temporary files or directories.</p> <p>Taskfile.yaml<pre><code>version: '3'\ntasks:\n  install:\n    cmds:\n      - npm install\n      - defer: rm -rf .cache\n      - echo \"Installation complete\"\n      - defer: echo \"Cleanup task executed\"\n</code></pre> From this example, the <code>rm -rf .cache</code> command will run after the <code>echo \"Installation complete\"</code> command. <code>defer</code> will always run at the end of the task.</p> Demo and Output<pre><code>ubuntu@touted-mite:~/nodejsfun$ task install \ntask: [install] npm install\nnpm WARN EBADENGINE Unsupported engine {\nnpm WARN EBADENGINE   package: 'ansi-escapes@6.2.1',\nnpm WARN EBADENGINE   required: { node: '&gt;=14.16' },\nnpm WARN EBADENGINE   current: { node: 'v12.22.9', npm: '8.5.1' }\nnpm WARN EBADENGINE }\nnpm WARN EBADENGINE Unsupported engine {\nnpm WARN EBADENGINE   package: 'marked-terminal@5.2.0',\nnpm WARN EBADENGINE   required: { node: '&gt;=14.13.1 || &gt;=16.0.0' },\nnpm WARN EBADENGINE   current: { node: 'v12.22.9', npm: '8.5.1' }\nnpm WARN EBADENGINE }\n\nup to date, audited 185 packages in 1s\n\n78 packages are looking for funding\n  run `npm fund` for details\n\n9 vulnerabilities (4 moderate, 5 high)\n\nSome issues need review, and may require choosing\na different dependency.\n\nRun `npm audit` for details.\ntask: [install] echo \"Installation complete\"\nInstallation complete\ntask: [install] echo \"Cleanup task executed\"\nCleanup task executed\ntask: [install] rm -rf .cache\n</code></pre>"},{"location":"docs/taskfile/task-dependencies-and-parallel-task-execution/","title":"Task Dependencies &amp; Parallel Task Execution","text":""},{"location":"docs/taskfile/task-dependencies-and-parallel-task-execution/#task-dependencies","title":"Task Dependencies","text":"<p>Run task in specific order</p> <p>If you want to run tasks in a specific order or run serially, then you can refer to run task in specific order.</p> <p>Dependencies actually run in parallel, so dependencies are not guaranteed to run in order. You can achieve this by using the <code>deps</code> keyword in your Taskfile.</p>"},{"location":"docs/taskfile/task-dependencies-and-parallel-task-execution/#single-task-dependency","title":"Single Task Dependency","text":"<p>Taskfile.yml<pre><code>version: '3'\ntasks:\n  build:\n    deps: [lint]\n    desc: Build the project\n    cmds:\n      - echo \"Building the project...\"\n\n  lint:\n    desc: Lint the code\n    cmds:\n      - echo \"Linting code...\"\n</code></pre> In this case, the <code>lint</code> task will run before the <code>build task</code>, ensuring that the code is linted before building it.</p> Demo and Output<pre><code>ubuntu@touted-mite:~$ task build\ntask: [lint] echo \"Linting code...\"\nLinting code...\ntask: [build] echo \"Building the project...\"\nBuilding the project...\n</code></pre>"},{"location":"docs/taskfile/task-dependencies-and-parallel-task-execution/#multiple-task-dependencies","title":"Multiple Task Dependencies","text":"Taskfile.yml<pre><code>version: '3'\ntasks:\n  build:\n    desc: Build the project\n    cmds:\n      - echo \"Building the project...\"\n\n  test:\n    desc: Run tests\n    cmds:\n      - echo \"Running tests...\"\n\n  lint:\n    desc: Lint the code\n    cmds:\n      - echo \"Linting code...\"\n\n  all:\n    desc: Run lint, build, and test in order\n    deps:\n      - lint\n      - build\n      - test\n      # - example\n      #   vars:\n      #     EXAMPLE_VAR: \"example value\"\n    cmds:\n      - echo \"All tasks completed!\"\n</code></pre> <ul> <li>You can also pass variable to dependencies. Is the same concept as passing variables to tasks.</li> </ul> <p>It does not guaranteed that <code>lint</code> will run before <code>build</code> or <code>test</code>, but it ensures that all tasks are executed before the <code>all</code> task completes.</p> Demo and Output<pre><code>ubuntu@touted-mite:~$ task all\ntask: [test] echo \"Running tests...\"\ntask: [build] echo \"Building the project...\"\ntask: [lint] echo \"Linting code...\"\nBuilding the project...\nRunning tests...\nLinting code...\ntask: [all] echo \"All tasks completed!\"\nAll tasks completed!\n</code></pre>"},{"location":"docs/taskfile/task-dependencies-and-parallel-task-execution/#parallel-task-execution","title":"Parallel Task Execution","text":"<p>Parallel task execution allows you to run multiple tasks concurrently, which can significantly speed up your workflow, especially for tasks that are independent of each other.</p> Taskfile.yml<pre><code>version: '3'\ntasks:\n  app:\n    dir: ./app\n    cmds:\n      - docker build -t app:latest ./\n  app1:\n    dir: ./app1\n    cmds:\n      - docker build -t app1:latest ./\n</code></pre> Bash<pre><code>task --parallel app app1\n</code></pre>"},{"location":"docs/taskfile/task-directory/","title":"Task Directory","text":"<p>Info</p> <p>Refer here for Include other Taskfile to run on different directory</p> <p>We know that tasks are run in the current directory where the Taskfile is located. However, you can actually run the Taskfile tasks in another directory by using the <code>dir</code> keyword.</p> Taskfile.yml<pre><code>version: '3'\ntasks:\n  get-working-directory:\n    dir: ./app1\n    cmds:\n      - pwd\n</code></pre> <p>If the directory does not exist, Task will create it.</p> Demo and output<pre><code>ubuntu@touted-mite:~$ task get-working-directory \ntask: [get-working-directory] pwd\n/home/ubuntu/app1\n</code></pre>"},{"location":"docs/taskfile/taskfile-overview/","title":"Taskfile Overview","text":""},{"location":"docs/taskfile/taskfile-overview/#what-is-taskfile","title":"What is Taskfile?","text":"<p>Repository</p> <ul> <li>https://github.com/go-task/task</li> <li>https://taskfile.dev</li> </ul> <p>Taskfile is a simple task runner for automating tasks in your development workflow and streamline your tasks. It is designed to be simpler and easier to use than other traditional tools like GNU Make.</p>"},{"location":"docs/taskfile/taskfile-overview/#benefits-of-taskfile","title":"Benefits of Taskfile","text":"<ul> <li>Easy to installation: It's written in Go and comes as a single binary without any other dependencies, which means, you just need to download a single binary and add it to your <code>$PATH</code> to get started.</li> <li>Portability: Taskfile is a single binary that can be run on any platform (Linux, macOS, Windows) without any dependencies.</li> <li>Flexibility: The developer can choose which tasks to run and in which order. You can also define dependencies between tasks, so that one task can depend on the completion of another task.</li> </ul>"},{"location":"docs/taskfile/taskfile-overview/#why-taskfile-is-important-for-devops-engineer-in-cicd","title":"Why Taskfile is important for DevOps Engineer in CI/CD?","text":"<p>We know that DevOps Engineer will be responsible for managing different deployment environment like staging, production, and testing. Sometimes, it could be a pain to manage same/different tasks in different environment.</p> <p>Assuming, you have the project with multiple microservices in different environment, and each microservice has its own set of tasks to run. In this case, you can use Taskfile to define the tasks for each microservice in a single file, and then run the tasks for each microservice in the desired environment.</p> <pre><code>graph TD\n    A[Centralized Taskfile Repository] --&gt; B[Taskfile Modules for Microservice A]\n    A --&gt; C[Taskfile Modules for Microservice B]\n    A --&gt; D[Taskfile Modules for Microservice C]\n    B --&gt; E[Staging Environment]\n    B --&gt; F[Production Environment]\n    C --&gt; E\n    C --&gt; F\n    D --&gt; E\n    D --&gt; F\n    E[Staging Environment] --&gt; G[Run Tasks for Microservices]\n    F[Production Environment] --&gt; G</code></pre> <p>For example, you can centralize the Taskfile modules for different tasks in a single repository, and then you can reuse the same Taskfile modules for different microservices. This way, you can avoid duplicating the same tasks in different microservices, and you can also easily update the tasks in a single place. Also, you don't have to worry about managing the tasks for each microservice separately.</p>"},{"location":"docs/taskfile/variables/","title":"Variables","text":"<p>What happens variables are not defined?</p> <p>If the variables are not defined in the Taskfile, the configuration still remains valid, as the variables can be passed at runtime.</p> Taskfile.yaml<pre><code>version: '3'\ntasks:\n  hello:\n    cmds:\n      - echo \"Hello {{.NAME}}\"\n</code></pre> Bash<pre><code>task hello NAME=KarChunT\n</code></pre>"},{"location":"docs/taskfile/variables/#variable-types","title":"Variable types","text":"<p>Taskfile supports several types of variables:</p> <ul> <li>string</li> <li>bool</li> <li>int</li> <li>float</li> <li>array</li> <li>map</li> </ul> Taskfile.yaml<pre><code>version: '3'\n\nvars: # global vars\n  GLOBAL_VAR: \"I am a global variable\"\n\ntasks:\n  display-variables:\n    vars:\n      STRING_VAR: \"Hello\"\n      BOOL_VAR: true\n      INT_VAR: 42\n      FLOAT_VAR: 3.14\n      ARRAY_VAR: [1, 2, 3]\n      MAP_VAR:\n        map: # You can also write like this map: {A: 1, B: 2, C: 3}\n          key1: value1\n          key2: value2\n    cmds:\n      - 'echo String: {{.STRING_VAR}}'\n      - 'echo Bool: {{.BOOL_VAR}}'\n      - 'echo Int: {{.INT_VAR}}'\n      - 'echo Float: {{.FLOAT_VAR}}'\n      - 'echo Array: {{.ARRAY_VAR}}'\n      - 'echo {{index .ARRAY 0}}' # 1\n      - 'echo Map: {{.MAP_VAR}}'\n      - 'echo Map: {{.MAP_VAR.key1}}' # display key 1 value\n</code></pre>"},{"location":"docs/taskfile/variables/#set-default-values-for-variables","title":"Set default values for variables","text":"Taskfile.yaml<pre><code>version: '3'\ntasks:\n  msg:\n    vars:\n      MESSAGE: '{{.MESSAGE | default \"Hello World\"}}'\n    cmds:\n      - 'echo \"Message is: {{ .MESSAGE }}\"'\n</code></pre> Demo and output<pre><code>ubuntu@touted-mite:~$ task msg \ntask: [msg] echo \"Message is: Hello World\"\nMessage is: Hello World\n\n# override the default value of MESSAGE\nubuntu@touted-mite:~$ task msg MESSAGE=\"Hi, I'm karchunt\"\ntask: [msg] echo \"Message is: Hi, I'm karchunt\"\nMessage is: Hi, I'm karchunt\n</code></pre>"},{"location":"docs/taskfile/variables/#dynamic-variables-shell-command-output","title":"Dynamic variables / Shell command output","text":"<p>You can use <code>sh</code> to execute shell commands and use their output as variable values. This is useful for dynamic data retrieval, such as system information or environment variables.</p> Taskfile.yaml<pre><code>version: '3'\n\ntasks:\n  os-version:\n    vars:\n      OS_VERSION:\n        sh: cat /etc/os-release | grep \"VERSION_ID\" | cut -d \"=\" -f2\n    cmds:\n      - echo \"OS version is {{.OS_VERSION}}\"\n</code></pre> Demo and output<pre><code>ubuntu@touted-mite:~$ task os-version \ntask: [os-version] echo \"OS version is \"22.04\"\"\nOS version is 22.04\n</code></pre>"},{"location":"docs/taskfile/variables/#set-required-variables","title":"Set required variables","text":"<p>You can set variables as required. If the variable is not defined, the task will not run and an error message will be displayed.</p> Taskfile.yaml<pre><code>version: '3'\ntasks:\n  greet:\n    requires:\n      vars:\n        - NAME\n    cmds:\n      - echo \"Hello {{.NAME}}\"\n</code></pre> Demo and output<pre><code>ubuntu@touted-mite:~$ task greet \ntask: Task \"greet\" cancelled because it is missing required variables: NAME\n\nubuntu@touted-mite:~$ task greet NAME=KarChun\ntask: [greet] echo \"Hello KarChun\"\nHello KarChun\n</code></pre>"},{"location":"docs/taskfile/variables/#ensure-variables-have-allowed-values","title":"Ensure variables have allowed values","text":"<p>You can ensure that variables have allowed values by using the <code>enum</code> field. If the variable value is not in the list of allowed values, the task will not run and an error message will be displayed.</p> Taskfile.yaml<pre><code>version: '3'\ntasks:\n  deploy:\n    requires:\n      vars:\n        - name: ENV\n          enum: [dev, staging, prod] # allowed values\n    cmds:\n      - echo \"Deploy to {{.ENV}}\"\n</code></pre> Demo and output<pre><code>ubuntu@touted-mite:~$ task deploy ENV=beta\ntask: Task \"deploy\" cancelled because it is missing required variables:\n  - ENV has an invalid value : 'beta' (allowed values : [dev staging prod])\n\nubuntu@touted-mite:~$ task deploy ENV=dev\ntask: [deploy] echo \"Deploy to dev\"\nDeploy to dev\n</code></pre>"},{"location":"docs/taskfile/variables/#referencing-other-variables","title":"Referencing other variables","text":"<p>You can reference other variables from one task to another. If you are using template engine concept, it will not able to work as expected. Here's an example.</p> Taskfile.yaml<pre><code>version: 3\n\ntasks:\n  foo:\n    vars:\n      FOO: [A, B, C] # &lt;-- FOO is defined as an array\n    cmds:\n      - task: bar\n        vars:\n          FOO: '{{.FOO}}' # &lt;-- FOO gets converted to a string when passed to bar\n  bar:\n    cmds:\n      - 'echo {{index .FOO 0}}' # &lt;-- FOO is a string so the task outputs '91' which is the ASCII code for '[' instead of the expected 'A'\n</code></pre> Demo and Output<pre><code>ubuntu@touted-mite:~$ task foo\ntask: [bar] echo 91\n91\n</code></pre> <p>As you can see, the output is not what we expected. So to reference variables correctly, you should use <code>ref</code> keyword.</p> <p>Taskfile.yaml<pre><code>version: 3\n\ntasks:\n  foo:\n    vars:\n      FOO: [A, B, C] # &lt;-- FOO is defined as an array\n    cmds: # deps ---&gt; same concept\n      - task: bar\n        vars:\n          FOO:\n            ref: .FOO # &lt;-- FOO gets passed by reference to bar and maintains its type\n          BAR:\n            ref: index .FOO 1\n  bar:\n    cmds:\n      - 'echo {{index .FOO 0}}' # &lt;-- FOO is still a map so the task outputs 'A' as expected\n      - 'echo {{.BAR}}' # outputs 'B' as expected\n</code></pre> The same concept applies to <code>deps</code> as well.</p> Demo and Output<pre><code>ubuntu@touted-mite:~$ task foo\ntask: [bar] echo A\nA\ntask: [bar] echo B\nB\n</code></pre>"},{"location":"docs/taskfile/variables/#parsing-jsonyaml-into-map-variables","title":"Parsing JSON/YAML into map variables","text":"<p>Info</p> <p>This only works with <code>ref</code> keyword, not template engine concept.</p> <p>You can parse JSON or YAML strings into map variables using the <code>fromJson</code> or <code>fromYaml</code> functions. This is useful for working with structured data.</p> Taskfile.yaml<pre><code>version: '3'\n\ntasks:\n  task-with-map:\n    vars:\n      JSON: '{\"a\": 1, \"b\": 2, \"c\": 3}'\n      FOO:\n        ref: \"fromJson .JSON\"\n    cmds:\n      - echo {{.FOO}}\n</code></pre> Demo and Output<pre><code>ubuntu@touted-mite:~$ task task-with-map \ntask: [task-with-map] echo map[a:1 b:2 c:3]\nmap[a:1 b:2 c:3]\n</code></pre>"},{"location":"docs/taskfile/warning-prompts/","title":"Warning Prompts","text":"<p>You can prompt the user for confirmation before running a task by using the <code>prompt</code> field in the Taskfile. This is useful for tasks that might have significant consequences, such as deleting files or making changes to a production environment.</p> Taskfile.yaml<pre><code>version: '3'\n\ntasks:\n  example:\n    cmds:\n      - task: not-dangerous\n      - task: dangerous\n      - task: super-dangerous\n      - task: another-not-dangerous\n\n  not-dangerous:\n    internal: true\n    cmds:\n      - echo 'not dangerous command'\n\n  another-not-dangerous:\n    internal: true\n    cmds:\n      - echo 'another not dangerous command'\n\n  dangerous:\n    internal: true\n    prompt: This is a dangerous command... Do you want to continue?\n    cmds:\n      - echo 'dangerous command'\n\n  super-dangerous:\n    prompt:\n      - This is a dangerous command... Do you want to continue?\n      - Please confirm your action.\n    cmds:\n      - echo 'super dangerous command'\n</code></pre> <ul> <li>the <code>prompt</code> field can be a list as well.</li> </ul> Demo and Output<pre><code>ubuntu@touted-mite:~/nodejsfun$ task example \ntask: [not-dangerous] echo 'not dangerous command'\nnot dangerous command\nThis is a dangerous command... Do you want to continue? [y/N]: n\ntask: Failed to run task \"example\": task: Task \"dangerous\" cancelled by user\n\nubuntu@touted-mite:~/nodejsfun$ task example \ntask: [not-dangerous] echo 'not dangerous command'\nnot dangerous command\nThis is a dangerous command... Do you want to continue? [y/N]: y\ntask: [dangerous] echo 'dangerous command'\ndangerous command\nThis is a dangerous command... Do you want to continue? [y/N]: y\nPlease confirm your action. [y/N]: y\ntask: [super-dangerous] echo 'super dangerous command'\nsuper dangerous command\ntask: [another-not-dangerous] echo 'another not dangerous command'\nanother not dangerous command\n</code></pre>"},{"location":"docs/taskfile/watch-tasks/","title":"Watch Tasks","text":"<p>Info</p> <p>If you set <code>watch: true</code> to a task, it will only run from the CLI via <code>task &lt;task-name&gt;</code>, but if you call it from another task either directly or as a dependency, it will not run in watch mode. This is because the watch mode is designed for interactive use, not for automated task execution.</p> <p>Watch tasks in Taskfile allow you to automatically run tasks when files change. This is particularly useful for development workflows where you want to rebuild or recompile your code whenever a file is modified. There are two ways to define watch tasks:</p> <ul> <li><code>--watch</code> or <code>-w</code> flag in the command line</li> <li><code>watch: true</code> field in the Taskfile</li> </ul> <p>The default watch interval is 100 milliseconds, but you can change the interval by setting <code>interval: 200ms</code> in the root of the Taskfile or passing <code>--interval 200ms</code> flag in the command line.</p> Taskfile.yaml<pre><code>version: '3'\ninterval: 200ms\ntasks:\n  dev:\n    watch: true\n    sources:\n      - 'test.txt'\n      - '**/*.py'\n    cmds:\n      - echo \"File changed, running task...\"\n      - echo \"Rebuilding project...\"\n</code></pre>"},{"location":"docs/taskfile/wildcard-args/","title":"Wildcard Arguments","text":"<p>Normally you can pass arguments to a command in Taskfile, but sometimes you may want to pass multiple arguments at once. In such cases, you can use wildcard arguments.</p> <p>The arguments are stored in <code>.MATCH</code> variable.</p> Taskfile.yaml<pre><code>version: '3'\ntasks:\n  service:*:*:\n    vars:\n      SERVICE_NAME: \"{{index .MATCH 0}}\"\n      REPLICAS: \"{{index .MATCH 1}}\"\n    cmds:\n      - 'echo Service Name: {{.SERVICE_NAME}}'\n      - 'echo Replicas: {{.REPLICAS}}'\n  service:*:\n    vars:\n      SERVICE_NAME: \"{{index .MATCH 0}}\"\n    cmds:\n      - 'echo Service Name: {{.SERVICE_NAME}}'\n</code></pre> Demo and Output<pre><code>ubuntu@touted-mite:~$ task service:nginx\ntask: [service:*] echo Service Name: nginx\nService Name: nginx\n\nubuntu@touted-mite:~$ task service:nginx:2\ntask: [service:*:*] echo Service Name: nginx\nService Name: nginx\ntask: [service:*:*] echo Replicas: 2\nReplicas: 2\n</code></pre> Bash<pre><code># You can alsop use whitespace in the arguments as long as you quote them\ntask \"service:nginx hello-world\"\n</code></pre>"}]}