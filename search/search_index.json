{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to KarChunT Wiki","text":"<p>Hi, I'm Kar Chun! I'm an Infrastructure and DevOps Engineer at Intel. Welcome to my website, visit my website for documentation, tutorials, blog and more. I love to code and design software architecture.</p> <p>My ambition is to develop a new technology that can revolutionize the world. As part of my motivation to inspire people, this site shares what I have learned and studied previously. It would be the greatest thing I could ever hope for if someone looked at me and said, <code>Thanks to you, I didn't give up</code>\ud83e\udd73.</p> <p>You can contact me via these channels!</p> <ul> <li>Website</li> <li>GitHub</li> <li>LinkedIn</li> <li>Newsletter</li> <li>Email</li> </ul> <p>Here is my Credly profile, where you can find my certifications and badges. It is my hope that you can find something useful or helpful here. Thank you!</p>"},{"location":"docs/","title":"Documentation","text":"<ul> <li> <p> Secure Shell Protocol (SSH)</p> <p>SSH stands for Secure Shell (SSH) Protocol that is mainly used to connect to a Linux server remotely.</p> <p> Getting Started</p> </li> <li> <p> 12 Factor App</p> <p>It is a methodology for building software-as-a-service applications with best practices.</p> <p> Getting Started</p> </li> <li> <p> Git</p> <p>Git is a distributed version control system that tracks file changes and who made changes.</p> <p> Getting Started</p> </li> <li> <p> Python OOP</p> <p>Object-Oriented Programming (OOP) is a programming paradigm that uses objects and classes to structure software programs.</p> <p> Getting Started</p> </li> <li> <p> Docker</p> <p>Docker is an open platform for developing, shipping, and running applications using containers.</p> <p> Getting Started</p> </li> </ul>"},{"location":"docs/12-factor-app/","title":"12 Factor App","text":"<p>Info</p> <p>Documentation - 12factor.net</p>"},{"location":"docs/12-factor-app/#what-is-12-factor-app","title":"What is 12 Factor App?","text":"<p>It's a methodology for building software-as-a-service (SaaS) or Cloud Native applications by providing a set of best practices to create web apps that are easy to deploy, scalable, maintainable, portable, and resilient.</p> <ul> <li>App can run in different execution environments without having the change the source code (Portability).</li> <li>Suitable for deployment on modern cloud platforms</li> <li>Minimize divergence between development and production</li> <li>Enable continuous deployment</li> <li>Easy to scale up</li> </ul>"},{"location":"docs/12-factor-app/#the-twelve-factors","title":"The Twelve Factors","text":"<p>Developers should consider 12 factors when building SaaS applications in accordance with this methodology. Of course, this methodology is not limited to building SaaS applications, instead, it can apply to other applications as well.</p>"},{"location":"docs/12-factor-app/#codebase","title":"Codebase","text":"<p>In summary, a codebase is always tracked in a version control system such as Git, and every app has only one codebase, but it will be deployed many times.</p> <ul> <li>Codebase = repository</li> <li>One to one relationship between codebase and app</li> <li>I'm referring app = service</li> </ul> <p>Imagine that you have a food web application that collaborates with multiple developers. So far, it only offers food ordering service. However, you need to ensure that the codebase of all developers is the same so that all deployments are consistent. In this case, Git will help all developers work on the same application at the same time, since everyone will push or pull changes from a central location like GitHub, GitLab, etc.</p> Folder structure<pre><code>Food web application/\n\u251c\u2500\u2500 ordering-service/\n\u2502   \u2514\u2500\u2500 source_code.py\n\u251c\u2500\u2500 delivery-service/\n\u2502   \u2514\u2500\u2500 source_code.py\n\u2514\u2500\u2500 payment-service/\n    \u2514\u2500\u2500 source_code.py\n</code></pre> <p>In the future, we may have delivery service, payment service, etc, but now the codebase has all the related services. When you have multiple application services, it's a distributed system and multiple application services sharing the same codebase already violates the 12 Factor.</p> <p></p> <p>So, we can separate those services into its own codebase, and within that codebase, we can have multiple deploys. Therefore, each service in a distributed system can comply with (obey) 12 factor.</p>"},{"location":"docs/12-factor-app/#dependencies","title":"Dependencies","text":"<p>In summary, all dependencies should be explicitly declare and isolate dependencies. It does not rely on implicit existence of system tools or libraries or packages.</p> app.py<pre><code>from fastapi import FastAPI\n\napp = FastAPI(\n  title='Notifications',\n  description='This is notification API',\n  version='0.1.0'\n)\n\nnotifications = [\n  {'name': 'Raymond Melton', 'message': 'Fight and road more hard whose.'},\n  {'name': 'Kevin Dunn', 'message': 'Ten environmental soldier often.'},\n]\n\n@app.get(\"/notifications\")\ndef get_all_notifications():\n  return notifications\n</code></pre> <p>Let's take a look on this example. We're building a backend services using FastAPI Python web framework. Before you start coding, you have to install this FastAPI Python web framework first.</p> <p>If we based on the 12 factor Dependencies concept.</p> <p>It does not rely on implicit existence of system tools or libraries or packages.</p> <p>That means, there is no guarantee that dependencies such as FastAPI will exist on the system where your application will be run. So, you have to declare all dependencies and isolate the dependencies as well. In Python;</p> <ul> <li>pip is used for declaration: <code>pip install -r requirements.txt</code></li> </ul> requirements.txt<pre><code>fastapi==0.45.0\n</code></pre> <ul> <li>virtualenv is used for isolation: <code>python -m venv venv</code>. As a result, we will be able to create an isolated environment for each application with its own version of dependencies.</li> </ul> <p>However, how about those <code>curl</code>, <code>wget</code>, and other tools that are dependent on the system. What we can do, is using Docker, as Docker container is a standalone or executable package of software that has everything like application source code, tools, libraries, dependencies, etc that you need to run an application.</p>"},{"location":"docs/12-factor-app/#config","title":"Config","text":"<p>In summary, we will need to store config in the environment variables, as configuration may be different between deployments.</p> app.py<pre><code>from fastapi import FastAPI\nimport mysql.connector\n\napp = FastAPI(\n  title='Notifications',\n  description='This is notification API',\n  version='0.1.0'\n)\n\n# Creating connection object\nmydb = mysql.connector.connect(\n  host = \"localhost\",\n  user = \"username\",\n  password = \"password\",\n  database = \"test\"\n)\n\ncursorObject = mydb.cursor()\n\n@app.get(\"/notifications\")\ndef get_all_notifications():\n  query = \"SELECT * FROM NOTIFICATIONS\"\n  cursorObject.execute(query)\n  myresult = cursorObject.fetchall()\n  return myresult\n</code></pre> <p>Let's take a look on this example. Right now, we're hard-coded the mysql host, user, password, and database values in the code. This already violates the 12 factor methodology concept, as the database configuration may be different between deployments.</p> .env<pre><code>HOST=localhost\nUSER=username\nPASSWORD=password\nDATABASE=test\n</code></pre> <p>So, we have to keep those configuration separately. What we can do is creating an <code>.env</code> file with those config values and inject this <code>.env</code> file into environment variable in the code.</p> app.py<pre><code>import os\nimport mysql.connector\n\nfrom fastapi import FastAPI\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napp = FastAPI(\n  title='Notifications',\n  description='This is notification API',\n  version='0.1.0'\n)\n\n# Creating connection object\nmydb = mysql.connector.connect(\n  host = os.getenv('HOST'),\n  user = os.getenv('USER'),\n  password = os.getenv('PASSWORD'),\n  database = os.getenv('DATABASE')\n)\n\ncursorObject = mydb.cursor()\n\n@app.get(\"/notifications\")\ndef get_all_notifications():\n  query = \"SELECT * FROM NOTIFICATIONS\"\n  cursorObject.execute(query)\n  myresult = cursorObject.fetchall()\n  return myresult\n</code></pre> <p>With this setup, we can use different configurations for different deployments, without compromising any credentials.</p>"},{"location":"docs/12-factor-app/#backing-services","title":"Backing Services","text":"<p>What is backing services?</p> <p>Any service the app consumes over the network.</p> <p>In summary, we will treat all backing services as attached resources.</p> <p>Example of Backing services;</p> <ul> <li>database (MySQL, PostgreSQL)</li> <li>caching (Redis)</li> <li>Messaging/queueing systems (Kafka, RabbitMQ)</li> <li>SMTP</li> <li>etc</li> </ul> <p>It makes no distinction between local and third party services</p> <p>\u2014 12factor.net</p> <p></p> <p>Imagine you have integrated PostgreSQL service to your application to store your data. A PostgreSQL database is an attached resource to your app, which can be run locally, in the cloud, or on a server. If it is hosted somewhere, it will work without having to change the application code since all the configurations, such as URL, credentials, etc., are stored in a config file like <code>.env</code>.</p>"},{"location":"docs/12-factor-app/#build-release-run","title":"Build, release, run","text":"<p>In summary, we will need to strictly separate the stage of building, release, and running.</p> <p></p> <ul> <li> <p>build stage</p> <ul> <li>Transform or convert the code into an executable or binary format. For example, you have a Python application that's going to run on Windows or Linux server, you can use setup tools to build the application, while in Java, you can use maven or etc to build the application. Of course you can use Docker as well.</li> <li>The build stage compiles assets and binaries based on vendor dependencies and the code.</li> </ul> </li> <li> <p>release stage</p> <ul> <li>It combines the executable file and config file to become the release object.</li> <li>executable + config = release object</li> <li>Every release should have a unique release Id. For example, timestamp (2024-05-11-13-30-10) or incrementing number (v1, v2, v3)</li> <li>Any changes to the codebase must create a new release and once a release has been created, it cannot be modified.</li> </ul> </li> <li> <p>run stage (also known as \"runtime\")</p> <ul> <li>Run the release object in the respective environment. You can deploy and run the release object into different environments and this will ensure that all environments will have the same codebase as they're coming from the same release object.</li> </ul> </li> </ul> <p>With that said, we can easily roll back to the previous releases based on the build artifacts that generated from the build stage.</p>"},{"location":"docs/12-factor-app/#processes","title":"Processes","text":"<p>In summary, applications should be deployed as one or more stateless processes (persist data stored on a backing service) and share-nothing.</p> app.py<pre><code>from flask import Flask\n\napp = Flask(__name__)\ntotal_visit = 0\n\n@app.route(\"/\")\ndef homepage():\n  global total_visit\n  total_visit += 1\n  return \"Welcome to my homepage\"\n\nif __name__ == \"__main__\":\n  app.run(host=\"0.0.0.0\")\n</code></pre> <p></p> <p>Let's take a look on this example. Right now, we have a <code>total_visit</code> global variable, it will increase it each time when the request/user comes in, but this only works with one application. Having multiple applications means they will each have their own version of this variable and they will not sync with each other (record everything separately), as the memory state is different (single-transaction cache).</p> <p>The another scenario is that when the user login to the website, normally we will cache the user session data. But, if we store all these user sessions data in the process memory or filesystem of the app, then when the user is redirected to another application, what will happen is that the user session data isn't available. Of course, it is possible to use sticky sessions to redirect the same user to the same application using load balancers. However, there is the possibility that the application may crash. In that case, all the data or sessions will be lost.</p> <p></p> <p>Sticky sessions are a violation of 12 factor and should never be used or relied upon. So, we should store all the data and session information on a backing service like database or caching system. Redis is a good option for storing session state data as it offers time-expiration.</p>"},{"location":"docs/12-factor-app/#port-binding","title":"Port binding","text":"<p>In summary, we have to export services via port binding, as 12 factor app is completely self-contained (services) and does not rely on a specific web server to function. This means that by declaring the port your application uses, the runtime and development environment know where to access your services. You must do this to ensure portability across platforms and environments.</p> app.py<pre><code>import os\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello_world():\n    return \"&lt;p&gt;Hello, World!&lt;/p&gt;\"\n\nif __name__ == '__main__':\n    app.run(debug=True, port=os.environ.get(\"PORT\", 5000))\n</code></pre> <p>For example, Flask application is using port 5000 by default. So, in the web application, HTTP is exported as a service by binding to a port 5000 and listening for requests. That means in local development environment, you can visit a service url like <code>http://localhost:5000</code> to access the service exported by their app.</p>"},{"location":"docs/12-factor-app/#concurrency","title":"Concurrency","text":"<p>In summary, we have to scale out via the process model. So the applications should scale out horizontally and not vertically by running multiple instances of the application concurrently.</p> <p></p> <p>As an example, you currently have one instance of your application serving multiple users. What if you have more users visiting your application? It's possible to scale your resources vertically by adding resources (RAM, Storage, etc) to the server, but that means the server must be taken down. This approach isn't good.</p> <p></p> <p>Because processes are the first class citizens of the twelve-factor app, we should scale out horizontally by adding/provisioning more servers so we can spin more instances. Of course, you can have a load balancer as well to balance your load across the instances of the application.</p>"},{"location":"docs/12-factor-app/#disposability","title":"Disposability","text":"<p>In summary, we will need to maximize robustness of a system with fast startup and graceful shutdown, as 12 factor app's processes are disposable, meaning they can be started or stopped at a moment's notice.</p> <p></p> <p>It's important for processes to make an effort to minimize startup time by avoiding complex startup scripts for provisioning the application and this concept also applies to reduce instances of the application.</p> <p>Processes shut down gracefully when they receive a SIGTERM signal from the process manager.</p> <p>\u2014 12factor.net</p> <p></p> <p>Here is an example, when executing the <code>docker stop</code> command for Docker containers, Docker initiates the SIGTERM signal to the container initially. If the container does not stop within a grace period, Docker will then send the SIGKILL signal to forcibly terminate the process running within the container.</p> <p>Info</p> <p>SIGTERM -&gt; SIGKILL -&gt; Terminate container process</p> <p>These two signals will allow the application to gracefully shutdown by stopping the acceptance of new requests and ensuring the completion of all ongoing requests. Therefore, this will prevent any impact on users who are waiting for a response from the application.</p>"},{"location":"docs/12-factor-app/#devprod-parity","title":"Dev/prod parity","text":"<p>In summary, we have to keep development, staging, and production environments as similar as possible by applying CI/CD concept.</p> <p></p> <p>Typically, there is a disconnect between the development and production environments, leading to gaps that can be categorized into three distinct areas</p> <ul> <li> <p>Time gap</p> <ul> <li>It could take the developer days, weeks, or even months to finalize code modifications and move them to production.</li> </ul> </li> <li> <p>Personnel gap</p> <ul> <li>Developers are responsible for writing code, while Ops engineers handle the deployment of changes. However, in most cases, they have limited or no knowledge about the new changes, making it difficult to identify issues caused by these new changes.</li> </ul> </li> <li> <p>Tools gap</p> <ul> <li>Different tools used in different environments, often leading to unforeseen outcomes (consequences). For instance, developers may use SQLite and Nginx in development environment, whereas MySQL and HAProxy use in production environment.</li> </ul> </li> </ul> <p>The twelve-factor app is designed for continuous deployment by keeping the gap between development and production small. The twelve-factor developer resists the urge to use different backing services between development and production.</p> <p>\u2014 12factor.net</p> <p>By applying Continuous Integration, Continuous Delivery, and Continuous Deployment. We can</p> <ul> <li>make the time gap small, as the developer can write code and deploy it in a matter of hours or even minutes.</li> <li>make the personnel gap small, the developer who writes the code is actively involved in deploying it and monitoring its behavior in production.</li> <li>make the tools gap small, we must ensure the tools as similar as possible that are being used in development and production environment. (Example: Docker)</li> </ul> Areas Traditional app 12-factor app Time Weeks or Months Hours or minutes Personnel Different people, Developer and Ops Engineers Same people, Developer Tools Different As similar as possible"},{"location":"docs/12-factor-app/#logs","title":"Logs","text":"<p>In summary, we need to treat logs as event streams and leave the execution environment to aggregate.</p> <p></p> <p>Logs give insight into how a running app behaves, such as detecting errors in code and recording all incoming requests. Normally, we save logs in a local file called <code>logfile</code>. However, this method has a drawback. In the era of containers, the container could be terminated at any moment, causing the logs to disappear.</p> <p></p> <p>In a different situation, an application may choose to send logs to a centralized logging system like fluentd. Although this is encouraged, it is not advisable to tightly couple or rely exclusively on that particular logging solution for the application.</p> <p>A twelve-factor app never concerns itself with routing or storage of its output stream.</p> <p>\u2014 12factor.net</p> <p></p> <p>We should</p> <ul> <li>Ensure your logs are stored in a structured format (JSON) and kept in a centralized location for easy access. An agent can easily transfer log data to a central location for querying, consolidating, and analyzing.</li> <li>Avoid writing to a particular file or being restricted to a specific logging system.</li> </ul>"},{"location":"docs/12-factor-app/#admin-processes","title":"Admin processes","text":"<p>In summary, admin/management tasks should be run as one-time processes, stored in source control, and separate them from application processes and they should be running on the same systems as the production application.</p> <p></p> <p>Let's say you discover that the total number of visits recorded in your webpage's PostgreSQL database is inaccurate. In such cases, you can reset the count. To do this, you can create a script that performs the reset as a one-time task. This process is similar to other tasks like database migration or fixing specific issues.</p> <p>You need to ensure the admin task is running on the same systems as the production application, but as a separate process.</p>"},{"location":"docs/docker/best-practices-for-creating-docker-image/","title":"Best Practices for creating a Docker image","text":""},{"location":"docs/docker/best-practices-for-creating-docker-image/#base-vs-parent-vs-custom-image","title":"Base vs Parent vs Custom Image","text":"<p>Note</p> <p>Before we dive into the best practices for creating a Docker image, it is important to understand the difference between base image, parent image, and custom image.</p> <pre><code>graph LR\n  A[scratch] --&gt; B[Base Image - ubuntu:22.04]\n  B --&gt; C[Parent Image - myparent]\n  C --&gt; D[Custom Image]</code></pre> <p>This is an example of base image. We can see that when an image is built from <code>scratch</code> image, it is called as base image.</p> Dockerfile - ubuntu:22.04 (base image)<pre><code>FROM scratch\nADD 433cf0b8353e08be3a6582ad5947c57a66bdbb842ed3095246a1ff6876d157f1 /\nCMD [\"bash\"]\n</code></pre> <p>This is an example of parent image. We can see that when an image is built from base image, it is called as parent image.</p> Dockerfile - mypythonparent (parent image)<pre><code>FROM ubuntu:22.04\nWORKDIR /app\n# Install additional dependencies if needed\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    python3 \\\n    python3-pip &amp;&amp; \\\n    pip3 install -r /app/requirements.txt &amp;&amp; \\\n    apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\nCMD [\"bash\"]\n</code></pre> <p>This is an example of custom image. We can see that when an image is built from parent image, it is called as custom image.</p> Dockerfile - mycustomimage (custom image)<pre><code>FROM mypythonparent\n\n# Copy application files into the container\nCOPY . /app\n\n# Set environment variables\nENV APP_ENV=production \\\n    APP_PORT=8080\n\n# Expose the application port\nEXPOSE 8080\n\n# Set the default command to run the application\nCMD [\"python3\", \"/app/main.py\"]\n</code></pre>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#best-practices-for-creating-a-docker-image_1","title":"Best Practices for Creating a Docker Image","text":""},{"location":"docs/docker/best-practices-for-creating-docker-image/#do-not-build-images-that-combine-multiple-applications","title":"Do not build images that combine multiple applications","text":"<p>Each image should be focused on a single application or service. For example, if you have a web server and a database, create separate images for each service.</p> <p>This allows for better scalability, maintainability, and reusability of images. It also helps in reducing the size of the images, as each image will only contain the necessary dependencies for that specific application.</p>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#do-not-store-data-or-state-in-the-container","title":"Do not store data or state in the container","text":"<p>Containers are ephemeral by nature, meaning they can be created and destroyed at any time. Therefore, if you store data or state in the container, it will be lost when the container is removed or destroyed.</p> <p>Instead, use volumes or any external storage (eg, Redis) solutions to persist data outside of the container.</p>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#keep-images-updated","title":"Keep images updated","text":"<p>Regularly update your images and dependencies to include the latest security patches.</p>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#scan-images-for-vulnerabilities","title":"Scan images for vulnerabilities","text":"<p>Regularly scan your images for vulnerabilities using tools like <code>docker scan</code> or third-party tools like Trivy.</p>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#avoid-installing-unnecessary-packages","title":"Avoid installing unnecessary packages","text":"<p>Only install the dependencies required for your application to run. Remove build tools and temporary files after installation.</p> Docker<pre><code>RUN apt-get update &amp;&amp; apt-get install -y \\\n  curl &amp;&amp; \\\n  apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#use-dockerignore-file","title":"Use <code>.dockerignore</code> file","text":"<p>Exclude unnecessary files and directories from the build context by using a <code>.dockerignore</code> file. This reduces the image size and speeds up the build process.</p> .dockerignore<pre><code>node_modules\n.git\n*.log\n</code></pre>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#leverage-multi-stage-builds","title":"Leverage Multi-Stage Builds","text":"<p>Use multi-stage builds to separate the build environment from the runtime environment, ensuring only the necessary files are included in the final image. This helps in reducing the image size and improving security by not including build tools in the final image.</p> Dockerfile<pre><code># Stage 1: Build\nFROM golang:1.20 AS builder\nWORKDIR /app\nCOPY . .\nRUN go build -o main .\n\n# Stage 2: Runtime\nFROM alpine:latest\nWORKDIR /app\nCOPY --from=builder /app/main .\nCMD [\"./main\"]\n</code></pre>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#optimize-layers","title":"Optimize Layers","text":"<p>Use the <code>RUN</code>, <code>COPY</code>, and <code>ADD</code> commands wisely to minimize the number of layers in your image. Each command creates a new layer, so combining commands can help reduce the overall size of the image, but keep readability in mind.</p> Docker<pre><code>RUN apt-get update &amp;&amp; apt-get install -y curl &amp;&amp; \\\n  apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#set-explicit-tags-for-base-images","title":"Set Explicit Tags for Base Images","text":"<p>Always use specific tags (e.g., <code>ubuntu:22.04</code>) instead of <code>latest</code> to ensure consistent builds and avoid unexpected changes. This helps in maintaining the stability of your application and avoiding compatibility issues that may arise from using the latest version of a base image.</p>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#use-non-root-user","title":"Use non-root user","text":"<p>Avoid running your application as the root user. Create a non-root user for better security. This helps in reducing the attack surface of your application and preventing unauthorized access to sensitive files and directories.</p> Docker<pre><code>RUN addgroup --system appgroup &amp;&amp; adduser --system --ingroup appgroup appuser\nUSER appuser\n</code></pre>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#document-the-image","title":"Document the Image","text":"<p>Use labels to document metadata about the image, such as the maintainer and version. This helps in keeping track of the image and providing information to users about the image.</p> <p>Benefits in Practice</p> <ul> <li>Automation: Tools like CI/CD pipelines can use labels to automate tasks (e.g., deploying specific versions).</li> <li>Compliance: Metadata can include compliance-related information, such as licensing or security details.</li> <li>Discoverability: Documented images are easier to search and identify in container registries.</li> </ul> Docker<pre><code>LABEL maintainer=\"karchunt\"\nLABEL version=\"1.0.0\"\nLABEL description=\"A custom Docker image for my application\"\n</code></pre>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#look-images-with-authenticity","title":"Look images with authenticity","text":"<p>Use official images or verified published tag from Docker Hub or other trusted registries.</p> <p>Use Docker Content Trust (DCT) to ensure that the images you are using are signed and verified. This helps in ensuring the authenticity of the images and preventing the use of malicious images.</p> Bash<pre><code># Enable Docker Content Trust\nexport DOCKER_CONTENT_TRUST=1\n</code></pre>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#use-minimalslim-base-images","title":"Use Minimal/Slim Base Images","text":"<p>Start with a minimal base image like <code>alpine</code> or <code>debian-slim</code> to reduce the image size and surface area for vulnerabilities. Only install the necessary dependencies for your application and remove any unnecessary tools like <code>curl</code>, <code>wget</code>, etc that could be used by attackers to download malicious files.</p>"},{"location":"docs/docker/best-practices-for-creating-docker-image/#use-distroless-images","title":"Use Distroless Images","text":"<p>Distroless Images</p> <p>https://github.com/GoogleContainerTools/distroless</p> <p>Use distroless images for production deployments. Distroless images contain only the application and its runtime dependencies, without any package manager, shell, network tools, text editors or other unwanted programs. This reduces the attack surface and improves security.</p> <p>For example;</p> <ul> <li><code>gcr.io/distroless/python3-debian12</code></li> <li><code>gcr.io/distroless/base-debian12</code></li> </ul>"},{"location":"docs/docker/container/","title":"Container","text":"<p>Info</p> <p>All changes that are made to containers or networks are logged under the Docker System Events.</p>"},{"location":"docs/docker/container/#create-container","title":"Create container","text":"<p>When we create a new container, Docker will create a new directory <code>/var/lib/docker/containers/&lt;id&gt;.json</code> and all the container logs will be stored under that file by default.</p> Bash<pre><code>docker container create &lt;image&gt;\ndocker container create ubuntu\n</code></pre> <p>After the container is created, you have to start the container.</p>"},{"location":"docs/docker/container/#list-container-details","title":"List container details","text":"Bash<pre><code>docker container ls # same as docker ps\ndocker container ls -a # list all containers, including exited containers\ndocker container ls -q # only display container id\n</code></pre>"},{"location":"docs/docker/container/#start-container","title":"Start container","text":"<p>You can get container id from <code>docker container ls</code>.</p> Bash<pre><code>docker container start &lt;container-id&gt;\n</code></pre>"},{"location":"docs/docker/container/#run-a-container","title":"Run a container","text":"<p>run container = create container + start container</p> <p>Some options:</p> <ul> <li><code>-it</code> = interactive terminal (creates and enter the terminal session with the command you specify. This means you can execute commands inside the container while it is still running. Useful for debugging purposes.)</li> <li><code>-d</code> = detach (Run container in background)</li> <li><code>--name</code> = container name</li> <li><code>--rm</code> = remove container after the process is done</li> <li><code>--hostname</code> = setup container hostname</li> <li><code>--user</code> - setup container username</li> <li><code>-p</code> = port mapping</li> <li><code>--env</code> or <code>-e</code> or <code>--env-file</code> = setup environment variables</li> <li><code>--health-cmd</code> and <code>--health-interval</code> = check container state</li> <li><code>--privileged</code> = Login as root user, the Privileged container has full access to all the host's devices and files</li> <li> <p><code>--restart</code> = Restart policy</p> Restart Options Description no The container will never be restarted on-failure When the container fails, it will restart the container always The container will always be restarted unless-stopped It's very similar to \"always\" option, but the container will not restart when it was manually stopped and not even when the Docker Daemon restarts </li> </ul> Bash<pre><code>docker container run &lt;image&gt;\ndocker container run ubuntu # same as docker run ubuntu\n\n# creates and enter the terminal session\ndocker container run -it ubuntu\n\n# bash is the command that runs inside the container, it will start a new Bash shell within the container\ndocker container run -it ubuntu bash\n\n# Run in background with container name = myubuntu\ndocker container run -d --name=myubuntu ubuntu\n\n# remove the container after the process is done\ndocker container run --rm ubuntu\n\n# set hostname\ndocker container run --hostname=myubuntu ubuntu\n\n# Security - setup a username instead of root\ndocker container run --user=1000 ubuntu\n\n# Port mapping\ndocker container run -p &lt;local_port&gt;:&lt;container_port&gt; &lt;image&gt;\ndocker container run -p 80:5000 ubuntu\n\n# the application is only available on the network 192.168.1.*\ndocker container run -p 192.168.1.10:8000:5000 ubuntu\n\n# map the container port to a random port on the host (Ephemeral Port Range 32768 - 60999)\ndocker container run -p 5000 ubuntu\n\n# Check Health intervals (liveness probe)\n# Note: You need to append health-* in front of all parameters and cmd\ndocker run --health-cmd \"curl -f http://localhost:8000\" --health-interval=5s web-ubuntu\n\n# Define environment variables\ndocker run --env &lt;key&gt;=&lt;value&gt; &lt;image&gt;\ndocker run --env PORT=8000 ubuntu\ndocker run -e PORT=8000 ubuntu\n\n# Define environment variables using env file\ndocker run --env-file &lt;filename&gt; &lt;image&gt;\ndocker run --env-file .env ubuntu\n\n# Privileged container\ndocker run --privileged ubuntu\n\n# Restart policy\ndocker run --restart=no ubuntu\n</code></pre>"},{"location":"docs/docker/container/#expose-container-port-capital-p","title":"Expose container port (Capital P)","text":"<p>Normally it will auto-publish the ports of the container on the host, but what port?</p> <ul> <li>So with the Capital P option, it will expose all the ports configured in the Dockerfile (expose instruction) when the image is being built.</li> <li>Docker uses IPTables to map a port on a container to a port on the host and it uses Docker IPTables chains to modify or configure port mapping on a host.</li> </ul> Dockerfile<pre><code>FROM ubuntu:22.04\n\nRUN apt-get update\n\n...\n\nEXPOSE 8000\n</code></pre> Bash<pre><code>docker run -P ubuntuWebApp\n# Add additional ports that were not specific in the Dockerfile\ndocker run -P --expose=5000 ubuntuWebApp\n</code></pre>"},{"location":"docs/docker/container/#rename-container","title":"Rename container","text":"<p>Use <code>docker ps</code> command to get the current container name.</p> Bash<pre><code>docker rename &lt;old-name&gt; &lt;new-name&gt;\ndocker container rename &lt;old-name&gt; &lt;new-name&gt;\n</code></pre>"},{"location":"docs/docker/container/#run-a-new-command-in-a-running-container","title":"Run a new command in a running container","text":"Bash<pre><code>docker exec &lt;container-id&gt; &lt;command&gt;\ndocker container exec &lt;container-id&gt; &lt;command&gt;\ndocker container exec -it &lt;container-id&gt; /bin/bash\n</code></pre>"},{"location":"docs/docker/container/#attach-the-terminals-io-to-a-running-container","title":"Attach the terminal's I/O to a running container","text":"Bash<pre><code>docker attach &lt;container-id&gt;\ndocker container attach &lt;container-id&gt;\n</code></pre> <p>When you attach the terminal's I/O to a running container, you enter the command, it will display the result to all the users who attach back the container. As an example, if you exit the container, all people will leave the container at the same time.</p>"},{"location":"docs/docker/container/#inspect-container","title":"Inspect container","text":"Bash<pre><code>docker inspect &lt;container-id&gt;\ndocker container inspect &lt;container-id&gt;\n</code></pre>"},{"location":"docs/docker/container/#display-a-live-stream-of-containers-resource-usage-statistics","title":"Display a live stream of containers resource usage statistics","text":"<p>It will list containers with CPU, memory, network, and disk consumption.</p> Bash<pre><code>docker stats\ndocker container stats\ndocker container stats &lt;container-id&gt;\ndocker container stats &lt;container-id&gt; &lt;container-id&gt;\n</code></pre>"},{"location":"docs/docker/container/#display-running-processes-of-a-container","title":"Display running processes of a container","text":"<p>Display the processes and their process IDs on the Docker host.</p> Bash<pre><code>docker container top &lt;container-id&gt;\n</code></pre>"},{"location":"docs/docker/container/#container-logs","title":"Container Logs","text":"Bash<pre><code>docker container logs &lt;container-id&gt;\ndocker container logs -f &lt;container-id&gt; # view live logs\n</code></pre>"},{"location":"docs/docker/container/#pause-and-unpause-container","title":"Pause and Unpause container","text":"Bash<pre><code>docker container pause &lt;container-id&gt;\ndocker container unpause &lt;container-id&gt;\n</code></pre>"},{"location":"docs/docker/container/#restart-container","title":"Restart container","text":"Bash<pre><code>docker container restart &lt;container-id&gt;\n</code></pre>"},{"location":"docs/docker/container/#update-container","title":"Update container","text":"<p>Reference</p> <p>https://docs.docker.com/reference/cli/docker/container/update/</p> Bash<pre><code>docker container update --restart always &lt;container-id&gt;\ndocker container update --cpus=1.5 &lt;container-id&gt;\n</code></pre>"},{"location":"docs/docker/container/#stop-remove-and-prune-the-container","title":"Stop, remove, and prune the container","text":"<p>Info</p> <p>SIGTERM -&gt; SIGKILL -&gt; Terminate container process</p> <p>When executing the <code>docker stop</code> command for Docker containers, Docker initiates the SIGTERM signal to the container initially. If the container does not stop within a grace period, Docker will then send the SIGKILL signal to forcibly terminate the process running within the container.</p> Bash<pre><code>docker container stop &lt;container-id&gt;\ndocker container stop $(docker container ls -q) # stop all containers\n\ndocker container rm &lt;container-id&gt;\ndocker container rm $(docker container ls -qa) # remove all containers\n\ndocker container prune # remove all stopped containers\n</code></pre>"},{"location":"docs/docker/daemon-configuration/","title":"Daemon Configuration","text":""},{"location":"docs/docker/daemon-configuration/#docker-service-configuration","title":"Docker Service Configuration","text":"Bash<pre><code>sudo systemctl start docker # start docker service\nsudo systemctl status docker # check docker service status\nsudo systemctl stop docker # stop docker service\n</code></pre>"},{"location":"docs/docker/daemon-configuration/#start-docker-daemon-manually","title":"Start Docker Daemon manually","text":"Bash<pre><code>dockerd\ndockerd --debug # useful for troubleshooting and debugging purposes\n</code></pre>"},{"location":"docs/docker/daemon-configuration/#unix-socket","title":"Unix Socket","text":"<p>Unit socket is an IPC (inter-process communication mechanism) that enables communication between Docker clients on the same host, such as CLI, SDK, and the Docker Daemon.</p> <p>It will listen on an internal Unix Socket at the path <code>/var/run/docker.sock</code> when the Docker daemon starts. So when the container is built, the Docker socket file from the host machine will be mounted into the filesystem of the Docker container, so the Docker container can access to the Docker Daemon API via Docker CLI, as Docker CLI is configured to interact with the Docker Daemon on this socket.</p> <p>Warning</p> <p>Making Docker Daemon accessible outside of the Docker host is not a good approach due to security reasons.</p> <p>By default, the Docker Daemon can only accessible within the same host as it's only listening on the Unix Socket. Of course, Docker Daemon can also listen on a TCP interface on the Docker host.</p> Bash<pre><code>dockerd --debug --host=tcp://192.168.0.196:2375\n</code></pre> <ul> <li><code>192.168.0.196</code> = IP address of the host/machine</li> <li><code>2375</code> = standard port for Docker (unencrypted traffic)</li> </ul> <p>Then, other hosts can trigger any Docker commands to this Docker host by targeting their Docker Daemon to the TCP interface.</p> Bash<pre><code>export DOCKER_HOST=\"tcp://192.168.0.196:2375\"\nexport DOCKER_TLS=true # initiate secure connection\n</code></pre> <p>You can fix the TCP security issue (unencrypted traffic) by setting up TLS encryption to Docker Daemon.</p> Bash<pre><code>dockerd --debug \\\n  --host=\"tcp://192.168.0.196:2376\" \\\n  --tls=true \\\n  --tlscert=\"/var/docker/server.pem\" \\\n  --tlskey=/var/docker/serverkey.pem\n</code></pre> <ul> <li>When TLS is enabled, remember to change the port to 2376, as it's the standard port for encrypted traffic.</li> </ul> <p>However, there is still no authentication for others. Therefore, they can do whatever they want with Docker Daemon straight away. So, we have to enable certificate-based authentication.</p> Bash<pre><code>dockerd --debug \\\n  --host=\"tcp://192.168.0.196:2376\" \\\n  --tls=true \\\n  --tlscert=\"/var/docker/server.pem\" \\\n  --tlskey=\"/var/docker/serverkey.pem\" \\\n  --tlsverify=true \\\n  --tlscacert=\"/var/docker/caserver.pem\"\n</code></pre> <ul> <li><code>tlsverify</code> = enable authentication</li> <li><code>tlscacert</code> = use to verify client certificates. Therefore, the client will only be able to access Docker Daemon when they have the respective certificate.</li> </ul> <p>On the client side, we will have to generate certificates for them called <code>client.pem</code> and <code>clientkey.pem</code>. After that, they will have to <code>export DOCKER_TLS_VERIFY=true</code>.</p> <p>Sometimes, it's hard for users to memorize and manually insert all those options and configurations. Therefore, those options and configurations can move to a file, <code>/etc/dockers/daemon.json</code>.</p> /etc/dockers/daemon.json<pre><code>{\n  \"debug\": true,\n  \"hosts\": [\"tcp://192.168.0.196:2376\"],\n  \"tls\": true,\n  \"tlscert\": \"/var/docker/server.pem\",\n  \"tlskey\": \"/var/docker/serverkey.pem\",\n  \"tlsverify\": true,\n  \"tlscacert\": \"/var/docker/caserver.pem\",\n  \"live-restore\": true // the container will continue to run even Docker Daemon stops\n}\n</code></pre> <p>If you continue to specify those options and configurations via <code>dockerd</code> command, it will display an error message.</p> <p>Once you edit this file <code>/etc/dockers/daemon.json</code>, remember to reload Docker.</p> Bash<pre><code>sudo systemctl reload docker\n# see your docker info after configuration\ndocker system info\n</code></pre>"},{"location":"docs/docker/daemon-configuration/#logging-driver","title":"Logging Driver","text":"<p>Info</p> <p><code>docker logs</code> command is used to get container logs.</p> <p>Docker Logging Driver Configuration</p> <p>Docker Daemon has a default logging driver, which is json-file. You can use <code>docker system info</code> to get the current logging driver. All the containers logs are stored under this file <code>/var/lib/docker/containers/&lt;id&gt;.json</code> by default.</p> Bash<pre><code>cat &lt;id&gt;.json\n</code></pre> <p>Of course, there are multiple logging driver options that the user can change;</p> <ul> <li>json-file (default)</li> <li>none</li> <li>syslog</li> <li>local</li> <li>journald (docker logs)</li> <li>splunk</li> <li>awslogs</li> </ul> /etc/docker/daemon.json<pre><code>{\n  \"log-driver\": \"awslogs\",\n  \"log-opt\": {\n    // additional options for logging region\n    \"awslogs-region\": \"ap-southeast-1\"\n  }\n}\n</code></pre> <p>With this setup, all the container logs will be sent to Amazon CloudWatch Logs.</p>"},{"location":"docs/docker/daemon-configuration/#storage-driver","title":"Storage Driver","text":"<p>Reference</p> <p>Docker uses the storage drivers to store image layers and to store data in the writable layer of a container. The storage driver controls how images and containers are stored and managed on your Docker host.</p> <p>\u2014 docker.docs</p> <p>Supported storage drivers;</p> <ul> <li>overlay2</li> <li>btrfs and zfs</li> <li>vfs</li> <li>fuse-overlayfs</li> </ul> <p>Proceed to this <code>/etc/docker/daemon.json</code> file to change the storage driver.</p> /etc/docker/daemon.json<pre><code>{\n  \"storage-driver\": \"overlay2\"\n}\n</code></pre>"},{"location":"docs/docker/daemon-configuration/#troubleshoot-docker-daemon","title":"Troubleshoot Docker Daemon","text":""},{"location":"docs/docker/daemon-configuration/#view-docker-daemon-logs","title":"View Docker Daemon Logs","text":"Bash<pre><code>journalctl -u docker.service\n</code></pre>"},{"location":"docs/docker/daemon-configuration/#check-free-disk-space-on-host","title":"Check free disk space on host","text":"Bash<pre><code>df -h\n</code></pre>"},{"location":"docs/docker/docker-architecture/","title":"Docker Architecture","text":""},{"location":"docs/docker/docker-architecture/#docker-engine-architecture","title":"Docker Engine Architecture","text":"<p>Docker Engine is the heart of the container and it consists of 3 core elements;</p> <ul> <li>Docker CLI<ul> <li>A command line interface that the user will use to run the commands to manage Docker objects.</li> </ul> </li> <li>REST API<ul> <li>Enables the communication between applications and Docker and gives Dockerd instructions.</li> </ul> </li> <li>Docker Daemon (dockerd) -&gt; Server<ul> <li>It's the server responsible for creating and managing objects.</li> <li>It's the heart of Docker.</li> </ul> </li> </ul>"},{"location":"docs/docker/docker-architecture/#containerd","title":"containerd","text":"<p>Open Container Initiative (OCI)</p> <p>A set of open industry standards around container formats and runtime. OCI created two specifications to make the creation of container standards more efficient.</p> <ul> <li>Runtime-spec</li> <li>Image-spec</li> </ul> <p>These two specifications mainly defines the lifecycle of a container/image technology. For example, delete command should delete a container/image, etc.</p> <p>It manages the container lifecycle (start, stop, pause, delete), image distribution (push, pull to/from registries).</p> <p>When the user makes a request to dockerd, containerd will push/pull the image to/from registries, and convert the image that was downloaded into an OCI compliance bundle.</p>"},{"location":"docs/docker/docker-architecture/#libcontainerrunc","title":"libcontainer/runC","text":"<p>At the very beginning, Docker has a monolithic architecture and used LXC (Linix Container) technology to build environments for applications. After a while, the architecture of Docker was modified to a modular design, allowing for quicker innovation. Also, they replaced LXC with libcontainer as the default execution environment, now known as runC.</p> <p>What is cgroups and namespaces?</p> <p>cgroups - Resource allocation for a given process can be easily monitored and managed via Linux OS, and resource limits can be set for memory, CPU, and network resources.</p> <p>namespaces - Isolating processes from one another. In each container, processes will run in their own namespace and won't be able to access anything outside of its namespaces. (Docker containers have network isolation (via libnetwork), allowing separate virtual interfaces and IP addresses for each container, that means that the container will have its own virtual interfaces, routing and ARP tables).</p> <p>Docker use namespaces to isolate</p> <ul> <li>Process Id (pid) - Two proccesses cannot have the same process Id, with the help of namespaces, each process can have multiple process IDs associated with it.</li> <li>Unix Timesharing Systems (UTS)</li> <li>Mount (mnt)</li> <li>Inter-Process Communication (IPC)</li> <li>Network (net)</li> </ul> <p>runC is a lightweight CLI and it's used to create and run containers. The user can use the CLI to spawn and run the containers without Docker, so it can be easily integrated with higher-level container orchestration systems like Kubernetes.</p> <ul> <li>It will interact with the cgroups and namespaces on the kernel levels to create and run a container.</li> <li>In each execution, <code>/tmp/docker/[uuid]/</code> is created as the container's root file system.</li> </ul>"},{"location":"docs/docker/docker-architecture/#containerd-shim","title":"containerd-shim","text":"<p>containerd-shim is mainly use to make the containers daemon less, monitors the state of the container, and it is in charge of handling input (STDIN) and output (STDOUT) and notifying the Docker Daemon about the exit status.</p> <p>It mainly takes care of the containers when the daemon is down or restarted. That means, the containers will run in the background and will be attached back to the daemon when it comes back or online.</p> <p>How containerd-shim make the container become daemonless container?</p> <ul> <li>Each time a container is created, containerd forks an instance of runC</li> <li>After runC creates the container, the runC process will exit, and shim will replace runC and become the new container parent.</li> </ul>"},{"location":"docs/docker/docker-architecture/#docker-objects","title":"Docker Objects","text":"<p>Docker Objects consists of 4 core elements;</p> <ul> <li>Images</li> <li>Containers</li> <li>Volumes</li> <li>Networks</li> </ul>"},{"location":"docs/docker/docker-architecture/#images","title":"Images","text":"<p>What is Dockerfile?</p> <p>A Dockerfile is a text file that will create a custom Docker image from a set of commands or instructions. Each instruction in the Dockerfile will represent a layer of the Docker image.</p> <p>Docker image acts as a set of instructions to build a Docker container, you can say it is a read-only template.</p> <p>The Docker image contains all the necessary components for the application to run as a container, including the source code, tools, libraries, dependencies, and more. The user can build an image from a Dockerfile through <code>docker build</code> command.</p>"},{"location":"docs/docker/docker-architecture/#containers","title":"Containers","text":"<p>Docker Container is an instance of an image running as a process. It is a standalone or executable package of software that has everything like application source code, tools, libraries, dependencies, runtime, settings, etc that you need to run an application.</p>"},{"location":"docs/docker/docker-architecture/#volumes","title":"Volumes","text":"<p>Docker volume is used to persist and share the container's data across containers. Folders on your host machine's hard drive are mounted into containers as volumes, which allows the container to write its data into the host volumes.</p> <p>Benefits: Volumes are easy to backup.</p>"},{"location":"docs/docker/docker-architecture/#networks","title":"Networks","text":"<p>Docker networking enables a container to be able to communicate with other containers. It can link a Docker container to as many networks as the user requires. It's used to create an isolated environment (provide an isolation) for Docker containers.</p> <p>Example of Docker network drivers;</p> <ul> <li>Bridge</li> <li>Host</li> <li>None</li> <li>Overlay</li> <li>Macvlan</li> </ul>"},{"location":"docs/docker/docker-architecture/#registry","title":"Registry","text":"<p>Note</p> <p>Docker Enterprise Edition provides a private trusted registy known as Docker Trusted Registry (DTR).</p> <p>Docker images are stored in a registry via <code>docker push</code> command, which enables the sharing and publishing of images either publicly or within a private organization.</p> <p>Example;</p> <ul> <li>Docker Hub</li> <li>Harbor</li> </ul>"},{"location":"docs/docker/docker-compose/","title":"Docker Compose","text":"<p>Info</p> <p>Use to startup multiple Docker containers at the same time.</p>"},{"location":"docs/docker/docker-compose/#composeyaml","title":"compose.yaml","text":"<ul> <li>Reference </li> <li>Examples</li> </ul> <p>Note</p> <p>All configurations here are the same from the <code>docker run</code> command. Currently, I'm showing docker compose v2 instead of v1. So some syntax might be different due to some syntax already deprecated.</p> <p>A default network is created for all the composed containers. that means all containers that are created will be automatically added to that network.</p> compose.yaml<pre><code>name: apiapp\nservices:\n  postgres:\n    image: postgres:14\n    networks:\n      - db\n    environment:\n      POSTGRES_USER: sonar\n      POSTGRES_PASSWORD: sonar\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n  sonarqube:\n    image: sonarqube:lts\n    ports:\n      - '9000:9000'\n      - '9092:9092'\n    networks:\n      - db\n    environment:\n      SONAR_JDBC_URL: jdbc:postgresql://postgres:5432/sonar\n      SONAR_JDBC_USERNAME: sonar\n      SONAR_JDBC_PASSWORD: sonarpasswd\n    depends_on:\n      - postgres\n  apiapplication:\n    build: ./\n    volumes:\n      - /app/node_modules\n      - ./app:/app\n    depends_on:\n      - postgres\n  server:\n    build:\n      context: ./server # system look dir to search for Dockerfile\n      dockerfile: Dockerfile.dev\nnetworks:\n  db:\nvolumes:\n  postgres_data:\n</code></pre> <p>Additional;</p> <ul> <li>If you want to specify <code>-it</code> in <code>compose.yaml</code>, you will have to specify these two fields (mandatory)<ul> <li><code>stdin_open: true</code> - This service needs an open input connection</li> <li><code>tty: true</code> - Attaching this terminal</li> <li><code>-it</code> = <code>stdin_open</code> + <code>tty</code></li> </ul> </li> <li>If you want to override the <code>Dockerfile</code> entrypoint, you can specify <code>entrypoint</code> within <code>compose.yaml</code>, then it will override the entrypoint in <code>Dockerfile</code> that is defined.</li> </ul>"},{"location":"docs/docker/docker-compose/#commands","title":"Commands","text":"<p>Reference</p> <p>Use <code>docker compose</code> instead of <code>docker-compose</code>.</p>"},{"location":"docs/docker/docker-compose/#list-containers","title":"List containers","text":"Bash<pre><code>docker compose ps\n</code></pre>"},{"location":"docs/docker/docker-compose/#build-or-rebuild-containers","title":"Build or rebuild containers","text":"<p>It only builds containers without starting it.</p> Bash<pre><code>docker compose build\n</code></pre>"},{"location":"docs/docker/docker-compose/#create-and-start-containers","title":"Create and start containers","text":"<p>Up = Build/rebuild + start</p> Bash<pre><code>docekr compose up\ndocker compose up --build # rebuild containers (build images before starting containers)\ndocker compose -d up # running in backgound\ndocker compose -f &lt;compose-file&gt; up\ndocker compose up &lt;service&gt; &lt;service&gt; # start only a few service\n</code></pre>"},{"location":"docs/docker/docker-compose/#execute-a-command-in-a-running-container","title":"Execute a command in a running container","text":"<p>By default, it will allocate a TTY by default, so you can straight away get an interactive prompt from this command <code>docker compose exec apiservice bash</code>.</p> Bash<pre><code>docker compose exec &lt;service-name&gt; &lt;commands&gt;\ndocker compose exec -it apiservice bash # old version\ndocker compose exec apiservice bash # new version\n</code></pre>"},{"location":"docs/docker/docker-compose/#display-service-log-output","title":"Display service log output","text":"Bash<pre><code>docker compose logs\n</code></pre>"},{"location":"docs/docker/docker-compose/#stop-services","title":"Stop services","text":"<p>Stop running containers without removing them. The services can be started again with <code>docker compose start</code>.</p> Bash<pre><code>docker compose stop\n</code></pre>"},{"location":"docs/docker/docker-compose/#stop-and-remove-containers-networks","title":"Stop and remove containers, networks","text":"Bash<pre><code>docker compose down\n</code></pre>"},{"location":"docs/docker/docker-installation/","title":"Docker Installation","text":"<p>Info</p> <p>Here is the link for installing Docker Engine. I will only show how to install Docker Engine on Ubuntu, as installing Docker on Windows is very straightforward.</p>"},{"location":"docs/docker/docker-installation/#install-docker-engine-on-ubuntu","title":"Install Docker Engine on Ubuntu","text":"<p>All the following steps are coming from this reference.</p> <ol> <li> <p>Setup Docker's <code>apt</code> repository</p> Bash<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n    \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n    $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n    sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\nsudo apt-get update\n</code></pre> </li> <li> <p>Install the Docker packages.</p> Bash<pre><code>sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> </li> <li> <p>Verify that the Docker Engine installation is successful by running the hello-world image.</p> Bash<pre><code>sudo docker run hello-world\n</code></pre> </li> <li> <p>[Optional] Manage Docker as a non-root user.</p> Bash<pre><code>sudo groupadd docker\nsudo usermod -aG docker $USER\nnewgrp docker\n\ndocker run hello-world # without sudo\n</code></pre> </li> </ol>"},{"location":"docs/docker/future/","title":"Future","text":"<p>My Docker documentation does not stop here, there are still a lot of things I haven't gone through, especially Docker Swarm, Docker Engine Enterprise, Docker updates, etc. Stay tuned!</p>"},{"location":"docs/docker/image/","title":"Image","text":""},{"location":"docs/docker/image/#image-operations","title":"Image Operations","text":""},{"location":"docs/docker/image/#list-images","title":"List images","text":"Bash<pre><code># All these commands are the same\ndocker image ls\ndocker image list\ndocker images\n</code></pre>"},{"location":"docs/docker/image/#search-images","title":"Search images","text":"<p>Reference</p> Bash<pre><code>docker search &lt;image&gt;\ndocker search --limit 2 &lt;image&gt;\ndocker search --filter stars=2 --filter is-official=true &lt;image&gt;\n</code></pre>"},{"location":"docs/docker/image/#pull-download-the-image","title":"Pull / Download the image","text":"Bash<pre><code>docker image pull &lt;image&gt;\ndocker pull &lt;image&gt;\n</code></pre>"},{"location":"docs/docker/image/#push-image","title":"Push image","text":"Bash<pre><code>docker image push &lt;image&gt;\ndocker push &lt;image&gt;\n</code></pre>"},{"location":"docs/docker/image/#tag-image","title":"Tag image","text":"Bash<pre><code># same as docker tag\ndocker image tag &lt;image:tag&gt; &lt;newimage:newtag&gt;\ndocker image tag &lt;image:tag&gt; &lt;newimage&gt;\n\ndocker image tag ubuntu:22.10 gcr.io/karchunt/ubuntu:latest\n</code></pre>"},{"location":"docs/docker/image/#inspect-image","title":"Inspect image","text":"<p>It will display detailed information on one or more images.</p> Sample JSON after you docker image inspect<pre><code>[\n  {\n    \"Id\": \"sha256:bf3dc08bfed031182827888bb15977e316ad797ee2ccb63b4c7a57fdfe7eb31d\",\n    \"ContainerConfig\": {\n      \"Hostname\": \"4329d013a36a\",\n      \"Domainname\": \"\",\n      ...\n    },\n    \"Architecture\": \"amd64\",\n    \"Os\": \"linux\",\n    ...\n  }\n]\n</code></pre> Bash<pre><code>docker image inspect &lt;image&gt;\ndocker image inspect &lt;image&gt; -f '{{.Os}}' # retrieve OS\ndocker image inspect &lt;image&gt; -f '{{.ContainerConfig.Hostname}}'\n</code></pre>"},{"location":"docs/docker/image/#remove-image-and-remove-all-unused-image","title":"Remove image and remove all unused image","text":"<p>Before deleting an image, all containers must be removed or deleted first, as they are dependent on that image.</p> Bash<pre><code>docker image rm &lt;image&gt;\ndocker image prune -a\n</code></pre>"},{"location":"docs/docker/image/#display-image-layers","title":"Display image layers","text":"Bash<pre><code>docker image history &lt;image&gt;\ndocker history &lt;image&gt;\n</code></pre>"},{"location":"docs/docker/image/#save-or-load-image","title":"Save or load image","text":"<p>Imagine you are in an environment without access to Wifi to download the image. With image save commands, you can convert your image into a tar file, copy that to your environment, and then extract it.</p> Bash<pre><code>docker image save &lt;image&gt; -o &lt;tarfile-path&gt;\ndocker image load -i &lt;tarfile-path&gt;\n\n# example\ndocker image save ubuntu:latest -o ubuntu.tar\ndocker image load -i ubuntu.tar\n</code></pre>"},{"location":"docs/docker/image/#convert-container-into-image-in-a-tar-format-using-import-and-export-operations","title":"Convert container into image in a tar format using Import and Export operations","text":"<p>Info</p> <p>Make or modify the image to a single layer.</p> <p>With import and export commands, basically, we are flattening a Docker image into a single layer, therefore, we will get a smaller size of the image.</p> <p>Note</p> <p>You can also export the exited container as well.</p> <p>It only exports the contents of the directory, without the contents of the volume.</p> Bash<pre><code># You will see a lot of layers inside, for example: ubuntu\ndocker image history &lt;image&gt;\n\n# run the container\ndocker container run &lt;image&gt;\n\n# export a container as a tar archive\ndocker container export &lt;container&gt; &gt; &lt;tarfile-path&gt;\ndocker container export &lt;container&gt; -o &lt;tarfile-path&gt;\ndocker container export ubuntu_container_name &gt; ubuntu.tar\n\n# import tar archive into image\ndocker image import &lt;tarfile-path&gt; &lt;new-image&gt;\ndocker image import ubuntu.tar newubuntu:latest\n</code></pre>"},{"location":"docs/docker/image/#image-naming-convention-and-authenticate-to-registries","title":"Image naming convention and Authenticate to registries","text":"<p>Reference</p> <p>It's made up of a slash-separated name components. Before you push those images to respective registries, you have to perform authentication to the registry that you want to push.</p> <p>For example;</p> <ul> <li>Harbor: \"harbor_address\"/project/repository<ul> <li>\"karchunt.registry.com/python/auto-deploy\"</li> </ul> </li> <li>Dockerhub: \"docker.io/username/repository\"<ul> <li>\"docker.io/karchunt/maven-with-docker\"</li> </ul> </li> </ul> Bash<pre><code># By default us docker.io\ndocker login\ndocker login &lt;registry-address/server&gt;\n\n# example\ndocker login karchunt.registry.com\n</code></pre> <p>Once you login successfully, those credentials will be stored in <code>$HOME/.docker/config.json</code> on Linux or <code>%USERPROFILE%/.docker/config.json</code> on Windows.</p> Naming registry project/user/account image/repository karchunt.registry.com/python/auto-deploy karchunt.registry.com python auto-deploy docker.io/karchunt/maven-with-docker docker.io karchunt maven-with-docker"},{"location":"docs/docker/image/#dockerfile-build-a-custom-image","title":"Dockerfile (Build a custom image)","text":"<p>Reference</p> <p>Dockerfile is a text file that will consist of all the steps that are required to build a custom image. Here is a very basic and sample Dockerfile to dockerize Flask application.</p> Dockerfile<pre><code>FROM python:3.11.0-slim-bullseye\n\nARG PORT=5000\n\nWORKDIR /app\nCOPY requirements.txt ./\n\nRUN pip install -r requirements.txt\n\nCOPY ./ ./\nEXPOSE $PORT\n\nENTRYPOINT [\"python\"]\nCMD [\"app.py\"]\n</code></pre> <p>When you run the <code>docker build</code> command, all files under the build context are transferred to the Docker Daemon. It stores them in <code>/var/lib/docker/tmp</code> for temporary storage. Docker looks for <code>Dockerfile</code> at whatever path is specified in the build context.</p> Bash<pre><code>docker image build -t &lt;name:tag or just name&gt; &lt;PATH&gt;\ndocker build -t &lt;name:tag or just name&gt; &lt;PATH&gt;\n# ARG (available inside of Dockerfile)\ndocker build --build-arg &lt;key&gt;=&lt;value&gt; &lt;PATH&gt;\n\n# \".\" or any path is build context\ndocker build -f &lt;Dockerfile-path&gt; &lt;folder&gt;\n\n# build with no cache\ndocker build --no-cache &lt;PATH&gt;\n\n# Get Dockerfile from repo\ndocker build &lt;repo-url&gt;\ndocker build &lt;repo-url#&lt;branch&gt;&gt;\ndocker build &lt;repo-url:&lt;folder&gt;&gt;\n\n# build the image for only 1 or more stages\ndocker build --target &lt;stage-name&gt; &lt;PATH&gt;\n\n# Example\ndocker build -t order-api:v0.0.1 .\ndocker build --build-arg PORT=8000 ./\ndocker build https://github.com/karchunt/app\ndocker build https://github.com/karchunt/app#develop\ndocker build https://github.com/karchunt/app:manifests\ndocker build -f Dockerfile.prod /folder1\ndocker build --target deployment ./\n</code></pre> <ul> <li>You can also create <code>.dockerignore</code> file to tell the build context to exclude or ignore those files or directories.</li> </ul>"},{"location":"docs/docker/image/#dockerfile-explanation","title":"Dockerfile explanation","text":"Instruction Description ADD Add local or remote files and directories. ARG Use build-time variables. This argument will be used during the <code>docker build</code> section, to remove those hardcoded values CMD Specify default commands should be executed when container is running. COPY Copy files and directories. ENTRYPOINT Specify default executable. ENV Set environment variables. EXPOSE Describe which ports your application is listening on. It does not actually publish the port. FROM Create a new build stage from a base image. Our image will be customized using this initial set of programs or tools HEALTHCHECK Check a container's health on startup. LABEL Add metadata to an image and it's a key value pair. MAINTAINER Specify the author of an image. ONBUILD Specify instructions for when the image is used in a build. RUN Execute build commands. SHELL Set the default shell of an image. STOPSIGNAL Specify the system call signal for exiting a container. USER Set user and group ID. VOLUME Create volume mounts that need specified folder to be persistent inside container. We also need to specify <code>docker run -v &lt;path&gt;:&lt;path-in-container&gt;</code> WORKDIR Change working directory."},{"location":"docs/docker/image/#workdir","title":"WORKDIR","text":"<p>It can be used multiple times in a <code>Dockerfile</code>.</p> Docker<pre><code>WORKDIR /app\nWORKDIR main # it will display warning as the path is absolute\n\nRUN pwd # output = /app/main, it will stack\n</code></pre>"},{"location":"docs/docker/image/#healthcheck","title":"HEALTHCHECK","text":"<p>Basically, it will check a container's health on startup by telling the platform on how to test the application is healthy. It will also monitor the container process when it's running.</p> <p>Parameters for HEALTHCHECK</p> <ul> <li><code>--interval=DURATION</code> (default: 30s)</li> <li><code>--timeout=DURATION</code> (default: 30s)</li> <li><code>--start-period=DURATION</code> (default: 0s)</li> <li><code>--retries=N</code> (default: 3)</li> </ul> Exit status Description 0 success 1 failure 2 reserved (do not use the exit code)"},{"location":"docs/docker/image/#copy-vs-add","title":"COPY vs ADD","text":"<p>Info</p> <p>In Dockerfile, <code>COPY</code> is recommended over <code>ADD</code> to reduce layer count</p> <p>Both of them are just copying files, but the <code>ADD</code> instruction will have more usage compared to <code>COPY</code>. <code>COPY</code> just lets you copy, while <code>ADD</code> can auto extract the tar file into the path inside the image. For URL, it will only download, but does not perform the extraction.</p> Dockerfile<pre><code>FROM ubuntu\n\nCOPY myfile.txt /app\n\nADD newfile.txt /app\nADD archive.tar /app\nADD https://mysamplearchive.tar /app\n</code></pre>"},{"location":"docs/docker/image/#cmd-vs-entrypoint-utility-container","title":"CMD vs ENTRYPOINT (Utility container)","text":"<p>A lot of people confuse <code>CMD</code> and <code>ENTRYPOINT</code> instructions.</p> Dockerfile<pre><code>FROM ubuntu\nENTRYPOINT [\"sleep\"]\nCMD [\"5\"]\n</code></pre> <p>ENTRYPOINT</p> <ul> <li>Specify the default executable, which means setting the image's main command. It cannot be overridden.</li> </ul> <p>CMD</p> <ul> <li>Able to override, for example <code>docker run &lt;image&gt; 7</code>, \"7\" will be replaced <code>CMD</code> command that you specify in <code>Dockerfile</code>.</li> </ul>"},{"location":"docs/docker/image/#build-cache","title":"Build cache","text":"<p>Every layer in the Dockerfile has a cache. It will compare instructions in Dockerfile and checksums of files in <code>ADD</code> or <code>COPY</code>. If the instructions have been modified, then it will rebuild that layer.</p> Bash<pre><code>docker build &lt;PATH&gt; # with cache\ndocker build --no-cache &lt;PATH&gt; # build with no cache\n</code></pre>"},{"location":"docs/docker/image/#multi-stage-builds","title":"Multi-stage builds","text":"<p>Multi-stage builds will help to generate smaller images. It makes the developers easy to read and maintain as you only keep the required dependencies, therefore resulting in a more secure container.</p> Dockerfile<pre><code>###### build stage\nFROM python:3.11.0-slim-bullseye as build\n\nENV VIRTUAL_ENV=/app\nENV PATH=\"$VIRTUAL_ENV/venv/bin:$PATH\"\n\nWORKDIR $VIRTUAL_ENV\n\nRUN apt-get update \\\n    &amp;&amp; apt-get install -y libpq-dev python3-psycopg2 \\\n    &amp;&amp; python -m venv $VIRTUAL_ENV/venv\n\nCOPY requirements.txt .\n\nRUN pip install --upgrade pip \\\n    &amp;&amp; pip install --no-cache-dir --upgrade -r requirements.txt\n\n###### base environment\nFROM python:3.11.0-slim-bullseye as base\n\n# Create system user and group\nRUN groupadd -g 999 apiuser \\\n    &amp;&amp; useradd -r -u 999 -g apiuser apiuser\n\nRUN mkdir /app &amp;&amp; chown apiuser:apiuser /app\nWORKDIR /app\n\n# 0 means build stage\n# You can also use 0, COPY --chown=apiuser:apiuser --from=0 /app/venv ./venv\nCOPY --chown=apiuser:apiuser --from=build /app/venv ./venv\nCOPY --chown=apiuser:apiuser . .\n\n# system user\nUSER 999\n\nEXPOSE 8000\n\nENV PATH=\"/app/venv/bin:$PATH\"\n\n###### testing environment\nFROM base as test\nRUN pip install --no-cache-dir --upgrade -r requirements-test.txt\nENTRYPOINT [ \"pytest\" ]\nCMD [\"-v\", \"--disable-pytest-warnings\"]\n\n##### development environment\nFROM python:3.11.0-slim-bullseye as development\nCMD [\"uvicorn\", \"src.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--reload\"]\n\n###### deployment environment\nFROM base as deployment\nCMD [\"uvicorn\", \"src.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\nHEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 CMD [ \"curl\", \"-f\", \"http://localhost:8000/healthcheck\" ]\n</code></pre>"},{"location":"docs/docker/image/#create-custom-image-from-running-container","title":"Create custom image from running container","text":"<p>Reference</p> <p>Warning</p> <p>Create custom image from running container is not recommended. Please use Dockerfile instead.</p> <p>When you change files or settings inside the container, <code>docker commit</code> command can be useful to commit them to a\u00a0new image. Do take note that, the processes within the container will be paused when the image is being committed.</p> Bash<pre><code>docker container commit &lt;container-id&gt; &lt;new-image-name&gt;\n# change the default command in container\ndocker container commit -a \"Author\" -c 'CMD [\"sleep\", \"5\"]' &lt;container-id&gt; &lt;new-image-name&gt;\n</code></pre> <p>The <code>--change</code> or <code>-c</code> option will apply <code>Dockerfile</code> instructions to the image that is created, it's supported <code>Dockerfile</code> instructions.</p> <ul> <li><code>CMD</code></li> <li><code>ENTRYPOINT</code></li> <li><code>ENV</code></li> <li><code>EXPOSE</code></li> <li><code>LABEL</code></li> <li><code>ONBUILD</code></li> <li><code>USER</code></li> <li><code>VOLUME</code></li> <li><code>WORKDIR</code></li> </ul>"},{"location":"docs/docker/network/","title":"Network","text":"<p>Info</p> <p>By default, Docker will create three networks automatically after installing Docker successfully.</p> <ul> <li>bridge</li> <li>host</li> <li>none</li> </ul> <p>There are two more network types;</p> <ul> <li>overlay - Multiple Docker Daemon hosts (Docker running on different machines) are able to connect with each other.</li> <li>macvlan - Set a custom MAC address to a container, this address can then be used for communication with that container.</li> </ul>"},{"location":"docs/docker/network/#network-types","title":"Network types","text":""},{"location":"docs/docker/network/#none","title":"None","text":"<p>No network is attached to the containers, and they have no access to any external network or other containers. As they run in an isolated network, there is no IP configuration for the containers.</p> Bash<pre><code>docker run --network=none ubuntu\n</code></pre>"},{"location":"docs/docker/network/#host-local","title":"Host (local)","text":"<p>Info</p> <p>container port 80 = host port 80</p> <p>The host network driver removes the network isolation between the Docker host and the Docker containers to use the host's networking directly. Therefore, the container won't get its own IP address.</p> <p>For instance, if you run a container which binds to port 80, the container's application will be available on port 80 on the host's IP address (localhost, IP). Therefore, you no need to specify <code>-p 80:80</code> for host networking, as that option is only for bridge network.</p> Bash<pre><code>docker run --network=host apiapp\n</code></pre>"},{"location":"docs/docker/network/#bridge","title":"Bridge","text":"<p>Info</p> <p>docker0 (172.17.0.1) is the bridge network, so all the bridge network will need to go through docker0 bridge (use <code>ip addr</code> to see more details)</p> <p>By default, the bridge driver network is attached to all container.</p> Bash<pre><code>docker run ubuntu # it uses Bridge network by default\n</code></pre> <p>Docker creates a private internal network called the bridge network on the host. The bridge network is attached to all containers by default and will get an internal IP address, normally in the <code>172.17</code> series range. Therefore, the containers can access each other using this internal IP.</p> <p>Additional information;</p> <ul> <li>Containers connected to the same bridge network can communicate using a software bridge while remaining isolated from containers not connected to it.</li> <li>Containers can find each other by name in the same network instead of using an IP address.</li> <li>It provides better isolation and interoperability between containerized applications.</li> <li>Environment variables are shared between linked containers on the bridge network.</li> </ul>"},{"location":"docs/docker/network/#user-defined-bridge-network","title":"User-defined bridge network","text":"<p>You can create your own internal network.</p> Bash<pre><code>docker network create --driver bridge --subnet 192.168.10.0/24 mynewinternalnetwork\n</code></pre> <p>With this setup, it will create a network with IP range of <code>192.168.10.0/24</code>. (192.168.10.1-192.168.10.254)</p>"},{"location":"docs/docker/network/#list-neworks","title":"List neworks","text":"Bash<pre><code>docker network ls\n</code></pre>"},{"location":"docs/docker/network/#inspect-networks","title":"Inspect networks","text":"Bash<pre><code>docker network inspect &lt;network-name&gt;\n</code></pre>"},{"location":"docs/docker/network/#remove-network","title":"Remove network","text":"Bash<pre><code>docker network rm &lt;network-name&gt;\ndocker network prune\n</code></pre>"},{"location":"docs/docker/network/#connectdisconnect-container-to-network","title":"Connect/Disconnect container to network","text":"Bash<pre><code>docker network connect &lt;network-name&gt; &lt;container&gt;\ndocker network disconnect &lt;network-name&gt; &lt;container&gt;\n</code></pre>"},{"location":"docs/docker/network/#container-communication","title":"Container Communication","text":""},{"location":"docs/docker/network/#container-to-host-local-communication","title":"Container to host (local) communication","text":"<p>If you want your container to communicate with the host, you have to replace the container's localhost or IP address to host.docker.internal. This domain will translate to the IP address of your localhost machine as seen from inside the Docker container.</p> <p>For example, now you have a Python API application that links to mongoDB. Here is the endpoint <code>mongodb://localhost:27017/user</code>. So, you have to change to <code>mongodb://host.docker.internal:27017/user</code>.</p>"},{"location":"docs/docker/network/#container-to-container-communication","title":"Container to container communication","text":"<p>There are two methods to let a container communicate with another container.</p> <ul> <li>Method 1 (not recommended)<ul> <li>Use <code>docker inspect &lt;container&gt;</code> to get the container IP address, then within your application code, put that specific IP address into it <code>mongodb://172.17.0.3:27017/user</code></li> </ul> </li> <li>Method 2<ul> <li>Use the container name as the input <code>mongodb://mymongodb:27017/user</code></li> <li>The container name refers to its IP address allocated by docker0.</li> <li></li> </ul> </li> </ul>"},{"location":"docs/docker/resource-limits/","title":"Resource Limits","text":""},{"location":"docs/docker/resource-limits/#cpu","title":"CPU","text":"<p>Reference</p> <p>Info</p> <p>By default, each container's access to the host machine's CPU cycles is unlimited. Docker us using default Completely Fair Scheduler (CFS), but it also supports Real-time Scheduler.</p>"},{"location":"docs/docker/resource-limits/#cpu-shares","title":"CPU Shares","text":"<p>By default, the Docker Host will assign a CPU share of 1024 to each container. You can modify that by using <code>--cpu-shares</code> option while creating the container.</p> Bash<pre><code>docker run --cpu-shares=&lt;amount&gt; &lt;image&gt;\ndocker run --cpu-shares=512 ubuntu\n</code></pre> <p><code>--cpu-shares</code> - It specifies how the host's total CPU resources are shared among its containers.</p>"},{"location":"docs/docker/resource-limits/#cpu-sets","title":"CPU Sets","text":"<p>It limits the specific CPUs or cores a container can use. A comma-separated list or hyphen-separated range of CPUs a container can use. Let's said that you have more than one CPU. Then the first cpu is numbered 0.</p> <ul> <li>1st CPU is numbered 0</li> <li>2nd CPU is numbered 1</li> </ul> <p>Value of 0-3 means, use the first, second, third, and fourth CPU. Value of 1,3 means, use the second, and fourth CPU.</p> Bash<pre><code>docker run --cpuset-cpus=&lt;value&gt; &lt;image&gt;\n\n# example\ndocker run --cpuset-cpus=0-1 ubuntu # this container can use first and second CPU\ndocker run --cpuset-cpus=1,3 ubuntu # this container can use second and fourth CPU\n</code></pre> <p><code>cpuset-cpus</code> - It specifies the CPU numbers to use.</p>"},{"location":"docs/docker/resource-limits/#cpu-count","title":"CPU Count","text":"<p>What happens when a container tries to consume more CPUs?</p> <p>Containers can consume all CPU resources available on the host, which can even affect other processes running on the host, such as other containers and Docker Daemons.</p> <p>It specifies how much of the available CPU resources a container can use. If host has 2 CPUs and if you set 1 CPU, then the container is guaranteed at most one CPU.</p> Bash<pre><code>docker run --cpus=&lt;value&gt; &lt;image&gt;\n\n# example\n# Let's say you have 4 CPUs, you specify 2.5, that means the container can only use as much as 2.5 out of 4.\ndocker run --cpus=2.5 ubuntu\n</code></pre>"},{"location":"docs/docker/resource-limits/#memory","title":"Memory","text":"<p>Reference</p> <p>We want to set the maximum amount of memory the container can use.</p> Bash<pre><code>docker run -m/--memory &lt;size&gt; &lt;image&gt;\n\n# example\ndocker run -m 512m ubuntu\ndocker run --memory=512m ubuntu\n</code></pre> <ul> <li>m = megabytes</li> <li>g = gigabytes</li> </ul> <p>So, setting <code>512m</code> will allocate 512MB of memory for the container. By default, the container will use the same amount of memory that is specified as swap space, that means when the container consumes more than 512MB, it can also consume another 512MB as swap space. Therefore, the container will get 1GB of memory, which effectively combines memory and swap space.</p> <p>Sum of memory = memory + swap space</p> <p>You can limit the container's use of swap space by specifying the <code>--memory-swap</code> option while creating the container.</p> Bash<pre><code>docker run --memory=&lt;size&gt; --memory-swap=&lt;size&gt; &lt;image&gt;\ndocker run --memory=512m --memory-swap=512m ubuntu\n</code></pre> <p>In this case, there is no swap space configured because the <code>--memory</code> and <code>--memory-swap</code> options are the same.</p> <p>Swap space = 512m - 512m = 0m If you want to allocate 256 megabytes for swap space, you have to set <code>--memory-swap=768m</code>.</p> Bash<pre><code>docker run --memory=512m --memory-swap=768m ubuntu\n</code></pre> <p>Swap space = 768m - 512m = 256m</p>"},{"location":"docs/docker/resource-limits/#additional-add-on","title":"Additional add-on","text":"Bash<pre><code>docker run --memory=512m --memory-swap=-1 ubuntu # unlimited memory swap(infinity)\n\n# Docker will ensure the memory allocated to this container\ndocker run --memory=512m --memory-swap=-1 --memory-reservation=100m ubuntu\n</code></pre> <p><code>--memory-reservation</code> = Allows you to specify a soft limit smaller than --memory which is activated when Docker detects contention or low memory on the host machine. It must be set lower than <code>--memory</code> for it to takje precedence. Because it is a soft limit, it doesn't guarantee that the container doesn't exceed the limit.</p>"},{"location":"docs/docker/useful-commands/","title":"Useful commands","text":""},{"location":"docs/docker/useful-commands/#docker-system-events","title":"Docker system events","text":"<p>Reference</p> <p>It will get real-time events that were logged from the server. So, all changes that are made to containers or networks are logged under this command.</p> Bash<pre><code>docker system events --since 60m\ndocker system events --since 2024-05-30\ndocker system events --filter '&lt;key&gt;=&lt;value&gt;'\ndocker system events --filter 'container=588a23dac085'\n</code></pre>"},{"location":"docs/docker/useful-commands/#disk-usage-metrics-for-docker-objects","title":"Disk usage metrics for docker objects","text":"<p>It will display the actual size of the different objects that are managed by Docker.</p> Bash<pre><code>docker system df\ndocker system df -v\n</code></pre>"},{"location":"docs/docker/useful-commands/#copy","title":"Copy","text":"<p>Info</p> <p>Make sure the directory exists for both side paths. You can copy entire folder too.</p> <p>Host to container</p> Bash<pre><code>docker container cp &lt;local-path&gt; &lt;container&gt;:&lt;container-path&gt;\ndocker container cp /home/username/config.yaml apiapp:/etc/api/config.yaml\n</code></pre> <p>Container to host</p> Bash<pre><code>docker container cp &lt;container&gt;:&lt;container-path&gt; &lt;local-path&gt;\ndocker container cp apiapp:/etc/api/config.yaml /home/username/config.yaml\n</code></pre>"},{"location":"docs/docker/volume/","title":"Volume","text":"<p>Info</p> <p>All the volumes are stored in <code>/var/lib/docker/volumes</code>. The volumes are mounted as read-write by default.</p> <p>Docker volume is used to persist and share the container's data across containers. Folders on your host machine's hard drive are mounted into containers as volumes, which allows the container to write its data into the host volumes.</p>"},{"location":"docs/docker/volume/#volume-types","title":"Volume types","text":""},{"location":"docs/docker/volume/#anonymous-and-named-volumes","title":"Anonymous and named volumes","text":"Volume Type Description Anonymous The volume will only exist as long as our container exists. Bind mount volumes can prevent some files from being overlapped. Named The volume persists even if you kill or close the container. With this volume, you can't edit the data inside the volume, instead you can use bind mounts for editing Bash<pre><code># Example of Anonymous volume\ndocker run -v /app/logs &lt;image&gt;\n\n# Example of Named volume\ndocker run -v mylog:/app/logs apiapp\ndocker run --mount source=mylog,destination=/app/logs apiapp\n</code></pre>"},{"location":"docs/docker/volume/#bind-mounts","title":"Bind mounts","text":"<p>Reference</p> <p>The source and the destination must be absolute path</p> <p>The use of bind mounts is good for development, but not for production since production might not have the same folder structure as development. To make managing different environments easier, I recommend the container has the same structure as your local.</p> <ul> <li>macOS / Linux = <code>$(pwd):/app</code></li> <li>Windows = <code>\"%cd%\":/app</code></li> </ul> <p>Bind mounts allow you to edit or reflect your code changes from local to container. Bind mount volumes can prevent some files from being overlapped. It's great for persistent.</p> Bash<pre><code>docker run -v &lt;local-path&gt;:&lt;container-path&gt; &lt;image&gt;\ndocker run --mount type=&lt;type&gt;,source=&lt;local-path&gt;,destination=&lt;container-path&gt; &lt;image&gt;\n\n# example\ndocker run -v /app/api:/app/api myapi\ndocker run --mount type=bind,source=/app/api,destination=/app/api myapi\n</code></pre>"},{"location":"docs/docker/volume/#read-only-volume","title":"Read-only volume","text":"<p>Read-only volume will prevent the local files from being modified when the files within the container have been modified.</p> Bash<pre><code>docker run --mount type=&lt;type&gt;,source=&lt;local-path&gt;,target=&lt;container-path&gt;,readonly &lt;image&gt;\ndocker run -v \"&lt;local-path&gt;:&lt;container-path&gt;:ro\"\n\n# example\ndocker run --mount type=bind,source=/root/logs,target=/api/logs,readonly myapi\ndocker run -v \"/root/logs:/api/logs:ro\"\n</code></pre> <p><code>readonly</code> and <code>:ro</code> are to indicate that this volume is only for reading</p>"},{"location":"docs/docker/volume/#list-volume","title":"List volume","text":"Bash<pre><code>docker volume ls\n</code></pre>"},{"location":"docs/docker/volume/#create-volume","title":"Create volume","text":"Bash<pre><code>docker volume create &lt;volume-name&gt;\n</code></pre>"},{"location":"docs/docker/volume/#inspect-volume","title":"Inspect volume","text":"Bash<pre><code>docker volume inspect &lt;volume-name&gt;\n</code></pre>"},{"location":"docs/docker/volume/#remove-volume","title":"Remove volume","text":"Bash<pre><code>docker volume rm &lt;volume-name&gt;\ndocker volume remove &lt;volume-name&gt;\ndocker volume prune\n</code></pre>"},{"location":"docs/docker/what-is-docker/","title":"What is Docker?","text":"<p>Docker is an open platform for developing, shipping, and running applications using containers.</p> <p>\u2014 docker.docs</p> <p>Consider the case where you build a Python application that contains a wide range of dependencies (legacy and latest). This Python application needs to be shipped to the production environment on different computers with different operating systems. You have to ensure that every computer has the same configurations as your local. Traditionally, doing it manually takes time and sometimes you may run into issues with different operating systems.</p> <p>With Docker, you can easily manage your infrastructure like you manage your applications, because it will package all the application source code as well as the tools, libraries, settings, runtime, and dependencies that your application needs into a container. So, you can deploy your Python application with the same configuration and settings into each computer.</p>"},{"location":"docs/git/branches/","title":"Branches","text":""},{"location":"docs/git/branches/#what-is-git-branch","title":"What is Git Branch?","text":"<p>Info</p> <p>Default branch will be created either master or main depends on the settings.</p> <p>Branch is an independent line of development work. As an example, if you are working on a large project and you want to add a new feature, you can't commit all your changes straight away and push them to the default branch, since that would make the commit history pretty messy and it would be hard to tell which changes were made.</p> <p>Regardless of how big or small your changes are, always create a branch and work on it to encapsulate your changes.</p> <p>All new commits you make to that branch will be recorded in that branch as well.</p>"},{"location":"docs/git/branches/#git-branch-commands","title":"Git branch commands","text":""},{"location":"docs/git/branches/#create-branch","title":"Create branch","text":"<pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    commit id: \"docs: create third story\"\n    branch develop\n    checkout develop\n    commit</code></pre> Bash<pre><code>git branch &lt;branch-name&gt;\n</code></pre>"},{"location":"docs/git/branches/#checkoutswitch-branch","title":"Checkout/Switch branch","text":"<p>Info</p> <p>Remember that the HEAD is Git's way of referring to the current snapshot</p> <p>Once we have created the branch, we have to switch or checkout to that branch.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    commit id: \"docs: create third story\"\n    branch develop\n    checkout develop\n    commit id: \"docs: edit first story\"\n    commit id: \"docs: edit second story\"</code></pre> <ul> <li>the default branch can still proceed with the commits even though we have checkout to develop branch</li> </ul> Bash<pre><code>git checkout &lt;branch-name&gt;\n# if the branch name is not exists, it will auto create and checkout to that branch\ngit checkout -b &lt;branch-name&gt;\n</code></pre> <p></p> <p>Watch animation</p>"},{"location":"docs/git/branches/#merge-branch-fast-forward","title":"Merge branch (Fast-forward)","text":"<p>The <code>git merge</code> command will combine two branches, so it will consolidate multiple commit sequences into a single history.</p> <p>Here is the scenario, since develop branch came directly from main and no other changes had been made to main, so it can fast-forward.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    branch develop\n    checkout develop\n    commit id: \"docs: edit first story\"\n    commit id: \"docs: edit second story\"\n    checkout main\n    commit id: \"docs: create programming\"\n    merge develop</code></pre> Bash<pre><code>git merge &lt;branch&gt;\n\n# example, if you want to merge develop branch to main\ngit checkout main\ngit merge develop\n</code></pre> <p></p> <p>Watch animation</p>"},{"location":"docs/git/branches/#merge-conflicts","title":"Merge conflicts","text":"<p>A merge conflict will arise when two developers have changed the same lines in a file. Because of that, Git does not know which one is correct.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    branch develop\n    checkout develop\n    commit id: \"docs: edit first story\"\n    commit id: \"docs: edit second story\"\n    checkout main\n    commit id: \"docs: create programming\"\n    commit id: \"docs: set title to first story\"\n    merge develop\n    commit id: \"merged with develop after fixing conflicts\"</code></pre> <p>To resolve merge conflict, you have to open the file to make necessary changes. Then, use <code>git add</code> command to stage the new merged commit and the final step is to merge your latest commits into it.</p> Bash<pre><code>git add &lt;file-that-has-conflicts&gt;\ngit commit -m \"merged with &lt;branch&gt; after fixing conflicts\"\n</code></pre> <p></p>"},{"location":"docs/git/branches/#list-branches","title":"List branches","text":"Bash<pre><code>git branch\ngit branch -a # list all local and remote branches\n</code></pre>"},{"location":"docs/git/branches/#delete-branch","title":"Delete branch","text":"Bash<pre><code>git branch -d &lt;branch-name&gt; # delete branch with safe operation\ngit branch -D &lt;branch-name&gt; # force delete branch\n</code></pre>"},{"location":"docs/git/branches/#rename-branch","title":"Rename branch","text":"Bash<pre><code>git branch -m &lt;branch-name&gt; &lt;new-branch-name&gt;\ngit branch -M &lt;branch-name&gt; &lt;new-branch-name&gt; # force rename branch\n</code></pre>"},{"location":"docs/git/branches/#create-remote-branches-and-push","title":"Create remote branches and push","text":"<p>Before we want to push our local branches to our remote repository. The remote repository needs to be added to our local project if it hasn't already been. Normally the remote name is origin, but you can change it.</p> Bash<pre><code>git remote add &lt;remote-name&gt; &lt;remote-repo-url&gt;\n# example\ngit remote add origin https://github.com/KarChunT/git-training\n</code></pre> <p>Once you initialize, you have access to this remote repository. So you can push your local branch to the remote at remote name.</p> <ul> <li>The reason why we need to set-upstream from local to remote is because it makes our jobs easier when we want to perform push and pull operation. With an upstream branch set, you can simply <code>git pull</code> or <code>git push</code> instead of <code>git push origin &lt;branch&gt;</code></li> </ul> <p>What is upstream?</p> <p>An upstream is just a remote tracking branch that is associated with your local branch. Do take note that, each branch only has one upstream.</p> Bash<pre><code># set-upstream, push the local branch to the remote at remote name\ngit push -u &lt;remote-name&gt; &lt;local-branch-name&gt;\ngit push &lt;branch-name&gt; # after you set up-stream, branch-name is optional\ngit push --force # force push, will overwrite the things in remote\n\n# example\ngit push -u origin develop\n</code></pre> <p>You can use the following command:</p> <ul> <li>if the remote branch already exists or</li> <li>you want to change upstream branch</li> </ul> Bash<pre><code>git branch -u origin/&lt;branch-name&gt;\n</code></pre>"},{"location":"docs/git/git-in-depth/","title":"Git in Depth","text":""},{"location":"docs/git/git-in-depth/#a-real-git","title":"A real Git","text":"<p>Git is a key-value store. A file is hashed using the SHA-1 algorithm when it's added to a commit. This hash is then used as a key name for the folder and used to store the file.</p> <p>Git has Porcelain and Plumbing commands</p> <ul> <li>Porcelain Commands (easy to remember)</li> <li>example: git add, status, commit, stash, etc</li> <li>Plumbing Commands (get access to Git internals)</li> <li>example: git hash-object, cat-file, ls-files, rev-parse, ls-remote, etc</li> </ul>"},{"location":"docs/git/git-in-depth/#hash-object","title":"hash-object","text":"<p>The <code>git hash-object</code> command hashes the file contents.</p> Bash<pre><code>git hash-object &lt;filename&gt;\n# example\ngit hash-object hello\n</code></pre> <p></p> <p></p> <p>As you can see, the key is made up of the first two characters of the hash. The contents of this file (hello) will be stored in this hash file 492f47831b16c8217339fcb1449345b424c72fcb.</p> <p>To see the hash file contents, you have to use git cat-file command.</p>"},{"location":"docs/git/git-in-depth/#cat-file","title":"cat-file","text":"<p>The <code>git cat-file</code> command will display the contents of the hash file by using the hash value.</p> Bash<pre><code>git cat-file -p &lt;object-hash&gt;\n# example\ngit cat-file -p 492f47831b16c8217339fcb1449345b424c72fcb # pretty prints the contents\n</code></pre>"},{"location":"docs/git/git-in-depth/#git-object-contents","title":"Git Object Contents","text":"<p>There are three types of folders in the object folder.</p> <ol> <li>commit<ul> <li>A commit is simply just a commit hash.</li> </ul> </li> <li>tree<ul> <li>It's a folder associated with the repository on your file system.</li> </ul> </li> <li>blob<ul> <li>It's just a piece of data.</li> <li>For example, the file that hashed (hello) is called a blob.</li> </ul> </li> </ol>"},{"location":"docs/git/git-installation/","title":"Git Installation","text":"<p>Reference</p> <p>Git Installation</p>"},{"location":"docs/git/git-installation/#install-git-on-windows","title":"Install Git on Windows","text":"<ol> <li> <p>Go to this link: Download for Windows</p> </li> <li> <p>Select your installer, for my case, I'm selecting 64-bit Git for Windows Setup.</p> <p></p> </li> <li> <p>Open the downloaded exe file and follow the instructions. I put all options as default.</p> <p></p> </li> <li> <p>After you have done the installation. Open your terminal and enter the following command.</p> Bash<pre><code>git --version\n</code></pre> <p>It will show the latest Git version that you have installed.</p> </li> <li> <p>Configure your Git username and email. Remember to replace the name and email with your own. Any commits you make will include these details.</p> Bash<pre><code>git config --global user.name \"Kar Chun\"\ngit config --global user.email \"karchuntan.1999@gmail.com\"\n</code></pre> </li> </ol>"},{"location":"docs/git/git-installation/#install-git-on-ubuntulinux","title":"Install Git on Ubuntu/Linux","text":"<p>The typical way of installing Git, will not install the latest version. With the following commands, it will help you to install the latest Git.</p> <p>More Information</p> <p>PPA Git</p> <ol> <li> <p>Install stable Git version</p> Bash<pre><code>sudo add-apt-repository ppa:git-core/ppa\nsudo apt-get update\nsudo apt-get install -y git\n</code></pre> </li> <li> <p>Check Git version</p> Bash<pre><code>git --version\n</code></pre> </li> <li> <p>Configure your Git username and email. Remember to replace the name and email with your own. Any commits you make will include these details.</p> Bash<pre><code>git config --global user.name \"Kar Chun\"\ngit config --global user.email \"karchuntan.1999@gmail.com\"\n</code></pre> </li> </ol>"},{"location":"docs/git/rebasing/","title":"Rebasing","text":""},{"location":"docs/git/rebasing/#rebase","title":"Rebase","text":"<p>Differences between Merge and Rebase</p> Merge Rebase Unique identifier (hash) won't be modified as we are not modifying the git commits history Unique identifier (hash) will be updated, as we are modifying the Git history when we are rebasing (copying the commits from one branch to another) the branches <p>When Developer A has been working on his own story in his own branch. Meanwhile, his team continues to contribute additional stories and updates to the main branch. The Developer A wants to make sure that his branch is up to date with the latest changes from the main branch.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    branch develop\n    checkout develop\n    commit id: \"docs: create third story\"\n    commit id: \"docs: edit first story\"\n    commit id: \"docs: edit second story\"\n    checkout main\n    commit id: \"docs: create programming\"\n    checkout develop\n    merge main id: \"Merged main into Develop\"</code></pre> <p>To do this, the Developer A would need to merge the main branch into his branch, but that will create a new merge commit and the Developer A doesn't want to do that.</p> Bash<pre><code>git merge main\n</code></pre> <p>The other way to accomplish this is by rebasing branches. When we rebase a branch, we are layering/putting it on top of another. That means, we can rebase develop branch on top of the main branch, so now the develop branch will have all the changes that were made to the main branch.</p> Bash<pre><code>git checkout develop\ngit rebase main\n</code></pre> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\n%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': { 'mainBranchName': 'develop' }} }%%\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    commit id: \"docs: create programming\"\n    commit id: \"docs: create third story\"\n    commit id: \"docs: edit first story\"\n    commit id: \"docs: edit second story\"</code></pre> <p></p> <p>Watch animation</p>"},{"location":"docs/git/rebasing/#interactive-rebasing","title":"Interactive rebasing","text":"<p>The <code>git rebase</code> command also allows us to modify the Git history on a certain branch before rebasing it.</p> <p>Let's just assume the Developer A is working on his branch (main), but there are many commits that should actually have been just one commit. To do that, we can use <code>git rebase -i</code> command to combine/squash/meld into one commit.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    commit id: \"docs: create programming\"\n    commit id: \"docs: create same story 1\"\n    commit id: \"docs: edit same-story\"\n    commit id: \"docs: more changes to same-story\"\n    commit id: \"docs: more more changes to same-story\"</code></pre> <p>Let's modify the latest 4 commits, as now we want to combine/squash/meld the latest 3 commits into the 1st commit.</p> Bash<pre><code>git rebase -i HEAD~4\n</code></pre> <p></p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    commit id: \"docs: create programming\"\n    commit id: \"squashed latest 3 commits into here\"</code></pre>"},{"location":"docs/git/rebasing/#cherry-pick","title":"Cherry-pick","text":"<p>In some cases, you may want to apply a certain change made to one branch to another branch without applying the complete changes made to that branch. By cherry-picking that particular commit, we can create a copy of it on your own branch.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    branch develop\n    checkout develop\n    commit id: \"docs: create third story\"\n    commit id: \"docs: edit first story\"\n    commit id: \"docs: edit second story\"\n    checkout main\n    commit id: \"docs: create programming\"</code></pre> <p>For example, let's say that the Developer A is on the main branch and he want to add the changes that were made to the \"docs: edit second story\" commit on the develop branch. In that case, we can cherry-pick that commit and it will only take that commit changes.</p> Bash<pre><code>git cherry-pick &lt;commit-id&gt;\ngit cherry-pick &lt;docs: edit second story-commit-id&gt;\n</code></pre> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"\n    branch develop\n    checkout develop\n    commit id: \"docs: create third story\"\n    commit id: \"docs: edit first story\"\n    commit id: \"docs: edit second story\"\n    checkout main\n    commit id: \"docs: create programming\"\n    cherry-pick id: \"docs: edit second story\"</code></pre> <ul> <li>the cherry-pick: \"docs: edit second story\" is the copy of that commit (docs: edit second story) on develop branch. We now have a copy of that commit on the main branch.</li> <li>Do take note that, it will only get the edit second story changes</li> </ul> <p>Steps to perform cherry-pick:</p> <ol> <li>identified the commit</li> <li>checkout the branch that you want to fetch other branch commit to your branch</li> <li>perform cherry-pick command</li> </ol> Bash<pre><code>git log &lt;branch-name&gt; --oneline # remember to commit hash, example 123\ngit checkout main\ngit cherry-pick &lt;commit-id&gt; # 123\n</code></pre> <ul> <li>It will straight away merge the file and create a new commit on top of the current commit.</li> <li>If you end up in merge conflict, merge the file manually and make sure to leave the latest contents in the file itself.</li> </ul> <p></p> <p>Watch animation</p>"},{"location":"docs/git/reflog/","title":"Reflog","text":""},{"location":"docs/git/reflog/#git-reflog","title":"Git Reflog","text":"<p>The <code>git reflog</code> command shows us all the actions that have been taken on our repository. Everyone will make mistake and if you make a mistake, you can simply undo that change by resetting your HEAD based on the information (commit-id) that reflog provides to us.</p> Bash<pre><code>git reflog\ngit reset --hard &lt;commit-id&gt;\n</code></pre> <p></p>"},{"location":"docs/git/remote-repositories/","title":"Remote repository","text":""},{"location":"docs/git/remote-repositories/#configuration-and-setup-git-config","title":"Configuration and setup Git config","text":"<p>You can refer more details in Create remote branches and push section.</p> Bash<pre><code>git remote add &lt;remote-name&gt; &lt;remote-repo-url&gt;\ngit push -u &lt;remote-name&gt; &lt;local-branch-name&gt;\n</code></pre>"},{"location":"docs/git/remote-repositories/#fetching-and-pulling","title":"Fetching and pulling","text":"Fetch Pull Only changes are copied into your local Git repository and does not reflect the changes It copies changes from a remote repository directly into your working directory and reflect the changes. Fetch just fetch pull = fetch + merge Bash<pre><code># one way\ngit fetch origin &lt;branch-name&gt;\ngit merge origin/&lt;branch-name&gt;\n\n# another way\ngit pull origin &lt;branch-name&gt; # or just git pull\n</code></pre>"},{"location":"docs/git/remote-repositories/#fetching","title":"Fetching","text":""},{"location":"docs/git/remote-repositories/#pulling","title":"Pulling","text":"<p>Watch animation</p>"},{"location":"docs/git/remote-repositories/#list-all-remote-repositories","title":"List all remote repositories","text":"Bash<pre><code>git remote -v\n</code></pre>"},{"location":"docs/git/reset-and-revert/","title":"Reset and Revert","text":"<p>Sometimes we commit changes that we don't intend to commit. If that is the case, we have several options to undo the action.</p> <ul> <li>Revert</li> <li>Reset<ul> <li>soft</li> <li>hard</li> <li>mixed</li> </ul> </li> </ul>"},{"location":"docs/git/reset-and-revert/#revert","title":"Revert","text":"<p>Info</p> <p>If you want to undo changes and keep them in your Git history, you can use the <code>git revert</code> command</p> <p>The <code>git revert</code> command will create a new commit, which reverses all the changes made on the specified commit. Let's say that we want to revert the commit where we create the <code>test</code> file.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n  commit id: \"docs: create first story\"\n  commit id: \"docs: create second story\"\n  commit id: \"docs: create programming\"\n  commit id: \"docs: create test\"</code></pre> Bash<pre><code>git revert &lt;docs: create test-commit-id&gt;\n</code></pre> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n  commit id: \"docs: create first story\"\n  commit id: \"docs: create second story\"\n  commit id: \"docs: create programming\"\n  commit id: \"docs: create test\"\n  commit id: \"Revert docs: create test\"</code></pre> <ul> <li>In the \"docs: create test\" commit, <code>test</code> file has created. The reverted commit contains all the opposite changes. It will then deletes the <code>test</code> file.</li> </ul> <p></p> <p>Watch animation</p>"},{"location":"docs/git/reset-and-revert/#reset","title":"Reset","text":"<p>The another way is to use the <code>git reset</code> command. There are three ways to reset the commit to undo it.</p> <ul> <li>soft</li> <li>hard</li> <li>mixed</li> </ul>"},{"location":"docs/git/reset-and-revert/#soft","title":"soft","text":"<p>With the <code>--soft</code> flag, we still have all the changes that we have made, and these changes will move to staged changes. You can use <code>git status</code> to check all the staged changes.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n  commit id: \"docs: create first story\"\n  commit id: \"docs: create second story\"\n  commit id: \"docs: create programming\"\n  commit id: \"docs: create test\"\n  commit id: \"docs: create hello\"</code></pre> <p>Let's reset the \"docs: create hello\" commit with a soft flag. Reset commands also receive the number of commits that we want to reset, in this case we want to reset one commit.</p> Bash<pre><code>git reset --soft &lt;commit-id&gt;\ngit reset --soft HEAD~1\n</code></pre> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n  commit id: \"docs: create first story\"\n  commit id: \"docs: create second story\"\n  commit id: \"docs: create programming\"\n  commit id: \"docs: create test\"</code></pre> <p></p> <p>Watch animation</p>"},{"location":"docs/git/reset-and-revert/#hard","title":"hard","text":"<p>With the <code>--hard</code> flag, we will lose all the changes that we have made on that commit.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n  commit id: \"docs: create first story\"\n  commit id: \"docs: create second story\"\n  commit id: \"docs: create programming\"\n  commit id: \"docs: create test\"\n  commit id: \"docs: create hello\"</code></pre> <p>Let's reset the \"docs: create hello\" commit with a hard flag.</p> Bash<pre><code>git reset --hard &lt;commit-id&gt;\ngit reset --hard HEAD~1\n</code></pre> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n  commit id: \"docs: create first story\"\n  commit id: \"docs: create second story\"\n  commit id: \"docs: create programming\"\n  commit id: \"docs: create test\"</code></pre> <p></p> <p>Watch animation</p>"},{"location":"docs/git/reset-and-revert/#mixed","title":"mixed","text":"<p>With the <code>--mixed</code> flag, we still have all the changes that we have made, and these changes will move to unstaged changes. You can use <code>git status</code> to check all the unstaged changes.</p> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n  commit id: \"docs: create first story\"\n  commit id: \"docs: create second story\"\n  commit id: \"docs: create programming\"\n  commit id: \"docs: create test\"\n  commit id: \"docs: create hello\"</code></pre> <p>Let's reset the \"docs: create hello\" commit with a mixed flag.</p> Bash<pre><code>git reset --mixed &lt;commit-id&gt;\ngit reset --mixed HEAD~1\n</code></pre> <pre><code>---\nconfig:\n  logLevel: 'debug'\n  theme: 'default'\n  themeVariables:\n      'git0': '#ff0000'\n      'git1': '#ff00ff'\n      'git2': '#00ffff'\n      'git3': '#ffff00'\n      'git4': '#ff00ff'\n      'git5': '#00ffff'\n---\ngitGraph\n  commit id: \"docs: create first story\"\n  commit id: \"docs: create second story\"\n  commit id: \"docs: create programming\"\n  commit id: \"docs: create test\"</code></pre> <p></p> <p>Web animation</p>"},{"location":"docs/git/save-changes/","title":"Save Changes","text":"<p>Warning</p> <p>Please follow the sequence.</p> <p>Clone the repository KarChunT/git-training to work-on.</p>"},{"location":"docs/git/save-changes/#git-add","title":"git add","text":"<p>The <code>git add</code> command simply pushes files or directories to staging area/environment. By doing this, you tell Git to include those changes in the next commit. Of course, you can add more than one file/directory at a time.</p> Bash<pre><code>git add &lt;file/directory&gt;\ngit add . # all files and directories\n\n# example\ntouch third-story programming # create new files\ngit add third-story\n</code></pre> <p></p> <p>Watch animation</p>"},{"location":"docs/git/save-changes/#git-status","title":"git status","text":"<p>The <code>git status</code> command simply lists which files are staged, unstaged, and untracked.</p> Bash<pre><code>git status\n</code></pre> <p></p>"},{"location":"docs/git/save-changes/#git-commit","title":"git commit","text":"<p>The <code>git commit</code> command simply saves or commits that file changes into a Git project. You can think of as snapshots or milestones along the timeline of a Git project.</p> <pre><code>gitGraph\n    commit id: \"docs: create first story\"\n    commit id: \"docs: create second story\"</code></pre> Bash<pre><code>git commit # opens a new editor\ngit commit -a # automatically stage files that have been modified and deleted\ngit commit -m \"commit message\" # flag of the message\ngit commit -am \"commit message\" # combination of am\ngit commit -s # user signed-off, certifies who is the author of the commit, tracking for patches\ngit commit --amend # modify the last commit\n\n# example\ngit commit -m \"docs: create third story\"\n</code></pre> <p> Watch animation</p>"},{"location":"docs/git/save-changes/#git-log","title":"git log","text":"<p>The <code>git log</code> command shows the information that you need to know about all the commits, such as</p> <ul> <li>commit hash</li> <li>author name</li> <li>data of the commit: which files to be committed</li> <li>commit message</li> </ul> Bash<pre><code>git log\ngit log --oneline # don't care about all extra information\ngit log --name-only # list the changed files\ngit log --graph --decorate\n</code></pre> <p></p>"},{"location":"docs/git/save-changes/#git-diff","title":"git diff","text":"<p>The <code>git diff</code> command shows the differences between two data sources. Data sources can be</p> <ul> <li>commits</li> <li>branches</li> <li>files, etc</li> </ul> Bash<pre><code>git diff # comparing all changes\ngit diff &lt;file&gt;\ngit diff HEAD &lt;filename&gt; # filename is optional\ngit diff --staged/--cached &lt;filename&gt; # only for staged changes, filename is optional\ngit diff &lt;commit_hash&gt; &lt;commit_hash&gt;\ngit diff &lt;branch&gt; &lt;branch&gt;\ngit diff &lt;branch&gt; &lt;branch&gt; &lt;file&gt; # comparing files from two branches\ngit diff --color-words # Highlighting changes with much better granularity\n\n# Example\nnano first-story # write some texts into it\ngit diff\n</code></pre> <p></p> <p>How to read this?</p> Text Only<pre><code>@@ -50,8 +50,12 @@\n</code></pre> <p>In this example, 8 lines have been extracted starting from line number 50. Then, 12 lines have been added starting at line number 50.</p>"},{"location":"docs/git/save-changes/#git-restore","title":"git restore","text":"<p>The <code>git restore</code> command will discard changes in working directory or if a file is tracked then you can restore or unstage that file to match the version in HEAD.</p> Bash<pre><code># discard changes\ngit restore . # all files\ngit restore &lt;file&gt; # same as git checkout &lt;file&gt;\ngit restore &lt;pattern&gt; # pattern = '*.c'\n\n# unstage\ngit add first-story\ngit restore --staged &lt;file&gt; # same as git reset HEAD &lt;file&gt;\n\n## unstage all files\ngit restore --staged .\ngit reset\n\n# restore both index and working tree ---&gt; same as git checkout\ngit restore --source=HEAD --staged --worktree &lt;file&gt;\n\n# Example\ngit restore first-story\ngit restore --staged second-story\n# even it's staged, it will restore all changes to same as HEAD commit\ngit restore --source=HEAD --staged --worktree second-story\n</code></pre> <p></p> <p>Watch animation</p>"},{"location":"docs/git/save-changes/#gitignore","title":".gitignore","text":"<p>Info</p> <p>Sources</p> <ul> <li>https://git-scm.com/docs/gitignore</li> <li>https://github.com/github/gitignore</li> <li>https://www.toptal.com/developers/gitignore</li> <li>https://www.atlassian.com/git/tutorials/saving-changes/gitignore</li> </ul> <p><code>.gitignore</code> is a file that will include all the globbing patterns to ignore the file to be commited. These could be the files like</p> <ul> <li>build artifacts like <code>/bin</code>, <code>/target</code></li> <li>machine generated files like <code>.pyc</code></li> <li>dependency caches like <code>node_modules</code></li> <li>log file</li> <li>environments or secrets like <code>.env</code></li> </ul> .gitignore<pre><code>*.log\n.env*\ntest.txt\nnode_modules/\n/bin\n</code></pre>"},{"location":"docs/git/setup-repository/","title":"Setup repository","text":""},{"location":"docs/git/setup-repository/#initialize-a-new-git-repo","title":"Initialize a new Git repo","text":"<p>To create a repo, you will use the <code>git init</code> command to initialize a new git repository. Do take note that <code>git init</code> is just a one-time command process. You will only use it when you want to initialize a new Git repo.</p> <ol> <li> <p>Create a folder</p> Bash<pre><code>mkdir project\ncd project\n</code></pre> </li> <li> <p>Initialize a Git repository       Bash<pre><code>git init\ngit init &lt;directory&gt; # initialize/point to an existing directory\n</code></pre></p> <ul> <li>As a result, Git knows that the folder you initiated it on should be monitored.</li> <li>It will also create a hidden folder named <code>.git/</code> to keep track of all the changes you made.</li> </ul> </li> </ol>"},{"location":"docs/git/setup-repository/#clone-an-existing-repo","title":"Clone an existing repo","text":"<p>If your project already been setup on centralized server like GitHub, Bitbucket, etc. Then you have to clone that remote repository to your local. Do take note that <code>git clone</code> is also just a one-time command process. You will only use it when you want to clone an existing repository.</p> Bash<pre><code>git clone &lt;repo_url&gt;\ngit clone https://github.com/KarChunT/karchunt.com\n</code></pre> <p></p>"},{"location":"docs/git/setup-repository/#clone-to-a-specific-folder","title":"Clone to a specific folder","text":"Bash<pre><code>git clone &lt;repo_url&gt; &lt;directory&gt;\ngit clone https://github.com/KarChunT/karchunt.com karchunt\n</code></pre>"},{"location":"docs/git/setup-repository/#clone-a-specific-branch","title":"Clone a specific branch","text":"Bash<pre><code>git clone --branch &lt;branch&gt; &lt;repo_url&gt;\ngit clone --branch development https://github.com/KarChunT/karchunt.com\n</code></pre>"},{"location":"docs/git/stashing/","title":"Stashing","text":""},{"location":"docs/git/stashing/#git-stash","title":"Git Stash","text":"<p>Let's say that the Developer A is working on a third story on the develop branch. Suddenly, the Developer B says that there are some critical bugs that needs to be corrected immediately on the main branch.</p> <p>Before the Developer A checkout to the main branch, it's necessary to place the changes made to the third story somewhere in order to have a clean working area if we want to solve the bugs on the main branch. As the story isn't finished yet, the Developer A don't want to commit it to his branch (develop).</p> <p>In this case, we can use <code>git stash</code> command to stash all changes in the working area. We can keep on pushing the changes to the stash and the changes will simlpy pile up (treat like queue FIFO).</p> Bash<pre><code># Stash the changes\ngit stash\ngit stash push\ngit stash save \"message\"\n\n# Apply the stash changes\ngit stash apply # re-applying the stashed changes without pop it\ngit stash pop\ngit stash pop stash@{1}\ngit stash branch branch-name stash@{1} # creating a branch from your stash, as you may run into conflicts when popping or applying your stash\n\n# List all the stash\ngit stash list\n\n# Show the differences between the current and a stash\ngit stash show\ngit stash show -p # view the full diff of a stash\n\n# clear or drop stash\ngit stash clear # clear all the stashes\ngit stash drop stash@{1}\n</code></pre>"},{"location":"docs/git/stashing/#stash-push","title":"stash push","text":"<p>Watch animation</p>"},{"location":"docs/git/what-is-git/","title":"What is Git?","text":""},{"location":"docs/git/what-is-git/#git-introduction","title":"Git Introduction","text":"<p>Git is a distributed version control system that tracks</p> <ul> <li>file changes</li> <li>who made changes</li> </ul>"},{"location":"docs/git/what-is-git/#local-vs-remote-repositories","title":"Local vs Remote repositories","text":"<p>Git has two repository types, local and remote.</p> Local Remote You can access the local repository directly from your own machine Repository is usually located on centralized server. For example, GitHub, Bitbucket, etc"},{"location":"docs/python-oop/abstraction/","title":"Abstraction","text":""},{"location":"docs/python-oop/abstraction/#concept","title":"Concept","text":"<p>What is the difference between abstraction and encapsulation?</p> <ul> <li>Abstraction is about hiding complexity and showing only essential features.</li> <li>Encapsulation is about hiding data and controlling access to the internal state of an object.</li> </ul> <p>Hiding complex internal implementation details and showing only the essential features of the object. It allows the developer to focus on \"what to do\" (focus on what an object does) rather than \"how to do it\" (how it achieves its functionality).</p> Types of Abstraction Description Data Abstraction Hides the details of data representation and focuses on the essential properties of the data. Process Abstraction Hides the implementation of operations and focuses on the essential features of the process. Control Abstraction Hides the complexity of control flow and focuses on the essential features of control structures."},{"location":"docs/python-oop/abstraction/#implementation","title":"Implementation","text":"<p>Remember Abstract classes cannot be instantiated directly. They are meant to be subclassed, and the subclasses must implement the abstract methods defined in the abstract class. It can contain both abstract methods (without implementation) and concrete methods (with implementation).</p> Python<pre><code>from abc import ABC, abstractmethod\n\nclass Shape(ABC):\n  @abstractmethod\n  def area(self):\n    pass\n\n  @abstractmethod\n  def perimeter(self):\n    pass\n\nclass Circle(Shape):\n  def __init__(self, radius):\n    self.radius = radius\n\n  def area(self):\n    return 3.14159 * self.radius ** 2\n\n  def perimeter(self):\n    return 2 * 3.14159 * self.radius\n\n# This will raise TypeError:\n# shape = Shape() # Can't instantiate abstract class\n\ncircle = Circle(5)\nprint(circle.area())\n</code></pre> <p>Let me give you another example of abstraction.</p> Python<pre><code>class DatabaseConnection(ABC):\n  @abstractmethod\n  def connect(self):\n    pass\n\n  @abstractmethod\n  def execute_query(self, query):\n    pass\n\n  def get_user(self, user_id):\n    query = f\"SELECT * FROM users WHERE id = {user_id}\"\n    return self.execute_query(query)\n\nclass MySQLConnection(DatabaseConnection):\n  def connect(self):\n    # MySQL-specific connection logic\n    pass\n\n  def execute_query(self, query):\n    # MySQL-specific query execution\n    pass\n\nclass PostgreSQLConnection(DatabaseConnection):\n  def connect(self):\n    # PostgreSQL-specific connection logic\n    pass\n\n  def execute_query(self, query):\n    # PostgreSQL-specific query execution\n    pass\n</code></pre>"},{"location":"docs/python-oop/class-and-object/","title":"Class &amp; Object","text":""},{"location":"docs/python-oop/class-and-object/#concept","title":"Concept","text":"Class Object A blueprint or template for creating objects An instance of a class Defines attributes (data) and behaviours (method) that the created objects will have Contains data and methods defined by the class or has its own set of attribute values Uses the <code>class</code> keyword Created using the class name followed by parentheses Contains the <code>__init__</code> method for initialization Created by calling the class like a function"},{"location":"docs/python-oop/class-and-object/#implementation","title":"Implementation","text":"<p>Let's explore how to create classes and objects in Python.</p>"},{"location":"docs/python-oop/class-and-object/#step-1-create-a-class","title":"Step 1: Create a Class","text":"<p>To create a class in Python, use the <code>class</code> keyword followed by the class name. By convention, class names are written in CamelCase.</p> <p><code>Car</code> is your blueprint for creating car objects. It defines the attributes and methods that each car object will have.</p> <pre><code>classDiagram\n    class Car {\n        +int wheels = 4\n        +str brand\n        +str model\n        +int year\n        +bool is_running\n        +start()\n        +stop()\n    }</code></pre> <p>car.py<pre><code>class Car:\n  # Class attributes (shared by all instances)\n  wheels = 4\n\n  def __init__(self, brand, model, year):\n    self.brand = brand # Instance attribute\n    self.model = model # Instance attribute\n    self.year = year # Instance attribute\n    self.is_running = False # Instance attribute\n\n  def start(self): # Instance method\n    self.is_running = True\n    print(f\"{self.brand} {self.model} is now running.\")\n\n  def stop(self): # Instance method\n    self.is_running = False\n    print(f\"{self.brand} {self.model} has stopped.\")\n</code></pre> - <code>__init__()</code> method: This is the constructor that initializes the instance attributes when a new object is created. It takes <code>self</code> (the instance itself) and other parameters to set the initial state of the object.   - Attributes created in <code>__init__()</code> are called instance attributes and are unique to each object.</p>"},{"location":"docs/python-oop/class-and-object/#step-2-create-an-object","title":"Step 2: Create an Object","text":"<pre><code>classDiagram\n    class Car {\n        +int wheels = 4\n        +str brand\n        +str model\n        +int year\n        +bool is_running\n        +start()\n        +stop()\n    }\n\n    Car &lt;|-- my_car : instance\n    Car &lt;|-- another_car : instance</code></pre> <p>car.py<pre><code>class Car:\n  # Class attributes (shared by all instances)\n  wheels = 4\n\n  def __init__(self, brand, model, year):\n    self.brand = brand # Instance attribute\n    self.model = model # Instance attribute\n    self.year = year # Instance attribute\n    self.is_running = False # Instance attribute\n\n  def start(self): # Instance method\n    self.is_running = True\n    print(f\"{self.brand} {self.model} is now running.\")\n\n  def stop(self): # Instance method\n    self.is_running = False\n    print(f\"{self.brand} {self.model} has stopped.\")\n\n# Creating objects (instances)\nmy_car = Car(\"BMW\", \"X5\", 2020)\nanother_car = Car(\"Audi\", \"A4\", 2021)\n\n# Accessing attributes and methods\nprint(my_car.brand) # BMW\nmy_car.start() # BMW X5 is now running!\nprint(my_car.is_running) # True\n</code></pre> </p> <p></p>"},{"location":"docs/python-oop/cohesion-and-coupling/","title":"Cohesion and Coupling","text":"<p>Important</p> <ul> <li>High coupling and low cohesion can lead to difficult-to-maintain code</li> <li>Low coupling and high cohesion can lead to more maintainable, understandable, and flexible code.</li> </ul>"},{"location":"docs/python-oop/cohesion-and-coupling/#coupling","title":"Coupling","text":"<p>Coupling refers to the degree of interdependence between software modules. Meaning that how closely connected modules are, and how much they rely on each other.</p> <ul> <li>High coupling means modules are tightly (closely) connected and changes in one module can significantly affect others.</li> <li>Low coupling means modules are independent and changes in one module has minimal impact on others.</li> </ul> <pre><code>graph\n  A[Module A] --- B[Module B]\n  B --- C[Module C]\n  A --- C</code></pre>"},{"location":"docs/python-oop/cohesion-and-coupling/#cohesion","title":"Cohesion","text":"<p>Cohesion refers to the degree to which elements of a module belong together. Meaning that how closely related and focused the responsibilities of a single module are.</p> <ul> <li>High cohesion means that a module has a single purpose, well-defined responsibility and all its elements are closely related.</li> <li>Low cohesion means that a module has multiple purposes, unrelated responsibilties and its elements are not closely related.</li> </ul>"},{"location":"docs/python-oop/cohesion-and-coupling/#differences-of-cohesion-and-coupling","title":"Differences of Cohesion and Coupling","text":"Aspect Coupling Cohesion Definition Degree of interdependence between modules Degree to which elements within a module belong together Purpose Relationships between modules Relationships within a module Goal Low coupling (loosely couple) High cohesion (highly cohesive) <p>Here's a real-life example using a Library System to illustrate low coupling and high cohesion.</p> <pre><code>graph TD\n  subgraph BookManager\n    A1[Add Book]\n    A2[Remove Book]\n    A3[Search Book]\n    A1 --- A2 --- A3\n  end\n  subgraph UserManager\n    B1[Add User]\n    B2[Remove User]\n    B3[Search User]\n    B1 --- B2 --- B3\n  end\n  subgraph NotificationManager\n    C1[Send Email]\n    C2[Send SMS]\n    C1 --- C2\n  end\n  BookManager -.-&gt; NotificationManager\n  UserManager -.-&gt; NotificationManager</code></pre> <p>High Cohesion - Each module has high cohesion (focused responsibilities):</p> <ul> <li>The <code>BookManager</code> module only manages books (add, remove, search). Low Coupling:</li> <li><code>BookManager</code>, <code>UserManager</code>, and <code>NotificationManager</code> work independently and interact through clear interfaces.</li> </ul> <p>Low Coupling - The modules are loosely coupled:</p> <ul> <li>The dashed arrows show low coupling (modules interact but are not tightly bound).</li> </ul>"},{"location":"docs/python-oop/concepts/","title":"Concepts of Object-Oriented Programming (OOP)","text":""},{"location":"docs/python-oop/concepts/#introduction-to-object-oriented-programming-oop","title":"Introduction to Object-Oriented Programming (OOP)?","text":"<p>Important</p> <p>This section will include all the topics related to Object-Oriented Programming (OOP) in Python. I will cover the following topics in each section.</p> <p>Object-Oriented Programming (OOP) is a programming paradigm that uses \"objects\" to design software. An object will contain attributes (data, properties, variables) and behaviors (methods, functions). You will start by defining classes which will act as blueprints for creating objects.</p> <ul> <li>Attributes: Variables associated with the object that hold data. For example, a <code>Car</code> object might have attributes like <code>color</code>, <code>model</code>, and <code>year</code>.</li> <li>Methods: Functions associated with the object that define its behavior/action. For example, a <code>Car</code> object might have methods like <code>start()</code>, <code>stop()</code>, and <code>accelerate()</code>.</li> </ul>"},{"location":"docs/python-oop/concepts/#classes-and-objects","title":"Classes and Objects","text":"Type Explanation Class A blueprint for creating objects. It defines the attributes and behaviours that the created objects will have. Object An instance of a class. It contains data and methods defined by the class."},{"location":"docs/python-oop/concepts/#four-pillars-of-oop","title":"Four Pillars of OOP","text":"Pillar Explanation Encapsulation Bundle data (attributes) and methods (behaviours) into a single unit (class). It restricts direct access to some of the object's components, which can prevent the accidental modification of data. Mainly hiding internal state and implementation details. Inheritance A mechanism to create a new class (subclasses) that inherits attributes and methods from an existing class (parent classes). It allows for code reuse and establishes a relationship between classes. Polymorphism The ability to use a single interface to represent different underlying data types. It allows methods to do different things based on the object it is acting upon. Also, it allows methods to have the same name but behave differently based on the object type by using method overriding or method overloading. Abstraction Hiding complex internal implementation details and showing only the essential features of the object. It allows the developer to focus on \"what to do\" (focus on what an object does) rather than \"how to do it\" (how it achieves its functionality)."},{"location":"docs/python-oop/concepts/#benefits-of-oop","title":"Benefits of OOP","text":"<p>I just highlight some of the key benefits of Object-Oriented Programming (OOP) below. There are many more benefits, but these are the most important ones that you should know.</p> Benefit Explanation Code Reusability OOP allows for reusing existing code through inheritance, which reduces redundancy and improves maintainability. Modularity Breaks down complex systems into smaller manageable pieces (classes and objects), making it easier to understand, develop, modify, maintain, and debug code. Flexibility and Scalability OOP allows for easier modifications and extensions to existing code. New features can be added with minimal changes to existing code, making it easier to scale applications. Clear structure and enhanced collaboration OOP promotes a clear structure for organizing xode, which makes it easier for teams to work together."},{"location":"docs/python-oop/encapsulation/","title":"Encapsulation","text":""},{"location":"docs/python-oop/encapsulation/#concept","title":"Concept","text":"<p>Bundle data (attributes) and methods (behaviours) into a single unit (class). It restricts direct access to some of the object's components, which can prevent the accidental modification of data. Mainly hiding internal state and implementation details.</p> Type of Encapsulation Description Public Accessible from anywhere, no restrictions. Protected Accessible within the class and its subclasses, but not from outside. Private Accessible only within the class, not from outside or subclasses. <ul> <li> <p>Data Protection</p> <ul> <li>Prevent direct access to sensitive data.</li> <li>Validate data before modification.</li> <li>Maintains data integrity.</li> <li>Controls how data is accessed and modified.</li> </ul> </li> <li> <p>Code Maintainability</p> <ul> <li>Internal implementation can change without affecting external code.</li> <li>External interface remains stable.</li> <li>Reduces dependencies on internal details (Reduces coupling between classes).</li> <li>Makes code easier to maintain and refactor.  </li> </ul> </li> </ul>"},{"location":"docs/python-oop/encapsulation/#implementation","title":"Implementation","text":"<p>Encapsulation can be implemented through private, public, and protected attributes and methods.</p>"},{"location":"docs/python-oop/encapsulation/#get-and-set-methods","title":"Get and Set Methods","text":"Python<pre><code>class BankAccount:\n  def __init__(self, owner, balance, pin):\n    self.owner = owner # Public attribute\n    self._balance = balance  # Protected attribute\n    self.__pin = pin  # Private attribute\n\n  def get_pin(self):\n    return self.__pin\n\n  def set_pin(self, pin):\n    self.__pin = pin\n\n  def deposit(self, amount):\n    if amount &gt; 0:\n      self._balance += amount\n      return True\n    return False\n\n  def withdraw(self, amount):\n    if 0 &lt; amount &lt;= self._balance:\n      self._balance -= amount\n      return True\n    return False\n\n  def get_balance(self):\n    return self._balance\n</code></pre>"},{"location":"docs/python-oop/encapsulation/#property-decorators","title":"Property Decorators","text":"<p>You can use property decorators to create getter and setter methods for encapsulated attributes.</p> Python<pre><code>class BankAccount:\n  def __init__(self, username, password, balance, pin):\n    self.username = username\n    self._balance = balance\n    self.__password = password\n    self.__pin = pin\n\n  @property\n  def balance(self):\n    return self._balance\n\n  @balance.setter\n  def balance(self, amount):\n    if amount &gt;= 0:\n      self._balance = amount\n    else:\n      raise ValueError(\"Balance cannot be negative\")\n</code></pre>"},{"location":"docs/python-oop/inheritance/","title":"Inheritance","text":""},{"location":"docs/python-oop/inheritance/#concept","title":"Concept","text":"<p>A mechanism to create a new class (subclasses) that inherits attributes and methods from an existing class (parent classes). It allows for code reuse and establishes a relationship between classes.</p> Type of Inheritance Description Example Single Inheritance A subclass inherits from one parent class. <code>class Dog(Animal): pass</code> Multiple Inheritance A subclass inherits from multiple parent classes. <code>class FlyingDog(Dog, Bird): pass</code> Multilevel Inheritance A subclass inherits from a parent class, which in turn inherits from another parent class. <code>Animal -&gt; Mammal -&gt; Dog</code> Hierarchical Inheritance Multiple subclasses inherit from a single parent class. <code>Animal -&gt; Dog, Cat, Bird</code> Hybrid Inheritance A combination of two or more types of inheritance. <code>class FlyingDog(Dog, Bird): pass</code> where <code>Dog</code> and <code>Bird</code> are subclasses of <code>Animal</code>."},{"location":"docs/python-oop/inheritance/#implementation","title":"Implementation","text":""},{"location":"docs/python-oop/inheritance/#single-inheritance","title":"Single Inheritance","text":"<pre><code>classDiagram\n  Animal &lt;|-- Dog</code></pre> <p>Python<pre><code>class Animal:\n  def __init__(self, name, age):\n    self.name = name\n    self.age = age\n\n  def speak(self):\n    print(\"Animal speaks\")\n\nclass Dog(Animal):\n  def __init__(self, name, age):\n    super().__init__(name, age)  # Call the parent class constructor\n\n  def speak(self): # method overriding\n    print(\"Woof!\")\n\ndog = Dog(\"Buddy\", 3)\ndog.speak()  # Output: Woof!\nprint(dog.name)  # Output: Buddy\nprint(dog.age)   # Output: 3\nprint(isinstance(dog, Animal))  # Output: True\n</code></pre> - <code>super().__init__(name, age)</code> is used to call the constructor of the parent class (<code>Animal</code>) to initialize the inherited attributes. - <code>isinstance(dog, Animal)</code> checks if <code>dog</code> is an instance of the <code>Animal</code> class or its subclasses.</p>"},{"location":"docs/python-oop/inheritance/#multiple-inheritance","title":"Multiple Inheritance","text":"<pre><code>classDiagram\n  Dog &lt;|-- FlyingDog\n  Bird &lt;|-- FlyingDog</code></pre> <p>When a class inherits from multiple parent classes, the order is determined by the Method Resolution Order (MRO), which follows the C3 linearization algorithm. This ensures a consistent order of method resolution. The MRO can be checked using the <code>__mro__</code> attribute or the <code>mro()</code> method. The MRO is a depth-first, left-to-right traversal of the class hierarchy.</p> Python<pre><code>class Dog:\n  def bark(self):\n    print(\"Woof!\")\n\nclass Bird:\n  def fly(self):\n    print(\"Bird flies\")\n\nclass FlyingDog(Dog, Bird):\n  pass\n\nfd = FlyingDog()\nfd.bark()  # Output: Woof!\nfd.fly()   # Output: Bird flies\n</code></pre> <p>The MRO for this is <code>FlyingDog -&gt; Dog -&gt; Bird</code></p>"},{"location":"docs/python-oop/inheritance/#multilevel-inheritance","title":"Multilevel Inheritance","text":"<pre><code>classDiagram\n  Animal &lt;|-- Mammal\n  Mammal &lt;|-- Dog</code></pre> Python<pre><code>class Animal:\n  def eat(self):\n    print(\"Animal eats\")\n\nclass Mammal(Animal):\n  pass\n\nclass Dog(Mammal):\n  pass\n\ndog = Dog()\ndog.eat()  # Output: Animal eats\n</code></pre>"},{"location":"docs/python-oop/inheritance/#hierarchical-inheritance","title":"Hierarchical Inheritance","text":"<pre><code>classDiagram\n  Animal &lt;|-- Dog\n  Animal &lt;|-- Cat</code></pre> Python<pre><code>class Animal:\n  def sleep(self):\n    print(\"Animal sleeps\")\n\nclass Dog(Animal):\n  pass\n\nclass Cat(Animal):\n  pass\n\ndog = Dog()\ncat = Cat()\ndog.sleep()  # Output: Animal sleeps\ncat.sleep()  # Output: Animal sleeps\n</code></pre>"},{"location":"docs/python-oop/inheritance/#hybrid-inheritance","title":"Hybrid Inheritance","text":"<pre><code>classDiagram\n  Animal &lt;|-- Dog\n  Animal &lt;|-- Bird\n  Dog &lt;|-- FlyingDog\n  Bird &lt;|-- FlyingDog</code></pre> Python<pre><code>class Animal:\n  def move(self):\n    print(\"Animal moves\")\n\nclass Dog(Animal):\n  def bark(self):\n    print(\"Woof!\")\n\nclass Bird(Animal):\n  def fly(self):\n    print(\"Bird flies\")\n\nclass FlyingDog(Dog, Bird):\n  pass\n\nfd = FlyingDog()\nfd.move()  # Output: Animal moves\nfd.bark()  # Output: Woof!\nfd.fly()   # Output: Bird flies\n</code></pre>"},{"location":"docs/python-oop/polymorphism/","title":"Polymorphism","text":""},{"location":"docs/python-oop/polymorphism/#concept","title":"Concept","text":"<p>The ability to use a single interface to represent different underlying data types. It allows methods to do different things based on the object it is acting upon. Also, it allows methods to have the same name but behave differently based on the object type by using method overriding or method overloading.</p> Types of Polymorphism Description Compile-time Polymorphism (Method or operator overloading) The method to be executed is determined at compile time. It allows the same method name to be used with different parameters or types. Runtime Polymorphism (Method Overriding) The method to be executed is determined at runtime. It allows a subclass to provide a specific implementation of a method that is already defined in its superclass."},{"location":"docs/python-oop/polymorphism/#implementation","title":"Implementation","text":""},{"location":"docs/python-oop/polymorphism/#method-overriding","title":"Method Overriding","text":"<p>Based on the below example, the <code>area</code> method is overridden in the <code>Circle</code> and <code>Rectangle</code> classes to provide specific implementations for calculating the area of each shape. The <code>Shape</code> class serves as a base class with a generic <code>area</code> method.</p> Python<pre><code>class Shape:\n  def area(self):\n    pass\n\nclass Circle(Shape):\n  def __init__(self, radius):\n    self.radius = radius\n\n  def area(self):\n    return 3.14 * self.radius ** 2\n\nclass Rectangle(Shape):\n  def __init__(self, length, width):\n    self.length = length\n    self.width = width\n\n  def area(self):\n    return self.length * self.width\n\n# Runtime Polymorphism\nshapes = [Circle(5), Rectangle(4, 6)]\nfor shape in shapes:\n  print(f\"Area: {shape.area()}\") # Calls the overridden method based on the object type\n</code></pre>"},{"location":"docs/python-oop/polymorphism/#method-overloading","title":"Method Overloading","text":"<p>Based on the below example, the <code>add</code> method is overloaded to handle different numbers of parameters. The method can accept either two or three arguments, and it will return the sum accordingly.</p> Python<pre><code>class Calculator:\n    def add(self, a, b):\n      return a + b\n    def add(self, a, b, c):\n      return a + b + c\n\ncalculator = Calculator()\nprint(calculator.add(2, 3))        # Output: 5\nprint(calculator.add(2, 3, 4))      # Output: 9\n</code></pre>"},{"location":"docs/python-oop/polymorphism/#operator-overloading","title":"Operator Overloading","text":"<p>More Information</p> <p>Methods like <code>__add__</code>, <code>__str__</code>, <code>__eq__</code>, etc are called dunder methods (double underscore methods) or magic methods. They allow you to define how operators and built-in functions behave for user-defined classes.</p> <p>Operator overloading allows you to define how operators behave for user-defined classes. For example, you can define how the <code>+</code> operator works for a custom class by implementing the <code>__add__</code> method.</p> Python<pre><code>class Vector:\n  def __init__(self, x, y):\n    self.x = x\n    self.y = y\n\n  def __add__(self, other):\n    return Vector(self.x + other.x, self.y + other.y)\n\n  def __str__(self):\n    return f\"Vector({self.x}, {self.y})\"\n\n  def __eq__(self, other):\n    return self.x == other.x and self.y == other.y\n\nvector1 = Vector(1, 2)\nvector2 = Vector(3, 4)\nresult = vector1 + vector2  # Uses the __add__ method\n\nprint(result)  # Output: Vector(4, 6)\nprint(vector1 == vector2)  # Uses the __eq__ method, Output: False\n</code></pre>"},{"location":"docs/python-oop/uml-class-diagram/","title":"UML Class Diagram","text":""},{"location":"docs/python-oop/uml-class-diagram/#what-is-a-uml-class-diagram","title":"What is a UML Class Diagram?","text":"<p>A UML (Unified Modeling Language) class diagram is a visual representation of classes and their relationships in an object-oriented system. It helps in understanding the structure of the system, showing how classes interact with each other.</p> <ul> <li>Class name</li> <li>Attributes (properties)</li> <li>Methods (functions)</li> <li>Relationships (associations, inheritance, etc.)</li> </ul>"},{"location":"docs/python-oop/uml-class-diagram/#uml-class-notation","title":"UML Class Notation","text":"<pre><code>classDiagram\n  class Car {\n    -brand: String\n    -model: String\n    -year: int\n    +startEngine(): void\n    +stopEngine(): void\n    +drive(speed: int): void\n  }</code></pre> <p>The class notation includes:</p> <ul> <li>Class name</li> <li>Attributes</li> <li>Methods</li> <li>Visibility:<ul> <li><code>+</code> Public</li> <li><code>-</code> Private</li> <li><code>#</code> Protected</li> </ul> </li> </ul>"},{"location":"docs/python-oop/uml-class-diagram/#relationships-between-classes","title":"Relationships between Classes","text":""},{"location":"docs/python-oop/uml-class-diagram/#association","title":"Association","text":"<p>Association represents a bi-directional (two-ways relationship) relationship between two classes where one class uses or interacts with another. It is a one-to-one or one-to-many relationship. It typically means uses, has-a, works with, or belongs to.</p> <ul> <li>Permanent reference between classes. This means one class holds a reference to another class as part of its state or structure, not just temporarily within a method</li> </ul> <p>Example: Suppose a <code>Car</code> has a <code>Engine</code> and <code>Wheel</code>. This is an association because a car object is associated with an engine object and multiple wheel objects.</p> <ul> <li>Each <code>Car</code> has one <code>Engine</code> and each <code>Engine</code> belongs to one <code>Car</code></li> <li>Each <code>Car</code> contains multiple <code>Wheels</code> and each <code>Wheel</code> belongs to one <code>Car</code></li> </ul> <pre><code>classDiagram\n  class Car {\n    -brand: String\n    -model: String\n    -year: int\n    +startEngine(): void\n    +stopEngine(): void\n    +drive(speed: int): void\n  }\n  class Engine {\n    -type: String\n    -horsepower: int\n    +start(): void\n    +stop(): void\n  }\n  class Wheel {\n    -size: int\n    -type: String\n  }\n  Car \"1\" -- \"1\" Engine : has a\n  Car \"1\" -- \"1..*\" Wheel : has</code></pre> <p>Another example is that <code>Library</code> has many <code>Books</code>. The association is bi-directional, meaning the <code>Library</code> contains multiple <code>Books</code>, and each <code>Book</code> belongs to one <code>Library</code>.</p>"},{"location":"docs/python-oop/uml-class-diagram/#directed-association","title":"Directed Association","text":"<p>A directed association is a one-way (unidirectional) relationship where one class points to another, indicating that one class uses or interacts with another.</p> <p>Example: Suppose a <code>Driver</code> drives a <code>Car</code>. The association is directed from <code>Driver</code> to <code>Car</code>, meaning the <code>Driver</code> uses the <code>Car</code>, but this car does not necessarily belong to the <code>Driver</code>.</p> <pre><code>classDiagram\n  class Driver {\n    -name: String\n    +drive(car: Car): void\n  }\n  class Car {\n    -brand: String\n    -model: String\n    -year: int\n    +startEngine(): void\n    +stopEngine(): void\n    +drive(speed: int): void\n  }\n  Driver --&gt; Car : drives</code></pre> <p>Another example is that <code>Teacher</code> teaches this <code>Course</code>. The association is directed from <code>Teacher</code> to <code>Course</code>, meaning the <code>Teacher</code> teaches the <code>Course</code>, but this course does not necessarily belong to the <code>Teacher</code>.</p>"},{"location":"docs/python-oop/uml-class-diagram/#aggregation","title":"Aggregation","text":"<p>Aggregation represents a part-of relationship where one class is a whole and another class is a part. It is a one-to-many relationship where the part can exist independently of the whole.</p> <p>Example: A <code>Team</code> consists of multiple <code>Players</code>. The <code>Team</code> is the whole, and the <code>Players</code> are the parts. If the <code>Team</code> is deleted, the <code>Players</code> can still exist independently.</p> <pre><code>classDiagram\n  class Team {\n    -name: String\n    +addPlayer(player: Player): void\n  }\n  class Player {\n    -name: String\n    -position: String\n  }\n  Team o-- \"1..*\" Player : has</code></pre> <p>Another example is that <code>Department</code> has many <code>Employees</code>. The <code>Department</code> is the whole, and the <code>Employees</code> are the parts. If the <code>Department</code> is deleted, the <code>Employees</code> can still exist independently.</p>"},{"location":"docs/python-oop/uml-class-diagram/#composition","title":"Composition","text":"<p>Composition is a special type of aggregation that represents a strong part-of relationship where the part cannot exist independently of the whole. It is a one-to-many relationship where the part is tightly coupled with the whole. If the whole is deleted, the parts are also deleted.</p> <p>Example: A <code>House</code> consists of multiple <code>Rooms</code>. The <code>House</code> is the whole, and the <code>Rooms</code> are the parts. If the <code>House</code> is destroyed, the <code>Rooms</code> are also destroyed.</p> <pre><code>classDiagram\n  class House {\n    -address: String\n    +addRoom(room: Room): void\n  }\n  class Room {\n    -name: String\n    -area: float\n  }\n  House *-- \"1..*\" Room : composed of</code></pre> <p>Another example is that <code>Book</code> has many <code>Chapters</code>. The <code>Book</code> is the whole, and the <code>Chapters</code> are the parts. If the <code>Book</code> is deleted, the <code>Chapters</code> are also deleted.</p>"},{"location":"docs/python-oop/uml-class-diagram/#generalization-inheritance","title":"Generalization (Inheritance)","text":"<p>Generalization (also known as inheritance) represents a is-a relationship where one class (subclass or child) &amp;&amp; from another class (superclass or parent). The subclass inherits the attributes and methods of the superclass, allowing for code reuse and polymorphism. </p> <p>Example: A <code>Bird</code> is a type of <code>Animal</code>, and a <code>Fish</code> is also a type of <code>Animal</code>. Both <code>Bird</code> and <code>Fish</code> inherit from the <code>Animal</code> class.</p> <pre><code>classDiagram\n  class Animal {\n    -name: String\n    +eat(): void\n    +sleep(): void\n  }\n  class Bird {\n    +fly(): void\n  }\n  class Fish {\n    +swim(): void\n  }\n  Animal &lt;|-- Bird : is a\n  Animal &lt;|-- Fish : is a</code></pre> <p>Another example is that <code>Vehicle</code> is a type of <code>Transport</code>. The <code>Vehicle</code> class inherits from the <code>Transport</code> class, allowing it to reuse its attributes and methods.</p>"},{"location":"docs/python-oop/uml-class-diagram/#realization-interface","title":"Realization (Interface)","text":"<p>Realization represents a contract between a class and an interface. A class that implements an interface must provide concrete implementations for the methods defined in the interface. This allows for polymorphism and code flexibility.</p> <p>Example: A <code>Shape</code> interface defines methods for drawing and calculating the area, and classes like <code>Circle</code> and <code>Rectangle</code> implement this interface.</p> <pre><code>classDiagram\n  class Shape {\n    &lt;&lt;interface&gt;&gt;\n    +draw(): void\n    +area(): float\n  }\n  class Circle {\n    -radius: float\n    +draw(): void\n    +area(): float\n  }\n  class Rectangle {\n    -width: float\n    -height: float\n    +draw(): void\n    +area(): float\n  }\n  Shape &lt;|.. Circle : realizes\n  Shape &lt;|.. Rectangle : realizes</code></pre> <p>Another example is that <code>Vehicle</code> interface defines methods for starting and stopping, and classes like <code>Car</code> and <code>Bike</code> implement this interface.</p>"},{"location":"docs/python-oop/uml-class-diagram/#dependency","title":"Dependency","text":"<p>Dependency represents a depends on relationship where one class depends/relies on another class to function correctly. Meaning that an object of one class might use an object of another class in the code of a method or temporarily within its code. It is considered a weak relationship because the dependent (Client) class is only affected if the supplier class changes. They are not tightly coupled, but changes in the supplier can still impact the client.</p> <ul> <li>If the supplier class changes (for example, its interface, method signatures, or behavior), the client class may need to be updated to work with those changes</li> </ul> <p>More Information</p> <p>You can consider dependency relationship is loosely coupled, as the client class uses the supplier class temporarily and does not maintain a permanent reference to it. This means the classes are not tightly bound together, making the relationship weak and flexible. However, changes in the supplier class (like method signatures) can still require updates in the client class.</p> <p>Example: A <code>ReportGenerator</code> class generates reports and uses a <code>Printer</code> class to print the reports. The <code>ReportGenerator</code> depends on the <code>Printer</code> to perform its function.</p> <ul> <li><code>ReportGenerator</code> - Client</li> <li><code>Printer</code> - Supplier</li> </ul> <pre><code>classDiagram\n  class ReportGenerator {\n    +generate(): void\n    +printReport(printer: Printer): void\n  }\n  class Printer {\n    +print(): void\n  }\n  ReportGenerator ..&gt; Printer</code></pre> <p>Another example is that <code>Person</code> class depends on the <code>Book</code> class to read books. However, the <code>Book</code> class does not depend on the <code>Person</code> class, meaning the <code>Book</code> can exist independently without the <code>Person</code>.</p>"},{"location":"docs/python-oop/uml-class-diagram/#usage-dependency","title":"Usage (Dependency)","text":"<p>More Information</p> <p>Same concept as Dependency, but with a focus on temporary usage.</p> <p>Usage Dependency is a type of dependency where one class (the client) uses another class (the supplier) for a specific purpose, usually within a method or temporarily. The client does not maintain a permanent reference to the supplier.</p> <p>Example: A <code>Mechanic</code> class repairs a <code>Car</code>. The <code>Mechanic</code> uses the <code>Car</code> to perform repairs, but the <code>Car</code> does not depend on the <code>Mechanic</code>.</p> <ul> <li><code>Mechanic</code> - Client</li> <li><code>Car</code> - Supplier</li> </ul> <pre><code>classDiagram\n  class Mechanic {\n    +repair(car: Car): void\n  }\n  class Car {\n    +startEngine(): void\n    +stopEngine(): void\n  }\n  Mechanic ..&gt; Car : uses</code></pre> <p>Another example is that <code>Chef</code> class cooks a <code>Dish</code>. The <code>Chef</code> uses the <code>Dish</code> to prepare a meal, but the <code>Dish</code> does not depend on the <code>Chef</code>.</p>"},{"location":"docs/python-oop/uml-class-diagram/#full-example","title":"Full Example","text":""},{"location":"docs/python-oop/uml-class-diagram/#order-management-system","title":"Order Management System","text":"<pre><code>classDiagram\n    %% Generalization (inheritance)\n    Customer &lt;|-- RegisteredCustomer\n    Customer &lt;|-- GuestCustomer\n\n    %% Realization (interface implementation)\n    PaymentMethod &lt;|.. CreditCard\n    PaymentMethod &lt;|.. PayPal\n\n    %% Association (bidirectional)\n    Customer \"1\" -- \"0..*\" Order : places\n\n    Order \"1\" *-- \"1\" ShippingInfo : ships to\n\n    %% Aggregation (hollow diamond)\n    Order \"1\" o-- \"1..*\" OrderItem : contains\n    OrderItem \"1\" o-- \"1\" Product : refers to\n\n    %% Dependency (dashed arrow)\n    Invoice ..&gt; Order\n\n    %% Usage Dependency (dashed arrow)\n    Order ..&gt; PaymentMethod : uses\n\n    %% Classes and Interfaces\n    class Customer {\n        +id: int\n        +name: string\n        +email: string\n    }\n    class RegisteredCustomer {\n        +username: string\n        +password: string\n    }\n    class GuestCustomer {\n        +guestId: string\n    }\n    class Order {\n        +orderId: int\n        +date: Date\n        +status: string\n    }\n    class OrderItem {\n        +quantity: int\n        +price: float\n    }\n    class Product {\n        +productId: int\n        +name: string\n        +price: float\n    }\n    class ShippingInfo {\n        +address: string\n        +deliveryDate: Date\n    }\n    class Invoice {\n        +invoiceId: int\n        +amount: float\n    }\n    class PaymentMethod {\n        &lt;&lt;interface&gt;&gt;\n        +pay(amount: float): bool\n    }\n    class CreditCard {\n        +cardNumber: string\n        +expiry: Date\n    }\n    class PayPal {\n        +email: string\n    }</code></pre> <ol> <li> <p>Generalization (Inheritance)</p> <ul> <li><code>RegisteredCustomer</code> and <code>GuestCustomer</code> both inherit from <code>Customer</code>.</li> <li>Meaning: Both types of customers share common attributes (like id, name, email) but also have their own specific attributes.</li> </ul> </li> <li> <p>Realization (Interface Implementation)</p> <ul> <li><code>PaymentMethod</code> is an interface.</li> <li><code>CreditCard</code> and <code>PayPal</code> implement the PaymentMethod interface.<ul> <li>Meaning: Both payment types must provide a <code>pay(amount: float): bool</code> method.</li> </ul> </li> </ul> </li> <li> <p>Association</p> <ul> <li><code>Customer \"1\" -- \"0..*\" Order : places</code><ul> <li>Meaning: One customer can place multiple orders; each order is placed by one customer.</li> </ul> </li> </ul> </li> <li> <p>Composition</p> <ul> <li><code>Order \"1\" *-- \"1\" ShippingInfo : ships to</code><ul> <li>Meaning: Each order has one shipping info, and if the order is deleted, its shipping info is also deleted (strong ownership).</li> </ul> </li> </ul> </li> <li> <p>Aggregation</p> <ul> <li><code>Order \"1\" o-- \"1..*\" OrderItem : contains</code><ul> <li>Meaning: An order contains multiple order items, but order items can exist independently of the order.</li> </ul> </li> <li><code>OrderItem \"1\" o-- \"1\" Product : refers to</code><ul> <li>Meaning: Each order item refers to a product, but products exist independently of order items.</li> </ul> </li> </ul> </li> <li> <p>Dependency</p> <ul> <li><code>Invoice ..&gt; Order</code><ul> <li>Meaning: An invoice is generated from an order, but the invoice does not own the order.</li> </ul> </li> </ul> </li> <li> <p>Usage Dependency</p> <ul> <li><code>Order ..&gt; PaymentMethod : uses</code><ul> <li>Meaning: An order uses a payment method to process payment (The <code>Order</code> just needs a <code>PaymentMethod</code> to complete the payment operation), but does not own the payment method.</li> </ul> </li> </ul> </li> </ol>"},{"location":"docs/ssh/host-configuration/","title":"Host Configuration","text":""},{"location":"docs/ssh/host-configuration/#host-specific-configuration-file","title":"Host Specific Configuration file","text":"<p>Create a config filename <code>config</code> in your local <code>~/.ssh</code> directory.</p> Bash<pre><code>nano ~/.ssh/config\n</code></pre> <p>You can define each individual SSH configuration options into this file. You can find all the SSH configuration options from this ssh_config.</p> ~/.ssh/config<pre><code>Host * # all hosts\n  ServerAliveInterval 180\n  StrictHostKeyChecking no\n  UserKnownHostsFile /dev/null\n\nHost &lt;remove-alias&gt;\n  HostName &lt;remote-host/ipaddress&gt;\n  Port &lt;port-number&gt;\n  User &lt;username&gt;\n\nHost &lt;remote-alias&gt;\n  HostName &lt;remote-host/ipaddress&gt;\n  Port &lt;port-number&gt;\n\n# sample\nHost server1\n  HostName 192.168.0.1\n  Port 22\n  User karchunt\n</code></pre> Declaration Description Host * All hosts Host <code>remote-alias</code> You can name <code>remote-alias</code> whatever you want ServerAliveInterval If set 180, then every 3 minutes, send a packet to the server to let it know not to close the connection StrictHostKeyChecking If set \"no\", it will disable host checking and it will auto-add new hosts to the <code>known_hosts</code> file (fingerprint) directly UserKnownHostsFile Not warn on new or changed hosts HostName remove host name or IP Address Port Port number to access User Username used to access"},{"location":"docs/ssh/local-tunneling/","title":"Local Tunneling","text":"More Information <p>A Visual Guide to SSH Tunnels: Local and Remote Port Forwarding</p>"},{"location":"docs/ssh/local-tunneling/#local-tunneling-to-a-server","title":"Local tunneling to a server","text":"<p>Traffic between the localhost and remote host can be tunneled via SSH connections. To establish a local tunnel on your remote server, use <code>-L</code> parameter when connecting and must provide</p> <ul> <li>Local port for accessing the tunneled connection</li> <li>Remote host IP/name</li> <li>Remote host port</li> </ul> <p>For general usage, connect to <code>10.0.0.12</code> on port 80 on your remote host, then your local machine is able to ping or make the connection to the remote host through port 8080.</p> Bash<pre><code>ssh -f -N -L &lt;local-port&gt;:&lt;remote-host-ip-address/name&gt;:&lt;remote-port&gt; &lt;username&gt;@&lt;host&gt;\n# example\nssh -L 8080:10.0.0.12:80 username@host\n</code></pre> <p>Now if you go to your browser/curl to <code>localhost:8080</code>, you are able to see the content that hosted at <code>10.0.0.12:80</code>.</p> Parameters Description -f let SSH go into the background before executing -N does not open a shell or execute a program on the remote side -L establish a local tunnel to your remote server <p>If you want to terminate the background connection, you have to find the PID and kill it.</p> Bash<pre><code>ps aux | grep &lt;local-port&gt;\nkill &lt;process-id&gt;\n</code></pre> Bash<pre><code>Output\n1001      5965  0.0  0.0  48168  1136 ?        Ss   12:28   0:00 ssh -f -N -L 8888:your_domain:80 username@remote_host\n1001      6113  0.0  0.0  13648   952 pts/2    S+   12:37   0:00 grep --colour=auto 8888\n</code></pre> Bash<pre><code>kill 5965\n</code></pre>"},{"location":"docs/ssh/local-tunneling/#local-tunneling-local-network","title":"Local tunneling local network","text":"<p>You can tunnel remote host local network to your local. Here is the animation diagram of the local network web server and common SSH server.</p> <p></p> Bash<pre><code>ssh -L 8080:localhost:8080 user@server\n</code></pre>"},{"location":"docs/ssh/local-tunneling/#local-tunneling-private-network","title":"Local tunneling private network","text":"<p>You can tunnnel remote host private network to your local. Here is the animation diagram of the private network web server and common SSH server.</p> <p></p> Bash<pre><code>ssh -L &lt;local-port&gt;:&lt;server-ip-address&gt;:&lt;server-port&gt; &lt;username&gt;@&lt;bastion-server-ip-address&gt;\nssh -L 8080:10.0.0.12:8080 user@bastion\n</code></pre>"},{"location":"docs/ssh/play-with-ssh-keys/","title":"Play with SSH Keys","text":""},{"location":"docs/ssh/play-with-ssh-keys/#generating-an-ssh-key-pair","title":"Generating an SSH Key Pair","text":"<p>Your first step should be creating a new SSH key pair on your computer, then you can connect without a password to a remote server.</p> Bash<pre><code># You can leave those settings as default by pressing ENTER\nssh-keygen\nssh-keygen -t dsa -C \"Comment\" -b 4096\n\n# make sure your private key file exists\nssh-keygen -p # remove or change passphrase on private key\nssh-keygen -l # display the SSH key fingerprint\n</code></pre> <p>For your information, private key's passphrase is just to secure the private key, so that no one will gain access to the remote server even they have your private key, but you have to enter your private key's passphrase everytime if you want to initiate a SSH connection, but this can avoid by using SSH agent.</p> <p>It will generate <code>id_rsa</code> and <code>id_rsa.pub</code> key file to <code>/home/&lt;username&gt;/.ssh</code> hidden directory.</p> <ul> <li><code>id_rsa</code> = private key</li> <li><code>id_rsa.pub</code> = public key</li> </ul>"},{"location":"docs/ssh/play-with-ssh-keys/#optional-parameters","title":"Optional Parameters","text":"Parameters Description Example -t Type of cryptographic algorithms. Default is RSA. rsa, dsa, ecdsa, ed25519 -C Comment simple comment -b The number of bits, default is 2048 bits 4096 -p Removing or changing passphrase on private key (make sure private key file exists) Your password or leave it empty -l Displaying the SSH key fingerprint (make sure private key file exists) -"},{"location":"docs/ssh/play-with-ssh-keys/#copy-the-public-ssh-key-to-the-server","title":"Copy the public SSH key to the server","text":"<p>You can authenticate yourself to the server without a password (passwordless), but you have to copy your public key to the server. There are multiple ways to do it.</p>"},{"location":"docs/ssh/play-with-ssh-keys/#using-ssh-copy-id-command","title":"Using <code>ssh-copy-id</code> command","text":"Bash<pre><code>ssh-copy-id &lt;username&gt;@&lt;remote-server-ip-address/name&gt;\n\n# You can specify the public key through through \"-i\" option\nssh-copy-id -i &lt;public-key-path&gt; &lt;username&gt;@&lt;remote-server-ip-address/name&gt;\n</code></pre> <p>After you type the remote server password, it will copy your public key from your local file <code>~/.ssh/id_rsa.pub</code> to remote server <code>~/.ssh/authorized_keys</code> file.</p>"},{"location":"docs/ssh/play-with-ssh-keys/#manually-copy-ssh-public-key-from-local-to-a-server","title":"Manually copy SSH public key from local to a server","text":"Bash<pre><code># You can copy your local public key through any methods you like\ncat ~/.ssh/id_rsa.pub\nmkdir -p ~/.ssh\necho \"&lt;public_key&gt;\" &gt;&gt; ~/.ssh/authorized_keys\n\n# combination\ncat ~/.ssh/id_rsa.pub | ssh &lt;username&gt;@&lt;remote-server-ip-address/name&gt; \"mkdir -p ~/.ssh &amp;&amp; cat &gt;&gt; ~/.ssh/authorized_keys\"\n</code></pre>"},{"location":"docs/ssh/play-with-ssh-keys/#using-an-ssh-agent-to-avoid-typing-your-private-key-passphrase","title":"Using an SSH agent to avoid typing your private key passphrase","text":"<p>Assume you had set your private SSH key with a passphrase, but you want to eliminate the typing of the private key passphrase. SSH Agent comes into the play to solve this kind of problem.</p> <p>Once the passphrase is entered for the first time, the SSH agent will store your private key to the agent so you don't have to reenter it again.</p> Bash<pre><code># start SSH agent\neval $(ssh-agent)\n\nssh-add # add your private key to agent, default to id_rsa\nssh-add &lt;private-key-path&gt; # You can specify the private key path\n</code></pre>"},{"location":"docs/ssh/play-with-ssh-keys/#forward-ssh-credentials-to-use-on-a-server","title":"Forward SSH credentials to use on a server","text":"<p>In order to connect to one server without a password from within another, you must forward the SSH key information.</p> <p>Before you proceed, you need to make sure your SSH agent starts and your SSH key is added to the agent (ssh-add).</p> Bash<pre><code># forwards your credentials to the server for this session\nssh -A &lt;username&gt;@&lt;remote-server-ip-address/name&gt;\n</code></pre> <p>This will allow you to SSH into any other host that your SSH key has permission to access because the server that you are connected to right now will \"know\" your private SSH key on this server.</p>"},{"location":"docs/ssh/remote-server-configuration/","title":"Remote Server Connection","text":""},{"location":"docs/ssh/remote-server-configuration/#connecting-to-a-remote-server","title":"Connecting to a remote server","text":"Bash<pre><code>ssh &lt;remote-host-ip-address/name&gt; # Your local machine username is same as on the remote server\nssh &lt;username&gt;@&lt;remote-host-ip-address/name&gt; # specify your username if it is different\nssh -p &lt;port-number&gt; &lt;username&gt;@&lt;remote-host-ip-address/name&gt; # specify different port number\nssh -v &lt;username&gt;@&lt;remote-host-ip-address/name&gt; # get the verbose information\n\n# running a single command on a remote host instead of spawning a shell session\n# After the command is completed running, the connection will straightaway close\nssh &lt;username&gt;@&lt;remote-host-ip-address/name&gt; &lt;command-to-run&gt;\nssh &lt;username&gt;@&lt;remote-host-ip-address/name&gt; ls -la\n</code></pre> Parameters Description Example -p Port Number 2222 -v More information (Verbose) - <p>When you SSH to a server, it will prompt you whether you want to connect by showing the fingerprint. It will save the fingerprint to known_hosts file.</p> Info <p>To simplify the connection process, you can create a Host specific configuration file.</p>"},{"location":"docs/ssh/remote-server-configuration/#known_hosts","title":"known_hosts","text":"Info <p>It will generate a <code>known_hosts</code> file in your local <code>~/.ssh</code> directory.</p> <p>A fingerprint will be displayed when you SSH to a server. If you put \"yes\", the system will save the fingerprint to your local <code>~/.ssh/known_hosts</code> file, so you won't have to enter the same thing again next time.</p> <p>As a result, it can help to prevent the man-in-the-middle-attack.</p>"},{"location":"docs/ssh/remote-tunneling/","title":"Remote Tunneling","text":""},{"location":"docs/ssh/remote-tunneling/#remote-tunneling-to-a-server","title":"Remote tunneling to a server","text":"Info <p>The concept is same as local tunneling.</p> <p>A remote tunnel makes a connection to a remote server. The remote port must be specified when creating the remote tunnel. As a result, the remote computer able to connect or access your localhost. To establish a remote tunnel to your remote server, use <code>-R</code> parameter when connecting and must provide</p> <ul> <li>Remote port for accessing the tunneled connection</li> <li>Local host IP/name</li> <li>Local host port</li> </ul> <p>For general usage, connect to <code>localhost</code> on port 8080 on our local computer, then your remote machine is able to ping or make the connetion to the localhost through port 8080.</p> Bash<pre><code>ssh -f -N -R &lt;remote-port&gt;:&lt;local-host-ip-address/name&gt;:&lt;local-port&gt; &lt;username&gt;@&lt;remote-host-ip-address/name/gateway&gt;\n# example\nssh -R 8080:localhost:8080 user@remote-host-ip-address/gateway\n</code></pre> <p>Now if you go to your remote browser/curl to <code>localhost:8080</code>, you are able to see the content that hosted at <code>localhost:80</code>.</p> Parameters Description -R establish a remote tunnel to your remote server"},{"location":"docs/ssh/remote-tunneling/#remote-tunneling-local-network","title":"Remote tunneling local network","text":"<p>You can tunnel your local network to remote host. Here is the animation diagram of the local network web server and common SSH server.</p> <p>Here is the animation diagram of the local network web server and common SSH server.</p> Bash<pre><code>ssh -R 8080:localhost:8080 user@remote-host-ip-address/gateway\nssh -R 0.0.0.0:8080:localhost:8080 user@remote-host-ip-address/gateway\n</code></pre>"},{"location":"docs/ssh/remote-tunneling/#remote-tunneling-private-network","title":"Remote tunneling private network","text":"<p>You can tunnel your local private network to remote host. Here is the animation diagram of the private network web server and common SSH server.</p> Bash<pre><code>ssh -R 8080:&lt;local-server-ip-address&gt;:8080 user@remote-host-ip-address/gateway\nssh -R 0.0.0.0:8080:&lt;local-server-ip-address&gt;:8080 user@remote-host-ip-address/gateway\n</code></pre>"},{"location":"docs/ssh/server-configuration/","title":"Server Configuration","text":""},{"location":"docs/ssh/server-configuration/#disable-password-authentication","title":"Disable password authentication","text":"<p>If you have set up your SSH keys, then my advice is to disable password authentication, as passwordless is more secure than password authentication.</p> <ol> <li> <p>Go to your remote server, find, and edit <code>/etc/ssh/sshd_config</code>.</p> Bash<pre><code>sudo nano /etc/ssh/sshd_config\n</code></pre> </li> <li> <p>Search for <code>PasswordAuthentication</code> text and set it to \"no\".</p> sshd_config<pre><code>PasswordAuthentication no\n</code></pre> </li> <li> <p>Restart the SSH service</p> Bash<pre><code>sudo service ssh restart\n</code></pre> </li> </ol>"},{"location":"docs/ssh/server-configuration/#change-ssh-daemon-runslistens-on-port","title":"Change SSH Daemon runs/listens on port","text":"<p>By default, SSH Daemon runs/listens on port 22. You can change it as well.</p> <ol> <li> <p>Go to your remote server, find, and open <code>/etc/ssh/sshd_config</code>.</p> Bash<pre><code>sudo nano /etc/ssh/sshd_config\n</code></pre> </li> <li> <p>Search for <code>Port</code> text and edit it based on your needs</p> sshd_config<pre><code>#Port 22\nPort 1234\n</code></pre> </li> <li> <p>Restart the SSH service     Bash<pre><code>sudo service ssh restart\n</code></pre></p> </li> </ol>"},{"location":"docs/ssh/server-configuration/#limit-authenticate-users-to-login","title":"Limit authenticate users to login","text":"<ol> <li> <p>Go to your remote server, find, and open <code>/etc/ssh/sshd_config</code>.</p> Bash<pre><code>sudo nano /etc/ssh/sshd_config\n</code></pre> </li> <li> <p>Search for <code>AllowUsers</code> or <code>AllowGroups</code>, if not found, then create it anywhere. Either one should be fine, or you want to implement both too.</p> sshd_config<pre><code>AllowUsers user1 user2 user3\nAllowGroups groupname\n</code></pre> </li> <li> <p>Restart the SSH service</p> Bash<pre><code>sudo service ssh restart\n</code></pre> </li> </ol>"},{"location":"docs/ssh/server-configuration/#disable-root-login","title":"Disable root login","text":"<p>It is a good practice to disable root login</p> <ol> <li> <p>Go to your remote server, find, and open <code>/etc/ssh/sshd_config</code>.</p> Bash<pre><code>sudo nano /etc/ssh/sshd_config\n</code></pre> </li> <li> <p>Search for <code>PermitRootLogin</code> text and set it to \"no\".</p> sshd_config<pre><code>PermitRootLogin no\n</code></pre> </li> <li> <p>Restart the SSH service</p> Bash<pre><code>sudo service ssh restart\n</code></pre> </li> </ol>"},{"location":"docs/ssh/ssh-installation/","title":"SSH Installation","text":"More Information <p>For Windows Users: Enable WSL and SSH into Windows with Bash. Get started with OpenSSH for Windows</p>"},{"location":"docs/ssh/ssh-installation/#install-ssh-on-linux","title":"Install SSH on Linux","text":"Bash<pre><code>sudo apt-get update\nsudo apt-get install -y openssh-client openssh-server\nsudo systemctl enable ssh\nsudo systemctl start ssh\n\n# you can trigger either one will do\nsudo systemctl status ssh\nsudo systemctl status sshd\n</code></pre> <p>You can find all the files under <code>/home/&lt;username&gt;/.ssh</code></p>"},{"location":"docs/ssh/ssh-overview/","title":"SSH Overview","text":""},{"location":"docs/ssh/ssh-overview/#what-is-ssh","title":"What is SSH?","text":"<p>SSH stands for Secure Shell (SSH) Protocol that is mainly used to connect to a Linux server remotely. Basically, it gives you the opportunity to access a server/computer over an unsecured network.</p>"},{"location":"docs/ssh/ssh-overview/#how-ssh-works","title":"How SSH Works?","text":"<p>Client-server architecture is used to implement SSH connections. The remote machine (Server) must be running SSH daemon (The heart of SSH). In SSH, a specific network port (22) is used for connection requests, authentication, and login into shell sessions when the user provides the correct credentials.</p>"},{"location":"docs/ssh/ssh-overview/#how-ssh-authenticate-users","title":"How SSH Authenticate Users?","text":"<p>We can use passwords or SSH keys to authenticate the right users to login.</p>"},{"location":"docs/ssh/ssh-overview/#password","title":"Password","text":"<p>It is not recommended to use passwords to log in, as when the malicious users or bots will keep repeatedly trying to authenticate their accounts, it might potentially lead to security compromises although password logins are encrypted. Therefore, it is less secure compared to SSH Keys.</p>"},{"location":"docs/ssh/ssh-overview/#ssh-keys","title":"SSH Keys","text":"<p>The SSH keys consist of both public and private cryptographic keys. For the public key, the user can share with anyone freely without any concerns, while the private key must be stored in a secure way and cannot be exposed to anyone.</p> <p></p> <p>Steps to authenticate;</p> <ol> <li>The clients must have an SSH key pair (public and private) on their local computers.</li> <li>The local client's public key must be copied to the user's home directory at <code>~/.ssh/authorized_keys</code> on the remote server.</li> <li>When the client connects to the host/server, it will inform the server which public key to use to authenticate.</li> <li>The server will validate the public key from <code>~/.ssh/authorized_keys</code> file, if it is valid, then it will generate a random string and encrypt it using the public key.</li> <li>The server will send the encrypted message to the client to test whether the client has the associated private key.</li> <li>After receiving an encrypted message from the server, the client will use the client's private key to decrypt and send the decrypted information back to the server.</li> <li>Lastly,the client is able to log into shell sessions when the server determines that the client has the associated private key by validating the decrypted information is correct.</li> </ol>"}]}